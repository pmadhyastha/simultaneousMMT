-- disable_filters -> False
-- batch_size -> 16
-- max_len -> 100
-- device_id -> gpu
-- override -> []
-- stochastic -> False
-- splits -> test_2016_flickr
-- source -> None
-- output -> wait2_multimodal_decoding_results
-- func -> vwk
-- delta -> 
-- n_init_tokens -> 1,2,3,5,7
-- criteria -> 
-- model -> ../experiments/simultaneouswaitknmt-r553c9-val036.best.bleu_29.370.ckpt
STranslator: setting batch_size=1 for 'vwk' decoding
SimultaneousWaitKNMT(
  (encoders): ModuleDict(
    (image): VisualFeaturesEncoder(
      (output): Sequential(
        (0): FF(in_features=2048, out_features=320, activ=tanh, bias=True, bias_zero=True)
        (1): Dropout(p=0.5, inplace=False)
      )
    )
    (src): TextEncoder(
      (do_emb): Dropout(p=0.4, inplace=False)
      (emb): Embedding(9797, 200, padding_idx=0)
      (enc): GRU(200, 320, num_layers=2)
      (output): Sequential(
        (0): Dropout(p=0.5, inplace=False)
      )
    )
  )
  (dec): SimultaneousConditionalDecoder(
    (emb): Embedding(7875, 200, padding_idx=0)
    (att): ModuleDict(
      (image): MLPAttention(
        (hid2ctx): Linear(in_features=320, out_features=320, bias=False)
        (ctx2ctx): Linear(in_features=320, out_features=320, bias=False)
        (ctx2hid): Linear(in_features=320, out_features=320, bias=False)
        (mlp): Linear(in_features=320, out_features=1, bias=False)
      )
      (src): MLPAttention(
        (hid2ctx): Linear(in_features=320, out_features=320, bias=False)
        (ctx2ctx): Linear(in_features=320, out_features=320, bias=False)
        (ctx2hid): Linear(in_features=320, out_features=320, bias=False)
        (mlp): Linear(in_features=320, out_features=1, bias=False)
      )
    )
    (fusion): Sequential(
      (0): Fusion(type=sum, adaptor=<function Fusion.__init__.<locals>.<lambda> at 0x7f7446218200>, activ=None)
    )
    (dec0): GRUCell(200, 320)
    (dec1): GRUCell(320, 320)
    (do_out): Dropout(p=0.5, inplace=False)
    (hid2out): FF(in_features=840, out_features=200, activ=tanh, bias=True, bias_zero=True)
    (out2prob): FF(in_features=200, out_features=7875, activ=linear, bias=True, bias_zero=True)
    (nll_loss): NLLLoss()
  )
)
Vocabulary of 9797 items ('train.lc.norm.tok.vocab.en')
Vocabulary of 7875 items ('train.lc.norm.tok-min2.vocab.de')
# parameters: 7.22M (7.22M learnable)

Post-processing filters enabled.
Will translate "['test_2016_flickr']"
Initializing dataset for 'src'...
0sents [00:00, ?sents/s]1000sents [00:00, 149369.80sents/s]
Initializing dataset for 'image'...
Skipping 'trg' as target
MultimodalDataset - (2 source(s) / 0 target(s))
  Sampler type: bucket, bucket_by: src
  Sources:
    --> TextDataset 'test_2016_flickr.lc.norm.tok.en' (1000 sentences)
    --> NumpyDataset 'test_2016_flickr-resnet50-res5c_relu-r256-c256-l2norm.npy' (1000 samples)


Ignoring batch_size 1 for simultaneous greedy search
Starting translation for 'test_2016_flickr'
  0%|                                     | 0/1000 [00:00<?, ?batch/s]  0%|                                     | 0/1000 [00:00<?, ?batch/s]
txt attention
[tensor([[-2.3118e-02,  7.2495e-02, -6.2658e-01, -2.5774e-01, -2.0251e-01,
          5.8373e-01,  1.7589e-01, -2.4382e-01, -6.3324e-02,  5.9930e-02,
          1.6809e-02,  3.2681e-01,  8.7423e-02,  3.8137e-01,  1.7332e-01,
         -1.6345e-01, -2.1103e-01,  7.1629e-01,  2.5750e-01,  3.5627e-02,
          6.4507e-02,  2.7727e-01,  1.9471e-01,  3.8165e-01, -6.3340e-02,
         -1.1610e-02,  1.0331e-01, -2.5166e-01,  1.1369e-01,  1.3616e-01,
         -8.5715e-02, -4.7593e-02,  2.3181e-02,  7.8982e-02,  2.7728e-01,
         -2.3456e-01,  1.7891e-01,  1.6118e-01,  5.6265e-01, -5.9026e-02,
          6.7995e-02,  4.8112e-01,  3.3537e-02, -1.9606e-01, -1.4543e-01,
         -3.4846e-02,  8.4357e-02,  1.4119e-01,  3.4035e-01,  1.2652e-01,
          1.7824e-01, -1.1036e-01,  1.4584e-01,  1.4241e-01, -3.6728e-01,
          2.9673e-01, -6.5008e-02, -1.7792e-01, -6.4744e-01,  7.1122e-02,
         -4.5005e-01, -6.3473e-02, -8.3637e-02, -2.2929e-01, -1.9489e-01,
          5.2128e-01,  2.7212e-01, -1.6711e-01,  3.1961e-01,  1.9182e-01,
         -2.7846e-01, -2.0669e-01, -1.7859e-02,  1.3496e-01,  2.2036e-04,
         -1.2825e-02,  1.4009e-01, -1.8348e-01, -1.9538e-02,  2.9448e-01,
          1.1876e-01, -1.8565e-01, -2.7834e-01,  9.2998e-02,  1.8210e-01,
         -1.6744e-01,  3.5806e-01, -3.3464e-01, -2.3949e-01, -3.0726e-01,
         -2.0189e-01,  2.8552e-01,  3.1935e-01,  1.0487e-03, -1.6228e-01,
         -3.2675e-01,  1.0054e-01, -5.5093e-02,  1.0689e-01,  1.8268e-01,
          6.5413e-01,  1.4863e-02,  1.0524e-01,  6.8913e-02, -5.5311e-02,
         -6.7372e-01, -2.8805e-02,  1.2949e-01, -6.4090e-02, -2.4740e-01,
          1.2077e-01, -2.8989e-01, -4.8750e-01, -2.0493e-01, -1.6950e-01,
          8.2655e-02, -2.6812e-01,  7.2235e-02,  5.2746e-01,  2.2630e-01,
         -1.5080e-01,  2.2148e-01,  2.4285e-01, -2.4154e-01,  1.3588e-01,
          1.5278e-01,  1.2159e-01,  4.4344e-01,  1.1743e-01,  5.8882e-01,
          2.6687e-01,  8.9309e-02, -3.0352e-01,  1.8430e-01,  1.5258e-01,
         -5.3584e-03,  2.6823e-01, -1.9828e-01,  4.4323e-01, -1.6332e-01,
         -1.1981e-01, -2.2364e-01,  1.7014e-01, -5.6279e-03,  3.2519e-01,
         -3.9537e-01, -1.0026e-01,  1.5421e-01, -4.4080e-01,  2.6729e-01,
          1.3846e-01,  7.1264e-02, -2.1388e-02,  7.1080e-02, -1.4538e-01,
         -1.7818e-01,  1.3680e-01, -3.4089e-01, -3.3686e-01, -1.4686e-01,
         -2.7029e-02,  1.4876e-01,  4.2135e-01,  2.5700e-01,  2.1334e-01,
         -2.0385e-01, -1.7683e-01,  2.1703e-01,  1.8844e-01,  5.1934e-01,
         -5.0731e-01,  3.5084e-01,  2.2895e-01, -1.0542e-01, -7.0393e-02,
         -3.2770e-02, -1.5533e-02, -8.7638e-02, -2.0036e-01, -1.1569e-01,
          7.1878e-02, -2.8945e-01,  3.9655e-01, -1.0306e-01, -1.6200e-01,
         -1.5512e-02, -2.2880e-01,  2.1399e-01,  8.2223e-02, -1.6934e-01,
          3.3178e-01, -1.2792e-01, -2.4594e-01, -1.4765e-01, -4.9669e-01,
          1.3923e-01,  1.0103e-01,  2.0565e-01,  1.9673e-01,  7.7407e-02,
         -1.5792e-01, -3.9293e-01, -9.7185e-02,  1.2292e-01,  2.1134e-01,
          1.3112e-02,  2.9309e-01, -8.6408e-02,  3.4463e-03,  1.3754e-01,
         -1.9684e-01, -4.9412e-02,  2.2253e-02, -2.0452e-02,  4.3296e-02,
          2.3195e-01, -8.4742e-02, -5.9899e-02, -1.8439e-01, -3.6151e-01,
         -5.2466e-02,  5.5308e-02,  6.9297e-03, -5.7832e-02, -1.8107e-01,
         -7.7534e-02,  6.3798e-02,  4.5803e-02, -2.3780e-01, -9.6592e-02,
          5.4510e-02,  9.1516e-02, -1.8262e-01,  1.4476e-02,  8.7603e-02,
         -5.7681e-02, -2.5398e-01, -1.8496e-01,  4.3419e-01,  1.5041e-01,
         -1.2563e-01,  3.9184e-01, -2.0214e-03, -1.3965e-01,  2.2638e-01,
          1.1868e-01, -1.9263e-02, -1.6716e-02,  6.7268e-02,  1.1776e-02,
         -1.1068e-01,  2.5397e-02, -1.1597e-01, -3.0629e-01,  4.7637e-01,
          2.3117e-01,  4.8435e-01, -2.7454e-01, -1.0968e-01, -3.7092e-01,
         -8.6995e-02, -2.9895e-01,  9.1296e-02, -7.0838e-02, -2.3363e-02,
         -1.8021e-02, -2.5922e-01,  5.2879e-01,  4.2179e-01,  2.6428e-01,
          1.2244e-01, -1.2097e-01, -3.0402e-02,  2.6021e-01,  1.6267e-01,
         -7.8106e-02,  2.9830e-01, -1.5302e-01, -1.1700e-01, -1.3019e-01,
         -8.8285e-02,  1.0100e-01,  6.9860e-03,  1.4498e-01, -3.9831e-01,
          1.8343e-01, -1.5648e-01, -4.5609e-01, -6.1509e-01,  2.9420e-01,
         -2.1191e-01,  3.8063e-02, -1.4968e-01, -1.0157e-01,  2.0312e-02,
          3.8054e-01,  6.7332e-02,  4.3557e-01,  2.4229e-01,  9.8919e-02,
          8.6727e-02,  2.4488e-01, -1.1277e-01,  4.6873e-01,  2.1173e-01,
          3.3462e-01, -1.4305e-01,  3.3697e-02, -1.0102e-01, -9.3691e-02,
          4.1019e-01, -2.4722e-01, -6.8807e-02, -6.6915e-02,  2.5321e-01,
          1.7086e-01, -1.4336e-01,  3.0811e-01, -5.7445e-02,  4.4875e-01]],
       device='cuda:0')]
Traceback (most recent call last):
  File "/vol/bitbucket/al4616/miniconda3/envs/simnmt/bin/nmtpy", line 7, in <module>
    exec(compile(f.read(), __file__, 'exec'))
  File "/vol/bitbucket/al4616/simmt/bin/nmtpy", line 100, in <module>
    stranslator()
  File "/vol/bitbucket/al4616/simmt/nmtpytorch/stranslator.py", line 129, in __call__
    self.translate(input_)
  File "/vol/bitbucket/al4616/simmt/nmtpytorch/stranslator.py", line 125, in translate
    translator.run_all()
  File "/vol/bitbucket/al4616/simmt/nmtpytorch/translators/verbose_waitk_greedy.py", line 21, in run_all
    hyps, actions, attentions, up_time = self.run(int(k))
  File "/vol/bitbucket/al4616/simmt/nmtpytorch/translators/verbose_waitk_greedy.py", line 99, in run
    print(self.model.dec.history['alpha_src'].data.shape)
AttributeError: 'list' object has no attribute 'data'

