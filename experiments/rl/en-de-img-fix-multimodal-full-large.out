-------
[train]
-------
         num_workers:0
          pin_memory:False
                seed:1593052642
               gclip:1
              l2_reg:1e-05
            patience:30
           optimizer:adam
                  lr:0.0004
            lr_decay:plateau
     lr_decay_revert:False
     lr_decay_factor:0.5
   lr_decay_patience:2
        lr_decay_min:1e-06
          model_type:SIMRL
            momentum:0.0
            nesterov:False
           disp_freq:30
          batch_size:6
          max_epochs:100
      max_iterations:1000000
        eval_metrics:bleu,loss
        eval_filters:
             de-hyphen
     eval_batch_size:32
           eval_freq:0
        eval_max_len:100
          eval_start:1
           eval_zero:False
   save_best_metrics:True
           save_path:/data/pranava/andy_multimodal/experiments/simnmt/nmtpy/w2w_models/en-de
    save_optim_state:False
     checkpoint_freq:5000
       n_checkpoints:0
     tensorboard_dir:/data/pranava/andy_multimodal/experiments/simnmt/nmtpy/w2w_models/en-de/tb_dir
     pretrained_file:
   pretrained_layers:
       freeze_layers:
          handle_oom:False
           subfolder:en-de-simrl-multimodal-full-large
              exp_id:simrl-r43eba
------
[vars]
------
                  sl:en
                  tl:de
-------
[model]
-------
            att_type:mlp
      att_bottleneck:hid
             enc_dim:320
   enc_bidirectional:False
             dec_dim:320
             emb_dim:200
         dropout_emb:0.4
         dropout_ctx:0.5
         dropout_out:0.5
          n_encoders:2
            tied_emb:2way
             max_len:None
      sched_sampling:0
            dec_init:zero
            bos_type:emb
           out_logic:deep
     translator_type:srlgs
           direction:src:Text, image:Numpy -> trg:Text
        sampler_type:bucket
           bucket_by:src
              splits:val,test
------
[data]
------
            txt_root:/data/pranava/andy_multimodal/simmt/data/multi30k/en-de
            img_root:/data/ozan/datasets/multi30k/features/conv/resnet50-256x256
    pretrained_model:/data/pranava/andy_multimodal/simultaneousMMT/models/resnetende/simultaneousnmt-r5c723-val028.best.loss_1.842.ckpt
           train_set:
                   src:/data/pranava/andy_multimodal/simmt/data/multi30k/en-de/train.lc.norm.tok.en
                 image:/data/ozan/datasets/multi30k/features/conv/resnet50-256x256/train-resnet50-res5c_relu-r256-c256-l2norm.npy
                   trg:/data/pranava/andy_multimodal/simmt/data/multi30k/en-de/train.lc.norm.tok.de
             val_set:
                   src:/data/pranava/andy_multimodal/simmt/data/multi30k/en-de/val.lc.norm.tok.en
                 image:/data/ozan/datasets/multi30k/features/conv/resnet50-256x256/val-resnet50-res5c_relu-r256-c256-l2norm.npy
                   trg:/data/pranava/andy_multimodal/simmt/data/multi30k/en-de/val.lc.norm.tok.de
test_2016_flickr_set:
                   src:/data/pranava/andy_multimodal/simmt/data/multi30k/en-de/test_2016_flickr.lc.norm.tok.en
                 image:/data/ozan/datasets/multi30k/features/conv/resnet50-256x256/test_2016_flickr-resnet50-res5c_relu-r256-c256-l2norm.npy
                   trg:/data/pranava/andy_multimodal/simmt/data/multi30k/en-de/test_2016_flickr.lc.norm.tok.de
test_2017_flickr_set:
                   src:/data/pranava/andy_multimodal/simmt/data/multi30k/en-de/test_2017_flickr.lc.norm.tok.en
                 image:/data/ozan/datasets/multi30k/features/conv/resnet50-256x256/test_2017_flickr-resnet50-res5c_relu-r256-c256-l2norm.npy
                   trg:/data/pranava/andy_multimodal/simmt/data/multi30k/en-de/test_2017_flickr.lc.norm.tok.de
------------
[vocabulary]
------------
                 src:/data/pranava/andy_multimodal/simmt/data/multi30k/en-de/train.lc.norm.tok.vocab.en
                 trg:/data/pranava/andy_multimodal/simmt/data/multi30k/en-de/train.lc.norm.tok.vocab.de
----------------------------------------------------------------------

Warning: unused model option: splits
Python 3.7.7 -- torch 1.4.0 with CUDA 10.1 (on machine 'lorikeet.doc.ic.ac.uk')
nmtpytorch 4.1.0
DeviceManager(cuda:0, n_gpu=1)
Seed for further reproducibility: 1593052642
Loading dataset(s)
Initializing dataset for 'src'...
0sents [00:00, ?sents/s]17668sents [00:00, 176629.63sents/s]29000sents [00:00, 123683.13sents/s]
Initializing dataset for 'image'...
Initializing dataset for 'trg'...
0sents [00:00, ?sents/s]15767sents [00:00, 157628.43sents/s]29000sents [00:00, 156139.05sents/s]
MultimodalDataset - (2 source(s) / 1 target(s))
  Sampler type: bucket, bucket_by: src
  Sources:
    --> TextDataset 'train.lc.norm.tok.en' (29000 sentences)
    --> NumpyDataset 'train-resnet50-res5c_relu-r256-c256-l2norm.npy' (29000 samples)

  Targets:
    --> TextDataset 'train.lc.norm.tok.de' (29000 sentences)

Initializing dataset for 'src'...
0sents [00:00, ?sents/s]1014sents [00:00, 155543.44sents/s]
Initializing dataset for 'image'...
Initializing dataset for 'trg'...
0sents [00:00, ?sents/s]1014sents [00:00, 53473.62sents/s]
MultimodalDataset - (2 source(s) / 1 target(s))
  Sampler type: bucket, bucket_by: src
  Sources:
    --> TextDataset 'val.lc.norm.tok.en' (1014 sentences)
    --> NumpyDataset 'val-resnet50-res5c_relu-r256-c256-l2norm.npy' (1014 samples)

  Targets:
    --> TextDataset 'val.lc.norm.tok.de' (1014 sentences)

Initializing dataset for 'src'...
0sents [00:00, ?sents/s]1014sents [00:00, 112018.97sents/s]
Initializing dataset for 'image'...
Skipping 'trg' as target
MultimodalDataset - (2 source(s) / 0 target(s))
  Sampler type: bucket, bucket_by: src
  Sources:
    --> TextDataset 'val.lc.norm.tok.en' (1014 sentences)
    --> NumpyDataset 'val-resnet50-res5c_relu-r256-c256-l2norm.npy' (1014 samples)


Ignoring batch_size 32 for simultaneous greedy search
Initializing dataset for 'src'...
0sents [00:00, ?sents/s]5725sents [00:00, 34192.00sents/s]22512sents [00:00, 84094.07sents/s]29000sents [00:00, 94759.43sents/s]
Initializing dataset for 'image'...
Skipping 'trg' as target
MultimodalDataset - (2 source(s) / 0 target(s))
  Sampler type: bucket, bucket_by: src
  Sources:
    --> TextDataset 'train.lc.norm.tok.en' (29000 sentences)
    --> NumpyDataset 'train-resnet50-res5c_relu-r256-c256-l2norm.npy' (29000 samples)


Initializing dataset for 'src'...
0sents [00:00, ?sents/s]10026sents [00:00, 100219.95sents/s]20615sents [00:00, 70122.37sents/s] 29000sents [00:00, 84245.78sents/s]
Initializing dataset for 'image'...
Skipping 'trg' as target
MultimodalDataset - (2 source(s) / 0 target(s))
  Sampler type: bucket, bucket_by: src
  Sources:
    --> TextDataset 'train.lc.norm.tok.en' (29000 sentences)
    --> NumpyDataset 'train-resnet50-res5c_relu-r256-c256-l2norm.npy' (29000 samples)


SIMRL(
  (critic_loss): MSELoss()
  (emb): Embedding(18050, 200, padding_idx=0)
  (dec_duel): MyConditionalDecoder(
    (emb): Embedding(1, 841, padding_idx=0)
    (dec0): GRUCell(841, 320)
    (do_out): Dropout(p=0.5, inplace=False)
    (hid2out): FF(in_features=320, out_features=841, activ=tanh, bias=True, bias_zero=True)
    (out2prob): FF(in_features=841, out_features=1, activ=linear, bias=True, bias_zero=True)
    (lin_class): Linear(in_features=1, out_features=1, bias=False)
    (nll_loss): NLLLoss()
  )
  (dec): MyConditionalDecoder(
    (emb): Embedding(2, 841, padding_idx=0)
    (dec0): GRUCell(841, 320)
    (do_out): Dropout(p=0.5, inplace=False)
    (hid2out): FF(in_features=320, out_features=841, activ=tanh, bias=True, bias_zero=True)
    (out2prob): FF(in_features=841, out_features=2, activ=linear, bias=True, bias_zero=True)
    (lin_class): Linear(in_features=1, out_features=1, bias=False)
    (nll_loss): NLLLoss()
  )
  (model_trans): SimultaneousNMT(
    (encoders): ModuleDict(
      (image): VisualFeaturesEncoder(
        (output): Sequential(
          (0): FF(in_features=2048, out_features=320, activ=tanh, bias=True, bias_zero=True)
          (1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (2): Dropout(p=0.5, inplace=False)
        )
      )
      (src): TextEncoder(
        (do_emb): Dropout(p=0.4, inplace=False)
        (emb): Embedding(9797, 200, padding_idx=0)
        (enc): GRU(200, 320, num_layers=2)
        (output): Sequential(
          (0): LayerNorm((320,), eps=1e-05, elementwise_affine=True)
          (1): Dropout(p=0.5, inplace=False)
        )
      )
    )
    (dec): SimultaneousConditionalDecoder(
      (emb): Embedding(18050, 200, padding_idx=0)
      (att): ModuleDict(
        (image): MLPAttention(
          (hid2ctx): Linear(in_features=320, out_features=320, bias=False)
          (ctx2ctx): Linear(in_features=320, out_features=320, bias=False)
          (ctx2hid): Linear(in_features=320, out_features=320, bias=False)
          (mlp): Linear(in_features=320, out_features=1, bias=False)
        )
        (src): MLPAttention(
          (hid2ctx): Linear(in_features=320, out_features=320, bias=False)
          (ctx2ctx): Linear(in_features=320, out_features=320, bias=False)
          (ctx2hid): Linear(in_features=320, out_features=320, bias=False)
          (mlp): Linear(in_features=320, out_features=1, bias=False)
        )
      )
      (fusion): Sequential(
        (0): Fusion(type=sum, adaptor=<function Fusion.__init__.<locals>.<lambda> at 0x7f005df408c0>, activ=None)
      )
      (dec0): GRUCell(200, 320)
      (dec1): GRUCell(320, 320)
      (do_out): Dropout(p=0.5, inplace=False)
      (hid2out): FF(in_features=840, out_features=200, activ=tanh, bias=True, bias_zero=True)
      (out2prob): FF(in_features=200, out_features=18050, activ=linear, bias=True, bias_zero=True)
      (nll_loss): NLLLoss()
    )
  )
  Vocabulary of 9797 items ('train.lc.norm.tok.vocab.en')
  Vocabulary of 18050 items ('train.lc.norm.tok.vocab.de')
  # parameters: 9.26M (9.26M learnable)
  
)
Vocabulary of 9797 items ('train.lc.norm.tok.vocab.en')
Vocabulary of 18050 items ('train.lc.norm.tok.vocab.de')
# parameters: 15.65M (15.65M learnable)

/data/ozan/datasets/multi30k/features/conv/resnet50-256x256/train-resnet50-res5c_relu-r256-c256-l2norm.npy
/data/ozan/datasets/multi30k/features/conv/resnet50-256x256/val-resnet50-res5c_relu-r256-c256-l2norm.npy
/data/ozan/datasets/multi30k/features/conv/resnet50-256x256/val-resnet50-res5c_relu-r256-c256-l2norm.npy
/data/ozan/datasets/multi30k/features/conv/resnet50-256x256/train-resnet50-res5c_relu-r256-c256-l2norm.npy
/data/ozan/datasets/multi30k/features/conv/resnet50-256x256/train-resnet50-res5c_relu-r256-c256-l2norm.npy
[('emb.weight', Parameter containing:
tensor([[-3.3220e-02, -1.3540e-02,  8.7879e-02, -3.9815e-03, -4.1526e-02,
         -6.8783e-02,  3.0300e-02, -1.1471e-01, -2.1936e-02, -1.0918e-02,
         -5.3750e-02, -2.3342e-02,  5.6760e-03, -6.3271e-02,  4.6753e-02,
         -1.1532e-01, -1.9617e-02,  7.1769e-02, -5.3664e-02,  4.2685e-02,
          9.5119e-02,  2.0701e-02,  6.4554e-03, -8.9570e-03,  4.4583e-02,
          1.0178e-02, -1.9685e-03,  2.9987e-02,  1.5978e-02, -9.2294e-03,
         -3.1164e-02,  3.8243e-02,  5.5277e-02,  3.8354e-02,  8.7520e-02,
          7.6882e-04,  2.6724e-02,  1.5487e-02,  7.6203e-02, -4.6351e-02,
          1.5320e-02, -5.7197e-02, -6.2117e-02, -1.8750e-02, -8.1071e-03,
         -1.0242e-02, -5.3516e-02, -1.2232e-02,  1.8836e-02,  3.0971e-02,
          3.5453e-02, -4.0346e-02, -3.4831e-02, -9.6826e-02,  1.0134e-01,
         -2.7497e-02,  2.9671e-02,  4.6415e-02,  1.5374e-02, -8.5946e-02,
         -5.0721e-02, -2.5300e-02, -4.7455e-02, -1.1155e-02, -1.7848e-02,
         -4.7164e-02,  4.5491e-03,  1.7567e-03, -1.4999e-01, -4.4030e-02,
          5.7155e-03,  5.2250e-02,  1.9214e-02, -5.9549e-02,  7.6223e-03,
          1.3129e-01, -7.5827e-02,  8.1297e-02, -1.0456e-02, -1.8485e-02,
         -1.0787e-01, -3.6039e-02,  6.3825e-02,  4.4908e-02, -3.5287e-02,
          3.7999e-02,  7.2461e-02,  4.3619e-02,  2.2528e-02, -1.3682e-02,
          6.6910e-02,  4.8827e-02,  8.5472e-02, -3.2669e-02,  1.5326e-02,
         -8.0311e-02, -4.9665e-02, -3.1498e-02, -1.6115e-02, -3.6104e-02,
         -4.0826e-02,  3.0298e-02, -9.7574e-02,  5.7319e-02, -6.7110e-02,
         -2.2374e-02, -2.5987e-02,  5.4386e-02, -1.8162e-02, -2.9437e-02,
          2.6572e-02,  1.6223e-02, -2.2360e-03,  4.3163e-02,  5.2423e-03,
         -7.7634e-02, -2.7068e-02,  2.9426e-02, -3.7529e-02, -3.1387e-02,
          5.4525e-02, -5.4761e-02,  4.5949e-03, -5.1268e-02, -6.3711e-02,
          2.0357e-02, -2.6369e-02,  1.9988e-02, -4.9321e-02,  3.7588e-03,
         -2.2735e-02, -3.6474e-02, -2.3256e-02,  6.1933e-02, -3.3278e-03,
          3.3459e-02, -2.1184e-02, -6.2521e-02, -4.2653e-03, -2.8549e-02,
         -4.4324e-02, -7.4884e-03, -9.4095e-03,  1.2457e-02,  4.0086e-02,
         -5.2418e-02, -4.8020e-02, -3.2990e-02, -6.1052e-02, -1.2614e-02,
          3.1651e-04, -3.2981e-02, -5.9716e-03,  5.6526e-02,  1.5613e-02,
         -2.0691e-04,  1.5100e-02,  1.8854e-02,  2.9068e-02, -1.4503e-02,
          2.7660e-02, -1.5061e-02, -4.1633e-03, -4.0367e-02, -2.8708e-02,
         -9.0668e-03, -1.1933e-02, -1.0106e-02, -4.7767e-02, -1.2469e-02,
         -9.1836e-02, -6.0757e-02, -7.8029e-02,  9.4511e-02,  7.5291e-02,
          2.0049e-02,  1.0648e-02, -4.1285e-02, -2.6473e-02, -5.3074e-03,
          1.9102e-02, -1.7050e-03, -4.0941e-02, -7.4513e-02,  1.2438e-03,
          3.6556e-02,  7.0535e-02, -2.2743e-02,  7.0885e-02, -2.8753e-02,
          1.1275e-02,  1.7834e-02,  5.6742e-02, -2.7741e-02,  6.8133e-02,
          4.4307e-02,  7.4118e-03, -2.8117e-02, -4.0693e-02,  5.6328e-02,
         -5.1412e-02,  2.1813e-02, -4.3533e-02,  1.0456e-01,  6.7554e-02,
         -2.3352e-02,  2.0016e-02,  4.0334e-02, -4.3353e-02,  5.1825e-03,
          3.2391e-02,  6.0251e-02, -5.0504e-02, -1.6228e-01,  6.0748e-04,
          2.1576e-02,  3.2052e-02, -5.0731e-02,  7.4567e-02, -7.7591e-02,
         -2.6496e-04,  8.9972e-02, -6.3001e-02,  6.8752e-03,  3.2496e-02,
          2.6147e-02,  9.5385e-03, -5.5201e-02, -3.4160e-02, -8.5552e-03,
          2.1590e-02,  2.6112e-02, -1.5711e-02,  4.0764e-02,  5.6502e-02,
          3.2401e-02,  5.9133e-02, -9.3275e-03, -3.1453e-02, -1.7397e-02,
          2.6958e-02,  2.9592e-03, -1.3536e-02,  3.7315e-02, -6.8022e-05,
          1.0281e-02,  3.9454e-02, -5.4482e-03, -4.0780e-02, -4.7698e-02,
         -2.5730e-02, -5.3817e-02, -5.1039e-02, -2.1147e-02,  1.7342e-02,
          1.5329e-02, -4.4289e-02, -1.7604e-02,  4.6699e-02, -8.8520e-02,
         -6.5518e-02,  8.0354e-02, -5.6833e-02, -3.4322e-02,  3.6448e-02,
          3.4772e-02, -3.0478e-02,  6.6419e-02,  1.5443e-02,  2.5381e-02,
          5.2979e-02,  3.7371e-02,  1.3026e-02,  6.5229e-02, -3.4066e-02,
         -1.3983e-02,  8.2992e-02,  2.7465e-02,  2.6145e-02,  1.3781e-02,
         -2.9761e-05, -5.6281e-02,  7.5200e-02, -1.2154e-02, -4.3956e-02,
          1.1570e-01, -2.6597e-02,  8.1043e-02,  7.2511e-03, -7.7681e-02,
         -9.1024e-03,  5.3024e-02,  1.5889e-02,  2.8596e-02, -1.3398e-03,
         -6.4291e-03, -1.1189e-02,  7.1082e-02, -5.0265e-02,  3.8954e-02,
         -5.7968e-02,  9.1038e-02,  5.9122e-02, -6.3080e-02, -6.3429e-02,
         -1.3489e-02,  8.6722e-03, -9.2608e-02, -3.7818e-02,  3.4015e-02,
         -6.0290e-04,  3.6300e-02,  6.0467e-02, -4.7975e-02,  3.4312e-02,
          5.5343e-02,  1.2431e-02,  1.0197e-02,  4.0925e-02, -8.9972e-03,
          4.4092e-02,  5.8568e-02, -1.8849e-02, -6.7950e-02, -4.6844e-03,
          1.9383e-02,  5.6478e-02, -8.1726e-03,  3.3021e-02,  5.5996e-02,
          6.8118e-02, -1.5950e-02, -5.3842e-02, -3.0965e-02, -6.3456e-02,
         -4.0621e-02,  3.2075e-02, -8.2028e-02, -7.0316e-03, -7.2870e-03,
          5.4766e-02,  4.7991e-05,  1.2996e-03, -2.5281e-03, -2.1377e-02,
         -1.3130e-02, -4.8867e-02, -1.4588e-02,  3.5649e-02, -3.2783e-02,
          9.6358e-02, -1.9091e-02,  1.5637e-02, -6.6012e-02, -2.7869e-02,
          1.5356e-02, -1.0676e-02,  2.9156e-02, -2.0592e-02, -8.6788e-02,
         -3.7012e-02, -3.8811e-02,  1.9386e-02, -4.5981e-02,  3.6581e-02,
         -2.2564e-03,  5.0301e-05,  2.8187e-02,  1.2247e-01, -2.9445e-02,
         -1.2735e-01, -5.9334e-02, -3.8156e-02,  3.1808e-04, -7.0872e-03,
         -4.2791e-02,  3.3918e-02,  1.5752e-02,  5.1455e-02, -7.7627e-04,
         -1.5551e-02, -2.2872e-02, -1.0189e-01,  6.6245e-02, -4.1209e-02,
          4.3375e-02, -5.7028e-02, -1.8859e-02,  1.2854e-03, -1.5578e-02,
         -3.4341e-02,  7.1741e-03,  4.6150e-02, -5.1822e-02, -1.5073e-02,
         -1.4555e-03,  1.8911e-02, -2.8126e-02, -2.7659e-02,  1.0723e-02,
         -5.5044e-02,  1.3524e-02, -1.5451e-02, -1.9964e-04, -3.1556e-03,
          4.5526e-02,  6.9061e-02, -6.6267e-02, -2.3536e-02,  2.4877e-02,
          4.2086e-02, -2.0515e-02,  1.4860e-02,  5.3668e-02, -2.8204e-02,
          4.8409e-02, -4.9759e-02, -5.9100e-02,  1.1392e-01,  2.9994e-02,
          8.1920e-03, -3.0931e-02,  2.6360e-03,  3.5131e-02, -1.9298e-02,
         -3.3540e-03,  1.0878e-02,  3.5271e-02, -6.8995e-02, -8.5444e-02,
         -1.4109e-02, -4.4061e-02, -4.4616e-02, -3.5193e-02,  7.7455e-03,
          4.6912e-02, -1.3988e-02, -4.2545e-02,  6.9640e-02, -5.3330e-02,
         -1.0865e-02, -8.4426e-02, -3.1043e-02,  2.5813e-02,  3.2476e-03,
         -4.9848e-03, -2.9256e-02,  4.5981e-02,  3.2285e-02, -4.2276e-02,
          7.2726e-02, -1.1260e-01, -1.2222e-02, -6.4115e-02, -3.8876e-02,
          2.2029e-02,  1.0354e-02,  5.2359e-02,  2.7023e-02,  6.9611e-03,
         -1.2394e-02,  1.3376e-02, -1.6402e-02, -1.9109e-03, -5.0876e-02,
         -3.0039e-02, -1.7037e-02,  2.2394e-02,  4.2892e-02, -4.3506e-02,
         -2.0274e-02,  5.9977e-02, -1.5311e-02,  1.7643e-02,  5.6114e-02,
          4.3901e-02, -2.0167e-02, -5.6940e-02, -4.0823e-02,  1.5593e-02,
         -7.8317e-02, -2.7257e-02,  9.0223e-02,  2.0693e-02,  4.9281e-02,
          3.4140e-02,  3.7282e-02,  7.4023e-03,  1.9854e-02, -5.4086e-03,
          5.5630e-02,  2.3163e-02,  1.6568e-02, -1.2171e-01,  1.9626e-02,
          1.8039e-02,  8.6058e-03, -6.9908e-02, -1.1256e-01,  3.7311e-02,
         -2.3544e-03,  1.3913e-02, -2.3210e-02,  4.8397e-02,  1.5221e-02,
          3.0050e-02,  1.0290e-01, -6.6247e-02, -1.3580e-02,  1.0975e-02,
         -1.0534e-01, -9.8397e-02,  1.2888e-02,  8.8302e-02,  3.7018e-02,
          8.9953e-02, -2.4761e-03,  3.9248e-02,  1.0494e-01,  5.6917e-02,
          3.1959e-02, -2.1167e-02, -3.2764e-02, -2.0564e-02, -7.1025e-02,
          3.0145e-02, -2.2404e-02,  9.1936e-02,  4.6415e-02, -2.2555e-02,
          1.7216e-02, -7.8162e-03, -1.0523e-01, -1.2457e-02, -3.4773e-03,
          1.1554e-02,  5.1877e-02,  5.8051e-03, -1.2020e-01,  1.0021e-01,
         -9.1679e-02,  4.3712e-03,  1.9123e-02,  7.3908e-02,  1.5460e-02,
          3.3174e-02, -1.3487e-03, -5.2849e-02, -5.1157e-03,  9.7458e-02,
         -6.1279e-02,  2.0743e-02, -1.2725e-02,  5.8980e-02,  7.6999e-03,
          1.3749e-02, -4.1378e-02, -4.9926e-02,  7.6832e-02, -2.0307e-03,
         -4.1705e-03, -5.5531e-02,  1.8081e-02,  4.1454e-02,  9.8415e-03,
          2.9815e-02,  2.2717e-02,  3.4241e-02, -9.7573e-02, -1.4108e-02,
          6.9098e-02,  5.7561e-04, -6.5715e-02, -2.0335e-03, -3.9968e-02,
         -1.7458e-02, -5.3896e-02, -2.0134e-02,  7.2692e-02,  1.6578e-03,
          8.1592e-02, -3.7574e-02,  4.3041e-02, -2.7347e-02, -9.5432e-03,
         -2.2093e-02,  7.7609e-02, -4.4355e-02, -3.6837e-02,  6.5582e-03,
         -6.1191e-02,  4.1726e-02, -2.1531e-02, -4.0582e-02, -4.6539e-03,
          4.7664e-02,  1.5759e-02,  3.9171e-03,  8.8673e-03,  1.9177e-02,
         -2.2488e-02,  1.4852e-01,  1.0061e-01, -3.2133e-02,  1.7098e-02,
         -4.7925e-02, -1.7099e-02, -9.0410e-02, -3.6239e-02,  9.0771e-02,
         -2.4484e-02,  5.7022e-02,  1.7453e-02, -1.3180e-01, -3.1670e-02,
         -4.2201e-02,  3.0901e-02,  7.3275e-03, -3.2344e-02,  4.6897e-02,
         -3.4126e-02, -1.9925e-03,  5.2778e-02, -1.4323e-02, -3.5289e-02,
         -2.0815e-02, -3.2499e-02,  4.9697e-02,  5.9416e-02, -6.9826e-02,
         -6.7354e-03, -1.8202e-02,  2.8583e-02,  3.1025e-02,  4.6076e-02,
          3.5539e-03, -6.4954e-02, -3.3836e-02,  2.9338e-02,  2.0130e-02,
          6.7060e-02,  1.0502e-01,  4.3234e-02, -2.5812e-02, -1.7075e-02,
         -3.0784e-02, -2.4580e-02, -2.3515e-02,  2.1620e-02, -1.4445e-01,
         -5.3944e-02, -6.0178e-02, -2.2849e-02,  1.3785e-02, -4.8469e-03,
          1.7938e-02, -5.1854e-02,  2.9949e-02,  1.3600e-02, -1.7835e-02,
         -3.8700e-02,  2.3175e-02, -6.6178e-02,  2.1608e-02, -5.8863e-03,
         -1.2855e-02,  5.3818e-03,  4.8066e-02,  4.3208e-02, -2.2495e-02,
         -2.2997e-02,  1.0274e-02,  1.0281e-01, -1.4501e-02, -5.2301e-02,
         -8.9181e-04,  3.9922e-02,  3.5666e-02, -1.7418e-02, -1.8160e-02,
          1.2741e-02, -4.1455e-02, -4.3649e-02, -3.3090e-02,  1.6521e-02,
          2.4718e-02,  1.1319e-01,  3.5183e-02,  1.0519e-02,  2.3064e-02,
          2.3109e-02,  8.4091e-02, -2.7978e-02, -3.0472e-02,  3.6962e-02,
          2.9375e-02,  4.7526e-02, -4.1275e-02,  3.5631e-02,  6.0510e-02,
          2.7127e-03, -8.5348e-03,  4.9633e-02, -1.6724e-02,  6.2402e-02,
          2.0263e-02, -1.9427e-02, -8.6716e-02,  6.2569e-03, -2.3493e-02,
          1.6420e-02, -4.2387e-02, -3.8483e-02,  1.9715e-03, -6.3842e-02,
          1.3620e-01, -1.8164e-02,  2.9932e-02,  8.9771e-02, -5.5958e-02,
          7.7664e-03, -3.2840e-02,  3.0873e-04, -1.4270e-02,  7.2946e-02,
          5.9461e-02,  2.1785e-03, -2.0534e-02, -2.5789e-02, -6.2629e-02,
         -4.1412e-02,  2.5210e-02,  3.9989e-02, -5.3189e-02,  2.5131e-02,
         -2.2851e-02, -3.8521e-03,  4.0976e-02, -1.2447e-02, -1.7232e-02,
          4.5220e-02, -9.8408e-02,  2.5742e-02, -7.1947e-02,  5.0735e-02,
          4.8403e-03, -7.8501e-02, -9.5824e-02, -5.2915e-03,  1.1694e-01,
          1.1935e-02,  9.1700e-03, -1.2329e-01,  1.7298e-02, -3.3559e-03,
          3.3387e-03, -7.7329e-02,  5.2193e-02, -4.9664e-02,  4.2926e-02,
         -6.0099e-02, -3.8860e-02,  8.8961e-03,  1.6919e-03,  8.4022e-03,
          2.8851e-02, -4.4423e-02, -2.3850e-02,  6.0157e-04,  2.5674e-02,
          2.1597e-02,  9.5635e-03,  5.5395e-02,  1.4175e-02,  3.4191e-02,
         -8.0231e-03, -5.8763e-02,  1.8161e-02,  3.5994e-02,  3.1775e-02,
         -2.8855e-02, -5.1615e-02, -5.0182e-02, -6.9285e-04, -3.6851e-02,
         -1.5710e-02,  6.6641e-02,  4.8690e-02, -1.3180e-02, -7.9267e-03,
          6.1235e-02,  1.9084e-02, -1.8994e-03,  6.5909e-02,  1.1944e-02,
         -4.6104e-02,  2.1447e-02,  2.1736e-02,  3.5320e-03, -1.4571e-02,
         -5.8193e-02,  7.4381e-02, -5.2391e-02,  1.0417e-02, -1.5830e-02,
         -1.1517e-01, -4.2975e-02,  6.6985e-02,  4.4141e-03,  5.7853e-02,
         -2.9639e-02, -2.9478e-02, -5.9328e-03, -4.7630e-02,  8.6268e-03,
          7.9901e-02, -8.2793e-02, -2.5532e-02,  2.7470e-02,  5.0020e-03,
          2.0366e-03, -7.0233e-03,  2.1190e-02,  6.2222e-02,  4.7300e-02,
          4.3109e-02, -4.3578e-02, -7.1955e-03,  3.9367e-02,  1.6392e-02,
         -7.8624e-02,  9.5083e-02, -1.6794e-02,  5.3744e-02, -9.5537e-02,
         -5.7244e-02,  6.5662e-02, -5.8464e-02,  5.6761e-02, -2.9469e-02,
          5.1697e-02]], device='cuda:0', requires_grad=True)), ('dec0.weight_ih', Parameter containing:
tensor([[ 0.0058, -0.0459, -0.0012,  ..., -0.0250, -0.0230,  0.0584],
        [-0.0235, -0.0024,  0.0210,  ...,  0.0705, -0.0866, -0.1013],
        [-0.0113, -0.0091, -0.0775,  ...,  0.0014, -0.0646,  0.0361],
        ...,
        [-0.0504,  0.0578, -0.0377,  ..., -0.0330, -0.0570,  0.0173],
        [-0.0405, -0.0886,  0.1075,  ..., -0.0138,  0.0413,  0.0531],
        [-0.0023, -0.0445, -0.0194,  ..., -0.0186,  0.0572,  0.0830]],
       device='cuda:0', requires_grad=True)), ('dec0.weight_hh', Parameter containing:
tensor([[-0.1200,  0.0755, -0.0781,  ...,  0.0268, -0.1357, -0.0187],
        [ 0.0182, -0.1695, -0.1550,  ..., -0.1768,  0.0486, -0.0225],
        [ 0.0394,  0.0475, -0.0325,  ..., -0.0385,  0.0790,  0.0064],
        ...,
        [-0.0365,  0.0581, -0.0497,  ...,  0.0393, -0.0539, -0.0442],
        [ 0.0370,  0.1606, -0.0151,  ..., -0.0081, -0.1575,  0.0291],
        [-0.1336, -0.0155,  0.0695,  ...,  0.0062,  0.0258, -0.0244]],
       device='cuda:0', requires_grad=True)), ('dec0.bias_ih', Parameter containing:
tensor([-5.4309e-02, -1.4946e-02, -1.6399e-03, -3.6468e-02, -5.0522e-02,
        -3.6298e-02, -1.3432e-02,  2.3981e-02,  2.9514e-02,  2.4121e-02,
         4.3088e-02,  1.7770e-02,  4.8243e-02,  7.4785e-05, -1.9271e-02,
         7.0779e-03,  5.5285e-02, -1.4542e-02,  7.1468e-03, -5.5254e-02,
        -1.8505e-02,  1.3337e-03, -1.6242e-02, -2.0955e-02,  3.0294e-02,
        -4.5094e-02,  2.7654e-02,  3.1657e-02, -5.1017e-02, -3.6088e-02,
        -7.7412e-03,  1.0272e-02, -3.5481e-02,  2.5113e-02,  5.8859e-03,
         3.6464e-02, -5.4832e-02, -2.9334e-02, -3.6850e-02,  1.2554e-02,
        -1.2382e-02,  4.4415e-02, -3.2487e-02,  2.5174e-02,  6.4686e-03,
        -1.3792e-02,  2.8871e-02, -5.5659e-02, -6.0879e-03, -4.5594e-05,
        -1.6563e-04, -4.6389e-02,  2.9865e-02, -2.2954e-02, -5.3148e-02,
        -3.2781e-02, -2.0225e-02, -3.8480e-02, -5.0584e-02,  4.1806e-02,
         3.7261e-02, -2.1603e-02,  5.0948e-02, -3.4332e-02, -4.7438e-02,
         1.1339e-02,  3.4303e-02, -3.1944e-02,  2.7352e-02, -3.3231e-02,
        -3.8448e-02,  6.1947e-03, -1.2967e-02, -2.6871e-03, -2.4410e-02,
        -9.8298e-03,  5.3280e-02, -2.1815e-02,  4.0488e-02,  2.4380e-03,
         3.1329e-02,  2.7641e-02,  4.9450e-02, -1.7224e-02, -1.0726e-02,
         2.9148e-02,  2.6487e-02,  1.4901e-02, -1.5120e-02,  1.3389e-02,
         4.3326e-02, -2.2878e-02,  4.2315e-02, -4.7471e-02,  4.9224e-02,
        -2.8293e-02, -2.0060e-02, -4.2656e-02,  3.1034e-02, -1.7174e-02,
         9.0925e-03,  3.7188e-02, -5.5294e-02,  2.3189e-03, -5.2409e-02,
         8.3260e-03, -1.5198e-02, -3.7528e-02, -5.4846e-02,  3.8467e-02,
        -5.1305e-02, -6.8804e-03, -2.2472e-02,  1.9664e-02, -2.8495e-02,
        -1.8223e-02, -4.2060e-02, -3.5997e-02,  2.4729e-03,  1.8716e-03,
         3.4911e-02, -3.8994e-02,  5.2008e-02,  4.1571e-03, -5.7536e-03,
        -5.3990e-02,  1.7424e-02, -7.8485e-04, -3.1794e-02,  6.9607e-03,
        -2.9781e-02, -4.8929e-02, -3.0888e-03,  2.3024e-02, -4.9607e-02,
        -2.4084e-02, -3.3772e-04,  1.0627e-02,  8.4004e-03, -5.2034e-02,
         5.0449e-02, -2.5356e-02,  7.2775e-03, -3.2001e-02, -2.0550e-02,
        -5.3509e-02,  9.5681e-03,  1.5327e-02, -2.1751e-02,  3.4336e-03,
        -1.9710e-03,  2.2018e-02, -4.7208e-02, -4.2128e-02, -1.2807e-02,
         3.4247e-04, -4.0184e-02,  3.6556e-02, -4.1619e-02,  4.8563e-02,
        -3.1234e-02,  5.0719e-02,  5.2856e-03,  2.5036e-02,  5.3424e-02,
        -3.3603e-02, -4.7689e-02,  2.3349e-02,  4.7355e-02,  1.4229e-02,
         4.5834e-02, -5.2169e-02,  4.9203e-02, -3.1913e-02, -1.1918e-02,
        -1.5188e-02, -2.2297e-02, -2.7868e-02, -3.7847e-02, -2.2005e-02,
         5.5134e-02,  1.7478e-02,  2.2544e-02, -5.2759e-02,  1.9857e-02,
        -1.2204e-02, -2.1677e-02,  1.6155e-02,  5.0899e-02,  4.3204e-02,
         1.9413e-02,  4.0169e-02,  3.8005e-02, -3.4835e-02, -3.1934e-02,
        -1.4646e-02, -1.6311e-02,  4.9878e-02, -7.4678e-03, -4.1363e-02,
         4.0428e-02,  4.2194e-02, -2.0762e-02, -3.6840e-02,  5.2102e-02,
        -5.5435e-02, -2.6266e-02,  4.6248e-02, -3.4181e-02,  9.1549e-03,
         1.3261e-02, -3.4780e-02,  4.8605e-03, -1.2472e-02,  2.1696e-02,
         3.4948e-02,  1.7455e-02, -1.6905e-02, -2.3331e-02, -3.9867e-02,
         4.1592e-02,  3.5380e-02,  3.2564e-02, -2.7806e-02, -1.8603e-02,
        -4.5079e-02,  1.2840e-02,  5.3290e-02, -1.8046e-02, -1.8053e-02,
         3.7405e-02,  1.9208e-03, -2.8768e-02,  1.4679e-02,  5.5316e-02,
        -1.5845e-02,  4.5277e-02, -3.3705e-02,  1.2441e-02, -5.5434e-02,
         2.6660e-02,  2.0125e-02,  3.5822e-03, -4.4693e-02,  5.1031e-02,
        -3.6207e-04, -2.0722e-02, -5.3050e-02,  7.9943e-03, -4.1124e-02,
         8.6874e-03,  2.0894e-02, -4.0365e-02, -4.2693e-02, -3.5820e-02,
         2.4100e-02, -1.9830e-02, -5.1827e-02,  2.7828e-02, -4.3216e-02,
         3.4828e-02,  3.9372e-02,  1.6709e-02, -2.8324e-02,  4.3154e-02,
        -1.9679e-02,  3.8320e-02,  3.1966e-02,  4.2652e-03, -3.2937e-02,
         1.8913e-02,  8.4338e-03, -4.4830e-02,  2.3378e-02, -3.6876e-02,
        -4.3102e-02, -1.7885e-02, -3.1907e-02, -4.0465e-02, -1.7503e-02,
         3.2472e-03, -1.9207e-02,  7.3811e-03, -1.0813e-02, -4.8510e-02,
        -2.2869e-02,  5.8178e-03, -2.5231e-02,  1.3390e-02, -3.5574e-02,
         2.0797e-02,  4.0756e-02, -2.9843e-02, -1.6295e-02,  1.0702e-02,
         4.7063e-03, -3.3534e-02, -2.5251e-02,  9.7976e-04,  5.0793e-02,
         5.0925e-02, -1.4990e-02,  3.0496e-02,  5.3629e-02, -4.3260e-02,
         4.1025e-02, -1.1848e-02, -1.0220e-02,  1.4875e-02, -2.3029e-03,
        -1.9229e-03, -2.5873e-02,  3.1694e-02,  3.0709e-02,  3.5820e-02,
         3.3616e-02, -6.4012e-03,  5.2213e-02, -1.2549e-02, -1.7124e-02,
         4.9178e-02, -1.6193e-02, -3.0278e-02,  3.9906e-02, -4.2326e-02,
         3.6487e-02, -2.9788e-02,  4.7670e-02, -2.2705e-02, -3.0498e-02,
         5.0538e-02, -7.2046e-03, -5.5112e-02,  3.3374e-03, -2.0307e-02,
        -3.6637e-02,  5.0106e-02,  2.8734e-02, -2.0816e-02,  2.4166e-02,
         1.4855e-02,  7.2022e-03, -4.3748e-02, -5.5835e-03, -1.6656e-02,
         4.3597e-02, -4.2425e-02, -4.6390e-02,  2.5940e-02,  2.8407e-02,
         1.0500e-02,  4.4916e-02, -2.6788e-03,  1.0197e-02, -3.0741e-02,
         1.6486e-02, -5.0991e-02, -1.3306e-02,  5.9196e-03,  1.9266e-03,
        -2.7535e-02, -2.9334e-02, -1.5230e-02,  5.2821e-02,  5.1914e-02,
         7.8964e-04, -3.2650e-02, -4.9375e-02, -1.9630e-02, -5.0694e-02,
        -4.0369e-03, -4.2337e-02, -2.3908e-02, -4.7591e-02,  5.0588e-02,
        -1.0199e-02, -3.0998e-02,  4.2171e-02, -9.9987e-03,  2.2064e-02,
         1.8371e-02,  5.9874e-03,  5.4601e-02, -5.5463e-02,  1.7063e-02,
         1.3770e-03,  6.0514e-03, -5.2518e-02, -2.5680e-02, -1.6086e-02,
         4.4155e-02, -5.3254e-02,  3.0950e-02, -4.9121e-02, -4.0679e-02,
        -3.6918e-02, -2.3430e-02,  4.9351e-02,  3.6649e-02, -3.4620e-02,
         3.3743e-02,  5.0360e-02,  5.3738e-02,  4.3058e-02, -2.7046e-02,
        -3.7277e-02,  3.6399e-02,  1.0520e-02,  3.8110e-03,  4.5000e-02,
        -1.3447e-02,  3.5678e-02, -2.3430e-02,  4.1333e-02, -1.8729e-02,
         5.1052e-02, -2.3705e-02,  5.5224e-02, -5.3576e-04, -2.1344e-02,
         4.8177e-02,  5.1582e-02,  2.1729e-02, -3.2752e-02,  1.3306e-02,
        -2.8855e-02,  1.3957e-02, -2.4482e-02,  5.0670e-02,  1.7011e-02,
         4.5811e-02, -1.1846e-02,  4.7195e-02,  2.3525e-02,  5.1360e-02,
         2.6245e-02,  1.1504e-02, -4.2833e-02,  1.8420e-03,  1.4047e-02,
        -2.2115e-02, -3.5156e-02,  4.9170e-02,  5.5191e-03,  1.9468e-02,
        -4.9439e-03,  2.5154e-02,  1.4662e-02,  6.1027e-03,  3.0811e-02,
         3.8143e-02,  1.8864e-02,  1.5897e-02, -1.1560e-02,  1.2648e-02,
        -2.4933e-02, -1.5773e-02,  2.5840e-04, -3.3689e-02, -4.8221e-02,
         1.2751e-02, -8.0120e-03,  4.1437e-02, -1.9243e-02, -2.1379e-02,
        -3.7154e-02,  1.2456e-02, -4.9246e-02, -2.1058e-02,  3.3620e-02,
         3.7189e-02,  2.6541e-02,  8.4010e-03, -3.0396e-02, -4.8237e-02,
         9.1956e-03, -1.5303e-02, -4.7323e-02,  2.7775e-02, -4.6300e-02,
        -1.0111e-02, -5.2291e-02, -1.4730e-02,  2.7451e-03,  1.6140e-02,
        -3.0795e-02,  4.0479e-02,  5.2880e-02, -2.1625e-02,  1.8290e-02,
         7.4574e-03,  3.9081e-02,  4.4258e-02, -2.3447e-02, -2.7727e-02,
         3.3344e-02,  1.4359e-02, -3.8615e-02, -4.2929e-02, -2.6537e-02,
         2.1366e-02,  2.3225e-04,  2.9493e-02,  8.5987e-03,  3.7600e-02,
        -2.9105e-02, -2.5573e-02,  1.4271e-02,  2.3526e-03,  2.5136e-02,
        -1.4636e-02, -5.5272e-02,  4.2971e-02, -4.5770e-02,  2.9665e-02,
        -5.8800e-04,  3.3914e-02,  2.3347e-03, -3.9669e-02, -1.3449e-02,
        -2.4357e-02,  1.0102e-02, -1.5434e-02,  1.4902e-02, -1.1696e-02,
        -3.0413e-02, -4.1342e-02, -1.0794e-02,  1.0987e-02,  4.8260e-02,
         8.8728e-03,  4.7958e-02, -1.2165e-02,  2.1563e-02,  7.0725e-03,
        -4.8990e-02,  3.1029e-02, -2.3070e-02, -2.6515e-02, -3.7805e-02,
        -1.0177e-02, -4.2992e-02, -4.9939e-02,  1.0977e-02,  1.4165e-02,
        -3.4580e-02,  5.3500e-02,  3.8549e-02,  3.4091e-02, -3.4332e-02,
        -4.0010e-02,  1.6430e-02, -2.8276e-02, -4.9295e-02, -3.5470e-02,
         4.0670e-03, -3.6558e-02, -8.0959e-03,  1.1932e-02, -3.4435e-02,
        -3.9716e-03, -4.7806e-02, -4.8394e-02, -4.8521e-02, -3.1051e-02,
        -1.2557e-02, -3.7675e-03, -3.2453e-02,  2.6596e-03, -4.1857e-02,
         5.4064e-02,  3.4304e-02, -2.6077e-07,  1.9043e-02,  5.5235e-02,
         3.3225e-03,  4.1075e-02,  5.3381e-02,  3.7604e-02,  5.3857e-02,
        -4.0571e-02, -3.4704e-02,  1.7830e-02,  1.9233e-02,  3.8336e-02,
         1.3982e-03, -4.4071e-02, -2.8321e-02, -3.8324e-02,  5.7610e-03,
         3.8554e-02,  2.6313e-02,  1.6510e-02,  4.8592e-03, -2.8484e-03,
        -2.1688e-02, -1.4240e-02, -4.5529e-02,  4.8120e-02, -4.4392e-02,
         2.7766e-02, -3.1141e-02, -7.4565e-03, -5.1816e-02,  2.5655e-02,
        -1.2226e-02,  2.3726e-02, -7.1638e-03, -4.6752e-02,  1.8722e-02,
         4.7620e-02, -3.5205e-02,  3.8684e-03, -5.4984e-02, -3.3430e-02,
         4.7670e-02, -4.8923e-02, -4.4658e-02,  6.9882e-03, -1.5610e-02,
        -3.0846e-02,  2.9974e-02, -3.5832e-02,  4.4911e-02, -1.7979e-02,
         5.1496e-02,  5.4205e-02, -6.2667e-03, -5.1526e-02, -1.5762e-02,
        -4.2539e-02, -1.4504e-02,  3.1842e-03, -4.6385e-02,  5.5004e-02,
        -4.0598e-02,  3.3860e-02, -2.6567e-02, -4.4509e-02, -4.1708e-03,
        -5.2598e-04,  1.3306e-02, -4.8485e-02, -3.5578e-02,  1.0474e-02,
        -7.3680e-03, -1.4790e-02,  2.2234e-02, -4.0024e-02, -1.8935e-02,
         3.1857e-02, -3.7633e-02, -1.8803e-02, -3.5258e-02,  2.8425e-03,
        -5.2388e-02,  4.9682e-02,  1.5517e-02,  1.4587e-02, -3.8707e-02,
        -6.7823e-03, -3.7220e-02,  2.0435e-02,  3.1776e-02,  2.0436e-02,
        -4.5246e-02,  3.6923e-02, -1.7075e-02,  2.4324e-02, -4.2579e-02,
        -1.3195e-02, -1.5108e-02,  5.3503e-02, -1.3501e-02,  1.1444e-03,
         3.3783e-02,  2.7445e-02,  2.8461e-02, -3.0753e-03, -3.4023e-02,
         4.5651e-02,  5.5889e-02, -1.3984e-02, -1.7418e-02, -3.5148e-03,
         4.5221e-02, -2.2994e-02,  1.7204e-02, -1.1931e-02, -4.3466e-02,
        -1.7893e-02, -5.3907e-02,  5.0726e-02, -6.5612e-03,  4.0540e-02,
         1.9513e-02,  4.2830e-02,  1.5891e-02, -1.7090e-02, -8.2101e-03,
         4.5515e-02,  1.8050e-02, -1.0764e-02,  5.4650e-02,  4.3986e-02,
         3.3006e-02, -2.7205e-02,  3.4794e-02, -4.3671e-02,  4.9201e-02,
         4.0944e-02,  2.9407e-02, -4.7578e-02,  4.1435e-02, -7.3907e-03,
         3.0218e-02,  3.8594e-02, -3.1234e-02, -6.2959e-03,  1.4821e-02,
         3.3500e-02,  4.4145e-02,  4.2707e-03, -1.0891e-02, -5.1995e-02,
        -3.2809e-02, -4.4984e-02,  3.1225e-02, -3.1887e-03, -3.9126e-02,
         1.1146e-02, -2.6430e-02,  5.2747e-02,  2.8081e-02, -1.8052e-02,
        -4.3397e-02, -2.7731e-02, -3.4913e-03, -5.0348e-02, -3.4389e-02,
         2.4251e-02, -1.6462e-02, -4.5622e-02, -4.1951e-02, -7.5070e-03,
         2.8600e-02,  5.1907e-02, -1.0888e-02, -4.3449e-02,  3.0802e-02,
         4.1166e-02, -1.8710e-02, -3.0941e-02, -2.9576e-02, -2.3813e-02,
        -2.6757e-02,  1.8043e-02, -2.2488e-02,  1.6522e-02,  3.0891e-02,
         4.1596e-02, -5.4306e-02, -5.4492e-02, -2.8994e-02, -5.8929e-03,
        -9.6748e-03,  1.6229e-02,  3.8996e-02, -4.1988e-02,  3.6529e-03,
         3.4716e-02, -2.1118e-02,  2.8318e-02,  1.0393e-02, -2.2001e-02,
        -4.1493e-02,  4.1744e-02, -3.6507e-02,  2.4837e-02,  1.6115e-02,
        -4.0518e-02, -1.6597e-03, -1.9945e-02, -2.1418e-02,  1.2274e-02,
        -7.7924e-03, -5.3592e-02,  1.3142e-02, -4.3217e-02,  1.0240e-02,
         3.3023e-02,  3.9816e-02,  1.6837e-03, -2.1623e-02,  3.3268e-02,
         1.7106e-02, -5.3414e-02, -8.1105e-03,  3.8304e-03,  5.1006e-02,
        -4.4144e-02, -1.6964e-02, -4.4210e-02,  4.6426e-02, -2.1621e-02,
         5.3931e-02, -2.8792e-02, -1.2599e-02,  2.1415e-02,  3.6941e-02,
         4.0636e-02,  5.0298e-02,  7.7282e-03,  2.8832e-02,  5.4693e-02,
        -2.8613e-03, -2.1310e-02,  6.3425e-03,  5.4554e-03,  1.8832e-03,
         4.3372e-02,  3.4705e-02,  1.0201e-02, -4.0777e-02,  3.6746e-02,
        -3.7747e-02, -4.9121e-02, -1.6693e-02,  3.1749e-02,  4.8964e-02,
        -4.8069e-02, -3.0887e-02, -5.2269e-02,  1.8981e-03, -3.9051e-03,
        -2.1625e-02, -2.6411e-04,  3.8739e-02,  9.5569e-03,  5.0596e-02,
        -2.6326e-03, -4.0403e-02,  1.6311e-02, -2.8794e-03, -3.5045e-02,
        -1.3504e-02,  3.0774e-02,  3.0372e-02,  3.0235e-02, -3.6605e-02,
         4.8265e-02,  2.7435e-02,  2.8095e-02, -2.6190e-02,  4.4317e-02,
        -2.5903e-02,  1.6467e-02, -9.8937e-03,  5.0593e-02,  5.1292e-02,
         2.6885e-02,  5.3316e-02, -4.0122e-02, -4.7955e-02,  2.7383e-02,
         5.5030e-02, -3.2724e-02, -5.5526e-02,  5.4825e-02,  1.7208e-02,
        -2.8065e-02,  9.0672e-03, -5.5358e-02,  2.7519e-02, -1.3587e-02,
        -8.2126e-03, -5.2688e-02, -3.8168e-02,  7.3748e-03,  1.0932e-02,
         4.1354e-02,  8.8528e-03,  1.7583e-02,  4.4527e-02, -2.3488e-02,
         5.5684e-02, -2.4581e-02, -4.3997e-02,  2.8410e-02, -1.3655e-02,
         5.3039e-02,  4.0419e-02, -1.2629e-02,  3.3619e-02,  5.3913e-02,
        -1.8596e-02,  5.9030e-04, -2.5854e-02,  3.1474e-02,  1.5603e-02,
         1.8153e-02,  5.5825e-02,  5.1224e-03, -4.0033e-02, -3.2405e-02,
         9.8496e-03,  3.3793e-02, -9.3133e-04, -1.0727e-02, -6.1879e-03,
         1.8207e-02, -2.1618e-02,  1.1559e-03,  1.3579e-02, -1.0177e-02,
         1.1719e-02,  9.5891e-03, -7.0763e-03,  4.4281e-02, -4.7358e-02,
         4.3504e-02,  3.4332e-02, -6.9664e-03,  2.5905e-02, -3.9127e-02,
         1.8582e-02, -6.3235e-04,  3.4445e-02,  3.7443e-02,  1.3344e-02,
         2.9283e-02,  1.7921e-02,  1.2603e-02, -3.9277e-03, -3.0199e-02,
        -1.2030e-02, -5.4500e-02,  3.0381e-02,  1.7405e-02, -1.7822e-02,
        -5.1542e-02, -1.7535e-02, -4.3292e-02, -1.3667e-02,  2.2191e-02,
         3.4981e-02,  1.6936e-02, -1.5624e-02, -3.9658e-02,  6.6232e-05,
         3.8547e-02, -3.8021e-02,  3.3422e-02, -2.4151e-02, -5.0210e-04,
        -2.0071e-02, -4.5920e-02, -4.2273e-02, -3.4860e-02, -3.2329e-02],
       device='cuda:0', requires_grad=True)), ('dec0.bias_hh', Parameter containing:
tensor([ 0.0397, -0.0220,  0.0181,  0.0421, -0.0067, -0.0174,  0.0492, -0.0188,
         0.0083,  0.0505, -0.0039,  0.0462,  0.0281, -0.0297,  0.0238,  0.0338,
        -0.0051, -0.0553,  0.0521, -0.0506,  0.0059,  0.0539,  0.0033, -0.0340,
         0.0174, -0.0330,  0.0347,  0.0489,  0.0411,  0.0220,  0.0527, -0.0276,
         0.0486,  0.0380,  0.0547, -0.0206,  0.0447,  0.0106, -0.0148, -0.0374,
        -0.0297, -0.0044,  0.0531,  0.0030,  0.0275,  0.0296, -0.0485, -0.0524,
        -0.0216, -0.0118,  0.0431, -0.0273,  0.0463,  0.0053,  0.0188, -0.0385,
        -0.0121,  0.0421, -0.0514, -0.0154,  0.0119, -0.0083,  0.0489,  0.0074,
         0.0484,  0.0094, -0.0015, -0.0103,  0.0156,  0.0034, -0.0327, -0.0549,
        -0.0452, -0.0065,  0.0250,  0.0071, -0.0423,  0.0163,  0.0426,  0.0456,
        -0.0208,  0.0061, -0.0262, -0.0443,  0.0135,  0.0447,  0.0223, -0.0132,
        -0.0231,  0.0127, -0.0171,  0.0281, -0.0100, -0.0214,  0.0135,  0.0048,
         0.0075,  0.0416,  0.0007,  0.0355, -0.0113, -0.0265,  0.0080,  0.0209,
        -0.0425, -0.0244, -0.0418,  0.0133, -0.0451, -0.0149,  0.0164, -0.0132,
         0.0552,  0.0324,  0.0143, -0.0527,  0.0509, -0.0239,  0.0348, -0.0052,
         0.0235,  0.0192, -0.0007, -0.0170, -0.0149,  0.0030,  0.0016, -0.0539,
         0.0266, -0.0517, -0.0232, -0.0503, -0.0471,  0.0483, -0.0226, -0.0395,
         0.0511,  0.0410, -0.0469, -0.0180,  0.0390, -0.0122,  0.0328,  0.0448,
        -0.0553, -0.0223,  0.0148, -0.0267,  0.0084, -0.0161, -0.0284,  0.0504,
        -0.0526, -0.0094, -0.0514, -0.0465,  0.0554, -0.0518, -0.0039,  0.0535,
         0.0190, -0.0559,  0.0430, -0.0075,  0.0210, -0.0452,  0.0107,  0.0201,
         0.0027,  0.0415, -0.0435,  0.0547, -0.0180,  0.0028,  0.0162,  0.0052,
         0.0408, -0.0243, -0.0392,  0.0361, -0.0231, -0.0147,  0.0349,  0.0061,
        -0.0260,  0.0410, -0.0356, -0.0210, -0.0506,  0.0376,  0.0196, -0.0345,
         0.0183, -0.0023, -0.0499, -0.0057,  0.0338, -0.0039, -0.0352, -0.0416,
         0.0296, -0.0014,  0.0491, -0.0173,  0.0063,  0.0550,  0.0057,  0.0301,
        -0.0338, -0.0263, -0.0132, -0.0442, -0.0195,  0.0168,  0.0538,  0.0521,
         0.0220,  0.0048, -0.0276,  0.0174, -0.0106, -0.0051,  0.0155, -0.0373,
         0.0298, -0.0189, -0.0187,  0.0110,  0.0509,  0.0541, -0.0412, -0.0533,
         0.0068, -0.0293, -0.0106,  0.0526, -0.0204, -0.0092, -0.0533, -0.0082,
        -0.0217,  0.0228, -0.0535, -0.0192, -0.0363,  0.0352,  0.0437, -0.0081,
        -0.0269,  0.0494, -0.0532,  0.0441,  0.0077, -0.0348, -0.0112, -0.0228,
        -0.0118,  0.0193,  0.0142,  0.0207,  0.0334, -0.0278,  0.0103,  0.0497,
         0.0019, -0.0165,  0.0043, -0.0160,  0.0283, -0.0141,  0.0158, -0.0118,
         0.0453, -0.0257, -0.0485, -0.0417,  0.0451, -0.0012,  0.0333,  0.0193,
        -0.0453, -0.0399, -0.0297, -0.0156, -0.0507, -0.0516,  0.0085, -0.0085,
         0.0310,  0.0202,  0.0251,  0.0537, -0.0251,  0.0487, -0.0173, -0.0417,
        -0.0258,  0.0398,  0.0554, -0.0550,  0.0165, -0.0435, -0.0119,  0.0370,
         0.0405,  0.0379, -0.0428, -0.0167, -0.0360, -0.0400,  0.0341, -0.0167,
        -0.0316, -0.0217,  0.0107, -0.0121, -0.0193,  0.0201, -0.0106,  0.0016,
         0.0239,  0.0007,  0.0266, -0.0507,  0.0212,  0.0302,  0.0317,  0.0485,
        -0.0207,  0.0454, -0.0304,  0.0037,  0.0164,  0.0453,  0.0303, -0.0039,
        -0.0326, -0.0073, -0.0258,  0.0474, -0.0303,  0.0119, -0.0003, -0.0293,
        -0.0345,  0.0409, -0.0350,  0.0075,  0.0409, -0.0236,  0.0243,  0.0312,
        -0.0371,  0.0524,  0.0460,  0.0419,  0.0409,  0.0067, -0.0236,  0.0354,
         0.0351,  0.0193,  0.0414,  0.0135, -0.0443, -0.0438,  0.0390,  0.0219,
        -0.0341,  0.0489,  0.0138, -0.0265, -0.0352, -0.0090, -0.0338, -0.0070,
         0.0086, -0.0383,  0.0486, -0.0155, -0.0095,  0.0094,  0.0284, -0.0448,
         0.0156, -0.0078,  0.0135,  0.0511, -0.0047, -0.0383, -0.0542, -0.0476,
         0.0379, -0.0093, -0.0149,  0.0137, -0.0055, -0.0083, -0.0045, -0.0257,
         0.0045,  0.0042, -0.0001,  0.0167, -0.0444,  0.0178, -0.0207, -0.0352,
         0.0116,  0.0077,  0.0183, -0.0286, -0.0479,  0.0513, -0.0235,  0.0101,
        -0.0332,  0.0156, -0.0180, -0.0426, -0.0549, -0.0041, -0.0029, -0.0432,
        -0.0524,  0.0470, -0.0295, -0.0329,  0.0528,  0.0521, -0.0177, -0.0490,
        -0.0364, -0.0084,  0.0220, -0.0281,  0.0400, -0.0134,  0.0134,  0.0428,
         0.0337,  0.0192, -0.0072, -0.0324,  0.0049, -0.0299, -0.0425, -0.0224,
        -0.0009, -0.0303, -0.0263, -0.0306, -0.0320, -0.0156,  0.0135,  0.0465,
         0.0539, -0.0489, -0.0390, -0.0397, -0.0272,  0.0063,  0.0195, -0.0189,
         0.0192, -0.0542, -0.0537, -0.0134,  0.0457,  0.0310, -0.0404, -0.0276,
        -0.0518,  0.0355,  0.0519,  0.0478,  0.0217,  0.0306, -0.0162, -0.0058,
        -0.0262, -0.0319,  0.0199, -0.0036, -0.0205,  0.0188,  0.0359,  0.0097,
        -0.0329, -0.0054, -0.0454,  0.0167, -0.0066,  0.0075,  0.0025,  0.0382,
        -0.0437,  0.0399,  0.0507,  0.0274, -0.0029,  0.0226, -0.0281, -0.0296,
         0.0090, -0.0232,  0.0175,  0.0546,  0.0431,  0.0453, -0.0322, -0.0197,
        -0.0136,  0.0011,  0.0254,  0.0109,  0.0305,  0.0060, -0.0093,  0.0078,
        -0.0521,  0.0494, -0.0458,  0.0085, -0.0323, -0.0297,  0.0039, -0.0225,
         0.0356, -0.0322,  0.0110, -0.0384,  0.0008,  0.0380, -0.0134,  0.0365,
        -0.0013, -0.0024,  0.0456,  0.0046, -0.0486, -0.0137,  0.0306,  0.0071,
         0.0075, -0.0246, -0.0259, -0.0134, -0.0436, -0.0162, -0.0522, -0.0172,
         0.0388,  0.0055,  0.0163,  0.0094,  0.0196, -0.0123,  0.0327, -0.0252,
         0.0458,  0.0306,  0.0454, -0.0382, -0.0038,  0.0113, -0.0024, -0.0017,
         0.0402, -0.0524,  0.0465,  0.0330,  0.0108, -0.0020, -0.0449,  0.0005,
        -0.0338,  0.0523, -0.0417,  0.0182,  0.0550, -0.0303,  0.0388,  0.0486,
        -0.0448, -0.0327, -0.0055,  0.0456, -0.0269,  0.0435,  0.0192,  0.0286,
        -0.0228,  0.0433, -0.0345, -0.0134,  0.0098, -0.0109, -0.0321,  0.0551,
        -0.0092,  0.0075,  0.0498, -0.0016,  0.0545,  0.0144,  0.0156,  0.0076,
         0.0263,  0.0400, -0.0270,  0.0458,  0.0384, -0.0461,  0.0167, -0.0359,
        -0.0084,  0.0198, -0.0402,  0.0559, -0.0221, -0.0277,  0.0321, -0.0010,
         0.0338, -0.0484,  0.0111, -0.0090, -0.0076, -0.0266, -0.0287,  0.0396,
        -0.0519,  0.0424,  0.0498, -0.0248, -0.0177,  0.0116,  0.0519, -0.0008,
        -0.0070, -0.0034,  0.0447, -0.0557,  0.0034, -0.0435, -0.0512, -0.0114,
        -0.0230,  0.0307,  0.0532,  0.0336, -0.0063, -0.0161,  0.0374,  0.0309,
        -0.0435,  0.0338, -0.0407, -0.0203, -0.0004,  0.0500, -0.0258, -0.0303,
        -0.0557,  0.0491,  0.0401,  0.0430, -0.0165, -0.0244, -0.0175, -0.0159,
        -0.0048,  0.0499, -0.0307,  0.0249,  0.0333,  0.0164, -0.0067,  0.0402,
        -0.0553, -0.0434, -0.0364, -0.0407, -0.0309, -0.0140,  0.0351, -0.0250,
        -0.0514, -0.0223,  0.0290,  0.0070, -0.0420, -0.0042, -0.0174, -0.0220,
        -0.0309,  0.0418,  0.0232, -0.0256, -0.0440,  0.0317, -0.0395,  0.0552,
         0.0127, -0.0310,  0.0439,  0.0516, -0.0492,  0.0548,  0.0088, -0.0427,
         0.0183, -0.0469,  0.0320,  0.0391, -0.0267, -0.0137, -0.0537, -0.0040,
         0.0469, -0.0184, -0.0370, -0.0082, -0.0509, -0.0285, -0.0471,  0.0047,
        -0.0139,  0.0345,  0.0143, -0.0033,  0.0198, -0.0133, -0.0019, -0.0232,
        -0.0432,  0.0191, -0.0215,  0.0284, -0.0046, -0.0112,  0.0210, -0.0209,
         0.0046,  0.0376, -0.0458,  0.0506, -0.0231, -0.0209,  0.0422,  0.0342,
         0.0452,  0.0342,  0.0354, -0.0190,  0.0429, -0.0507, -0.0477,  0.0222,
        -0.0358, -0.0058,  0.0215,  0.0125,  0.0057, -0.0223,  0.0422, -0.0066,
        -0.0536, -0.0127, -0.0415, -0.0455, -0.0172, -0.0021,  0.0375,  0.0182,
        -0.0261, -0.0493,  0.0086,  0.0545,  0.0267,  0.0265, -0.0492, -0.0430,
         0.0281, -0.0409, -0.0035,  0.0394, -0.0013, -0.0131,  0.0508,  0.0222,
         0.0445, -0.0111, -0.0086,  0.0091,  0.0493,  0.0532,  0.0321, -0.0467,
        -0.0049, -0.0428, -0.0330,  0.0351, -0.0317, -0.0072, -0.0469, -0.0090,
        -0.0421,  0.0456, -0.0254,  0.0315,  0.0480,  0.0451,  0.0381,  0.0094,
         0.0459, -0.0201,  0.0183, -0.0056,  0.0039, -0.0166,  0.0538,  0.0460,
        -0.0354, -0.0062, -0.0015,  0.0125,  0.0521, -0.0167,  0.0205, -0.0137,
         0.0437, -0.0292,  0.0412,  0.0326, -0.0463, -0.0218, -0.0123, -0.0066,
        -0.0287,  0.0035,  0.0026,  0.0086,  0.0348,  0.0248, -0.0094, -0.0542,
         0.0479, -0.0022,  0.0440,  0.0318, -0.0461, -0.0090,  0.0054,  0.0352,
        -0.0351, -0.0360, -0.0138, -0.0494,  0.0128, -0.0054,  0.0366,  0.0321,
        -0.0432,  0.0472,  0.0078,  0.0358, -0.0215,  0.0493, -0.0192, -0.0299,
         0.0186,  0.0009,  0.0304,  0.0159,  0.0305,  0.0496, -0.0385, -0.0071,
        -0.0406, -0.0056, -0.0139, -0.0443,  0.0016, -0.0100, -0.0555,  0.0297,
        -0.0082,  0.0050, -0.0128,  0.0403,  0.0115,  0.0124,  0.0250, -0.0255,
         0.0003,  0.0415,  0.0006, -0.0498,  0.0526,  0.0087,  0.0502, -0.0283,
         0.0175,  0.0097, -0.0263, -0.0487,  0.0334, -0.0069, -0.0108, -0.0340,
        -0.0558, -0.0404, -0.0472,  0.0509, -0.0380, -0.0356, -0.0285,  0.0330,
        -0.0408,  0.0288, -0.0141,  0.0521,  0.0426,  0.0035,  0.0253,  0.0110,
         0.0475, -0.0009, -0.0459,  0.0042,  0.0237,  0.0182,  0.0379,  0.0438,
         0.0150, -0.0465, -0.0427, -0.0401, -0.0482, -0.0038, -0.0206,  0.0178,
        -0.0478,  0.0111, -0.0194,  0.0302, -0.0550, -0.0002,  0.0532, -0.0175,
         0.0476,  0.0417,  0.0486, -0.0534,  0.0197,  0.0138, -0.0195, -0.0015],
       device='cuda:0', requires_grad=True)), ('hid2out.weight', Parameter containing:
tensor([[-0.0935, -0.0820,  0.0814,  ...,  0.0763, -0.0579, -0.0670],
        [-0.0077,  0.0376, -0.0140,  ..., -0.1666, -0.0245,  0.0257],
        [-0.0651,  0.0964,  0.0094,  ..., -0.1159,  0.0374, -0.0075],
        ...,
        [-0.0988,  0.0097, -0.0490,  ...,  0.1725, -0.0464, -0.1495],
        [-0.0864,  0.0060,  0.1356,  ..., -0.0397, -0.0472, -0.0114],
        [-0.0006,  0.1903,  0.0361,  ..., -0.0246, -0.0117,  0.0978]],
       device='cuda:0', requires_grad=True)), ('hid2out.bias', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.], device='cuda:0', requires_grad=True)), ('out2prob.bias', Parameter containing:
tensor([0.], device='cuda:0', requires_grad=True)), ('lin_class.weight', Parameter containing:
tensor([[-2.7573]], device='cuda:0', requires_grad=True))]
mumu
[('emb.weight', Parameter containing:
tensor([[-0.0257,  0.0463,  0.0319,  ...,  0.0551, -0.0934,  0.0524],
        [ 0.0259,  0.0194, -0.0264,  ..., -0.0183,  0.0126,  0.0357]],
       device='cuda:0', requires_grad=True)), ('dec0.weight_ih', Parameter containing:
tensor([[-0.0797,  0.0185, -0.0446,  ...,  0.0308, -0.0085,  0.0690],
        [ 0.0125,  0.0660,  0.0373,  ...,  0.0618, -0.0465,  0.0666],
        [ 0.0355, -0.0087,  0.0768,  ..., -0.0029,  0.0830, -0.0384],
        ...,
        [ 0.0682,  0.0205, -0.0439,  ..., -0.0931,  0.0480,  0.0027],
        [ 0.0024, -0.0176, -0.0371,  ...,  0.0103,  0.0205,  0.0824],
        [-0.0714,  0.0344,  0.0035,  ...,  0.0887, -0.0510,  0.0936]],
       device='cuda:0', requires_grad=True)), ('dec0.weight_hh', Parameter containing:
tensor([[-0.0356,  0.0656,  0.0214,  ...,  0.0852, -0.0186,  0.0028],
        [ 0.0592, -0.1770,  0.1208,  ..., -0.0120,  0.1367, -0.0101],
        [ 0.1023,  0.0008, -0.1353,  ..., -0.0335, -0.0125, -0.0200],
        ...,
        [ 0.0855, -0.1308,  0.0568,  ..., -0.0609, -0.1242,  0.0290],
        [ 0.0033, -0.0267,  0.0930,  ..., -0.0558, -0.0711,  0.0139],
        [ 0.1145,  0.0594,  0.0029,  ...,  0.0253,  0.1095,  0.0314]],
       device='cuda:0', requires_grad=True)), ('dec0.bias_ih', Parameter containing:
tensor([ 3.1629e-02, -3.1881e-02,  4.8915e-02,  2.4354e-02,  5.2876e-02,
         4.1881e-02,  2.8910e-02, -2.5705e-02, -1.7171e-02,  4.7037e-02,
         7.0042e-04,  4.8403e-02, -2.0437e-02, -4.0998e-02, -4.0803e-02,
        -2.8874e-02,  4.0482e-02, -4.5430e-02, -5.2194e-02,  3.0108e-02,
        -5.6550e-03,  4.3245e-02, -4.5542e-02,  1.3264e-03, -4.3821e-02,
         1.6202e-02, -4.3953e-02,  4.1187e-02,  3.7559e-02, -1.3114e-02,
         4.9519e-02, -1.8356e-02, -3.3159e-02, -4.9430e-02,  5.0876e-02,
        -5.8918e-03,  1.5450e-02,  3.8429e-02,  4.5399e-02,  2.8437e-02,
        -3.1825e-02,  2.6892e-02, -1.4605e-02,  2.9896e-02, -4.5027e-02,
        -2.1741e-03,  1.5584e-02,  5.4411e-02,  1.3820e-02,  5.2445e-02,
         2.7404e-03, -3.0248e-02, -2.1981e-02, -2.4438e-02, -1.8579e-02,
         2.6745e-02,  3.1728e-02, -3.5527e-03, -2.9333e-02, -3.4235e-02,
         1.2051e-02,  1.0228e-02,  5.3968e-02, -4.4442e-02,  2.0265e-02,
         5.3280e-02,  5.1955e-02,  1.7218e-03,  3.2964e-02, -3.1496e-02,
         1.6181e-02,  5.2699e-02, -1.9512e-02,  4.3437e-02,  2.2306e-02,
         1.2781e-02,  2.0287e-02,  5.5799e-02,  3.7309e-02, -2.6828e-02,
         3.4628e-02, -4.0893e-02,  1.2951e-02,  3.6133e-02, -4.2045e-02,
         3.9211e-02, -9.7521e-03, -2.4314e-03, -5.3163e-02, -3.4870e-02,
         2.7737e-02,  3.9340e-02,  5.0346e-02,  3.4461e-03, -4.4958e-02,
         4.1971e-02, -6.8803e-03,  3.6726e-02,  2.0739e-02, -4.8264e-02,
        -3.5584e-02, -4.3106e-02,  3.8971e-02, -3.2340e-03,  3.0137e-02,
        -4.6059e-02, -9.9959e-03,  2.1265e-02, -1.4500e-02, -1.5555e-02,
         1.9247e-03, -1.0561e-02,  6.5167e-03,  5.4679e-02,  2.6091e-02,
         3.4658e-02,  2.6805e-02,  1.4630e-02,  4.3349e-02,  3.7900e-02,
         4.8220e-02, -1.2925e-02,  3.6778e-02,  2.5026e-02, -9.0935e-03,
        -1.8396e-02, -2.6937e-02, -3.2098e-02, -8.9274e-03,  2.3112e-02,
         1.9722e-02, -3.4683e-02, -1.7958e-02,  2.5733e-02,  9.3715e-04,
        -8.9231e-04, -4.1738e-02,  1.3601e-02, -5.9625e-03,  3.9808e-02,
        -2.4078e-02, -5.2643e-02, -2.3085e-03, -2.9002e-03, -3.6327e-02,
        -1.2042e-02,  2.6832e-02,  3.2503e-02, -2.3911e-03, -1.0547e-03,
        -5.9955e-03, -3.3451e-02, -3.1602e-02, -4.2468e-02,  1.9245e-02,
        -1.9394e-02,  3.5641e-02, -4.4909e-02,  4.7852e-03,  1.7677e-02,
         8.1292e-03,  2.6557e-02, -1.1172e-02,  1.3219e-02, -4.3154e-02,
        -8.5926e-03, -2.1571e-03,  9.1993e-03, -3.8666e-02, -3.4279e-02,
         5.4071e-02, -4.1867e-03, -1.6758e-02, -5.2892e-02, -3.9118e-02,
         1.1966e-02,  1.0736e-02,  2.7186e-02, -1.7381e-02, -4.6510e-02,
         2.7599e-03,  3.9973e-02, -5.0975e-02, -5.2606e-02,  4.1809e-02,
        -2.3722e-02, -2.5440e-02, -7.1083e-03, -8.3225e-04,  5.0876e-03,
         6.7424e-04, -2.8705e-02,  5.7987e-03,  4.1441e-02, -3.7135e-02,
         3.1001e-02,  5.4692e-02,  4.3101e-02,  4.0406e-02,  2.0464e-02,
        -8.8839e-03,  1.8147e-02,  4.2233e-02,  3.6162e-02, -4.3956e-02,
        -2.1824e-02, -5.9018e-03,  1.3527e-02, -3.3234e-02, -4.5782e-02,
         3.4243e-02, -5.2422e-02, -1.5580e-02,  3.9135e-02,  3.0641e-02,
         5.0664e-02, -6.4571e-03, -4.7101e-03,  5.5430e-03,  2.1092e-02,
        -2.5106e-02,  1.6267e-02,  4.6833e-02,  2.7795e-03,  3.8948e-02,
         9.5580e-03, -1.2884e-04,  1.2490e-02,  2.0861e-02,  7.3317e-03,
         2.5707e-02,  2.0850e-02, -2.2329e-02, -1.7567e-02, -2.9483e-02,
        -2.6759e-02, -2.5594e-02,  2.5854e-02, -2.0389e-02,  2.0165e-02,
         1.1653e-02,  1.3662e-02, -6.2610e-03, -2.4885e-02,  3.1015e-02,
        -6.0284e-03, -3.2242e-02,  2.3202e-02,  8.3914e-03,  3.3066e-03,
         4.6736e-02, -4.9546e-02, -2.1738e-02,  4.3884e-02, -5.1692e-02,
        -4.6281e-02, -1.8590e-02, -4.0152e-02, -4.8103e-02,  4.9004e-02,
        -3.7612e-02,  2.9241e-02, -2.5682e-02,  9.4586e-03,  2.3821e-02,
        -2.3632e-02, -5.2588e-02,  4.6151e-02, -5.4705e-02,  5.4345e-02,
        -1.4321e-02,  2.8657e-02, -3.0051e-02, -3.2774e-02,  5.0311e-02,
         4.8129e-02,  4.4964e-02,  3.9081e-02, -3.6330e-02, -5.3512e-02,
        -2.5234e-03, -2.8266e-02,  3.6036e-02, -7.4534e-03, -4.0241e-02,
        -3.5976e-02,  1.6362e-02, -4.4932e-02, -3.2700e-02,  3.4432e-02,
         3.5030e-02,  1.9237e-02,  1.8444e-02,  4.3250e-02,  3.1831e-02,
         3.5061e-02,  5.1144e-02, -7.0157e-03, -3.5064e-02,  3.0983e-02,
        -3.2360e-02, -1.7662e-04,  3.4621e-02, -9.1132e-03, -3.8745e-02,
         3.7831e-02, -1.5250e-02, -5.0198e-02,  3.7156e-02, -2.2558e-02,
        -3.8404e-02,  6.9607e-03, -3.8881e-02, -4.6951e-02, -3.9848e-02,
        -2.3230e-02, -2.0961e-02,  1.2655e-02, -2.5030e-02, -5.5712e-02,
         6.7165e-03, -5.5776e-02,  4.7181e-02, -3.8966e-02,  1.6598e-02,
        -9.8593e-03, -5.0237e-02,  1.5269e-02,  1.3297e-02,  3.4522e-02,
         5.2190e-02,  1.0805e-02, -9.1123e-03, -1.1134e-02, -2.1107e-02,
        -1.9430e-02,  6.2317e-03,  3.9165e-02,  4.9839e-02,  9.5775e-03,
        -1.8117e-02,  3.9562e-02,  2.4222e-02, -2.3777e-02, -4.3892e-02,
         1.7679e-02,  1.7057e-02, -3.0439e-02,  1.1866e-02,  7.4751e-03,
         3.6803e-02, -1.6207e-02,  1.9186e-02, -4.2189e-02,  2.1892e-02,
        -9.7983e-03, -5.0983e-02, -1.1251e-02, -4.2816e-03, -3.1711e-02,
        -2.8823e-02,  8.6347e-04, -9.4005e-03, -3.2368e-02,  2.3528e-02,
         1.7911e-02, -3.2302e-02, -1.6053e-02,  3.8758e-02,  2.4245e-02,
         1.3489e-02,  6.9998e-03,  4.2047e-02, -1.3627e-02, -4.5881e-02,
        -5.5802e-02, -2.0405e-02, -1.1602e-02, -3.0235e-02,  3.0195e-02,
         2.1709e-02, -1.3480e-02, -3.2733e-02, -4.1819e-02, -3.7162e-02,
        -5.2392e-02,  1.6630e-02, -1.1416e-02, -3.1895e-02,  4.8154e-02,
        -5.1934e-02, -3.2254e-02, -1.5218e-02,  4.9720e-02, -4.1009e-02,
        -5.3473e-02,  1.4063e-02,  4.0608e-02, -4.3713e-02, -5.3940e-02,
        -1.5887e-04,  5.1620e-02, -2.0590e-02, -1.1530e-02, -2.9281e-02,
         2.4768e-02, -2.7743e-02,  2.1299e-02, -2.2151e-02, -9.8405e-03,
         4.2815e-02, -4.8921e-02,  1.2818e-02,  2.8016e-02,  2.6958e-02,
        -2.0695e-02, -1.8671e-02, -1.0967e-02, -6.3696e-03,  3.7751e-02,
        -4.3607e-02, -6.8681e-03,  2.7710e-02, -1.3093e-02, -1.8153e-02,
         2.1958e-02,  4.9956e-02, -4.7834e-02,  1.6967e-02,  2.2048e-02,
        -4.4963e-03,  4.6162e-02,  3.1827e-02,  4.8139e-02,  2.1752e-02,
        -2.3995e-02,  1.8378e-02, -3.3668e-02,  1.4962e-02, -1.0060e-02,
        -4.0396e-02, -5.0426e-02,  3.6163e-02,  5.1559e-02, -9.4082e-03,
        -4.1998e-02,  2.6021e-02, -4.0995e-02, -5.1106e-03,  4.6002e-02,
        -3.9150e-03,  4.2936e-02, -4.7259e-02,  4.5122e-02, -8.8405e-03,
         1.6924e-02, -4.8282e-02,  4.8290e-02, -3.3294e-02, -4.1446e-02,
        -3.1670e-02, -5.1255e-02,  3.0356e-02,  3.5470e-03,  2.0785e-02,
        -1.1245e-02, -1.2934e-02, -2.7816e-02,  4.1660e-02,  1.9976e-02,
        -3.7044e-02,  1.8139e-02,  5.2326e-02,  1.6633e-02, -1.5043e-02,
        -3.6521e-02, -7.2208e-03, -2.1816e-02,  3.1985e-02,  1.1401e-02,
         4.8046e-02,  3.9375e-02, -5.8200e-04, -4.9137e-02,  5.1932e-03,
        -2.3783e-02, -8.5476e-03, -4.2653e-02, -3.2300e-02, -4.1120e-02,
         5.1101e-02,  1.5085e-02,  5.0824e-02, -4.1131e-02,  1.3638e-02,
         2.5782e-02, -1.2294e-02,  4.7950e-02, -2.7514e-02, -1.6998e-02,
        -5.0044e-02, -9.6256e-03, -4.3691e-02, -1.8093e-03, -2.4363e-02,
         5.4275e-03,  4.6591e-03,  4.8135e-02,  3.5274e-02, -2.5625e-02,
        -6.3703e-03,  9.3473e-03, -1.7672e-02, -5.5871e-02,  4.3069e-02,
         4.2903e-02,  4.5424e-02, -5.4195e-02, -2.4650e-02,  2.2629e-02,
         2.1866e-02,  1.9937e-02, -1.3193e-02, -2.1034e-02, -1.1848e-03,
        -2.6728e-02,  9.4973e-03, -4.5682e-02, -3.8308e-02,  3.6747e-02,
        -3.4573e-02,  4.1354e-02, -2.4154e-02,  4.8144e-02,  1.3936e-02,
         2.7746e-03,  2.5659e-02, -3.8539e-02,  3.8202e-02,  4.5081e-02,
        -1.0833e-02, -3.3692e-02, -1.9365e-02,  3.6340e-02, -3.2276e-02,
         3.9710e-02, -1.9791e-02, -3.5688e-02,  3.0806e-02,  1.4788e-02,
        -4.7238e-02,  3.8625e-02, -2.4085e-02,  1.5535e-02,  4.9106e-02,
        -1.9521e-02,  1.7720e-02, -4.8304e-02,  3.2647e-03,  2.3048e-02,
        -5.4693e-02, -8.6101e-04,  1.3055e-02, -2.6697e-02, -2.6493e-02,
         3.4198e-02,  1.8001e-02, -3.6884e-02,  4.8441e-02,  5.1325e-02,
         8.3306e-03,  9.3092e-03, -5.2899e-02,  4.9927e-02, -3.8885e-02,
        -2.7666e-02,  8.3122e-03,  1.4499e-02, -2.6184e-02,  1.4516e-02,
        -1.1452e-02, -3.5657e-04,  4.9101e-02,  2.1638e-02,  3.9707e-02,
        -1.9954e-02,  5.4832e-02, -3.5164e-02, -5.3340e-02, -2.7968e-02,
         2.6654e-02,  5.1439e-02, -4.6592e-02,  1.6697e-02,  1.9394e-02,
         3.5247e-02, -3.7533e-02, -2.6960e-03,  8.2112e-03,  1.0930e-02,
         1.7096e-02, -2.4405e-02,  2.0891e-02,  1.9682e-02, -2.1524e-03,
         4.1983e-02,  1.7639e-02,  3.4562e-02,  1.1424e-03, -2.0800e-02,
         1.7386e-02, -2.8948e-02,  4.2230e-02,  9.7993e-03,  3.9182e-02,
         4.3533e-02,  3.5551e-02, -4.0090e-02, -2.9818e-02, -8.9373e-03,
         5.3402e-03, -9.4669e-03, -4.5917e-02, -3.1020e-02, -2.7055e-02,
         1.7888e-02,  3.0163e-02,  3.8845e-02,  1.5860e-02, -4.6562e-02,
         3.5935e-02, -4.6997e-02, -4.6762e-03,  1.8880e-02,  1.5236e-02,
         8.4962e-03,  1.6129e-02,  4.2444e-02, -3.7322e-02,  8.2904e-03,
         4.3407e-02,  2.0187e-02,  3.2160e-03, -6.0479e-03, -4.8899e-02,
        -3.7666e-02,  4.7391e-02,  1.5560e-02,  1.3591e-02,  5.1015e-02,
        -4.8134e-02,  5.3320e-02, -5.0653e-02,  2.4057e-02, -8.7124e-04,
         1.5789e-02, -4.6345e-02,  1.7873e-03, -5.5018e-02, -3.2488e-03,
         4.5765e-02,  3.8611e-02, -1.0488e-02, -4.0058e-02,  1.2469e-02,
         3.3943e-02,  5.3895e-02,  4.1866e-02, -3.2414e-02,  3.8972e-02,
         3.2368e-02,  3.8909e-02,  2.7362e-02, -3.5128e-03, -5.3628e-02,
        -5.0044e-02,  1.3338e-02, -5.5899e-02,  5.2154e-02,  3.0980e-02,
         3.4493e-02, -4.6047e-03,  1.4106e-02,  2.1700e-02,  5.2911e-02,
        -5.0851e-02,  1.1568e-02, -1.5688e-02,  3.7497e-02, -3.4420e-02,
        -2.6207e-02,  4.5016e-02, -4.7780e-02,  2.8015e-02, -2.6246e-02,
         3.2273e-02, -5.0425e-02,  2.8665e-02, -3.9989e-02, -1.4664e-02,
        -5.5811e-03, -7.4662e-03, -2.5457e-02,  3.7163e-02, -4.9925e-02,
        -2.7880e-02,  1.3799e-02, -1.1398e-02, -5.1521e-02, -4.3921e-02,
        -2.0154e-02, -4.9352e-02,  3.5765e-02,  5.3503e-02,  3.0128e-02,
         4.7074e-02, -2.0607e-02,  1.3203e-02,  3.5351e-02,  5.0624e-02,
        -4.4153e-02,  2.4029e-02,  1.1468e-02, -4.4421e-02, -4.6845e-02,
        -5.0057e-02, -7.8295e-03, -4.3215e-02,  2.5349e-02, -8.2403e-04,
         5.4979e-02, -3.7463e-02, -3.4463e-02,  2.5787e-02, -4.6093e-02,
        -1.5503e-02, -3.6553e-02, -3.3153e-02,  4.1750e-02,  3.2130e-02,
         2.8890e-02,  3.6257e-02, -3.3881e-02,  1.5614e-02, -5.5540e-02,
         3.2647e-02,  5.7849e-03,  5.3697e-02, -5.1993e-02,  6.1736e-03,
        -3.2314e-02, -1.6496e-02, -3.1362e-02, -4.1101e-02, -8.9627e-03,
         7.1998e-03, -1.8050e-02,  3.6287e-02,  3.6857e-03,  2.9411e-02,
        -1.8651e-02,  3.1941e-02,  2.1441e-02,  1.4839e-02,  4.0195e-02,
        -3.4274e-02,  5.0115e-02,  2.7662e-02, -1.9122e-02, -4.2525e-02,
        -2.5650e-02, -5.1169e-02, -3.2043e-02,  2.8422e-02, -4.1414e-02,
         4.9307e-03, -8.5037e-03,  4.3268e-02, -1.4437e-02,  1.4875e-02,
        -4.0665e-03,  2.3048e-02,  2.8445e-02,  4.6780e-02,  3.3280e-02,
        -2.9770e-02,  4.1608e-02, -4.7972e-02, -4.4655e-02,  5.1801e-02,
        -4.2333e-02, -1.1883e-03,  2.2115e-02, -9.2431e-03, -5.3004e-02,
         3.1928e-02,  1.5832e-02,  3.6163e-02,  3.4054e-02, -3.9709e-02,
         1.1222e-02, -1.1869e-02,  2.9417e-02, -1.3441e-02, -5.4680e-02,
        -3.2939e-02, -4.1842e-02,  1.8051e-02,  5.1430e-02, -5.2899e-02,
         2.9650e-02,  2.7577e-02,  1.1089e-02,  1.9439e-02, -2.8849e-02,
        -1.6114e-02, -5.0846e-02, -4.9956e-02, -9.1565e-03, -3.7775e-02,
         1.7246e-02,  4.2443e-02,  1.5475e-02, -4.3317e-02,  2.5437e-02,
        -3.5708e-02, -2.4117e-02, -2.1806e-02,  3.9349e-02, -4.1118e-02,
        -3.4541e-02, -2.3823e-03, -1.9986e-02, -2.6449e-02, -6.1375e-03,
         3.1757e-02, -1.6223e-02, -1.0535e-02,  7.1956e-03,  4.3578e-02,
        -4.4521e-03,  2.7950e-02, -3.2341e-02, -1.5139e-02,  6.2560e-03,
        -4.5692e-02,  4.9644e-02, -5.0692e-02, -2.6332e-02,  5.9133e-03,
         7.5736e-03,  1.2668e-02, -2.0174e-02,  1.3133e-02, -4.2106e-02,
         2.7469e-02, -2.7979e-02, -2.0309e-02, -4.2870e-02,  4.0369e-02,
         3.0377e-02,  1.2405e-02,  4.1946e-02,  5.1109e-02,  5.2279e-02,
        -3.5863e-02,  4.3648e-02,  1.5807e-02,  4.0219e-02, -2.0423e-02,
        -8.0629e-03, -1.0602e-02,  5.2226e-02,  1.5850e-03, -7.5698e-05,
         1.5521e-03,  1.2915e-02,  2.8305e-02, -5.2713e-02,  6.9459e-03,
        -3.8644e-02, -4.6879e-02,  3.9438e-02,  1.1126e-02,  1.1920e-02,
         3.5304e-02,  1.5087e-02, -1.7272e-02, -5.2947e-02, -4.0646e-02,
        -9.4577e-03, -5.2945e-02, -4.1236e-02,  2.6372e-02,  6.6182e-03,
        -4.7572e-02, -1.4683e-02, -4.8996e-02,  5.1732e-02,  3.8175e-03,
         4.9964e-02,  5.4723e-02,  2.6128e-02, -3.3761e-02, -3.4502e-02,
        -1.2007e-02, -2.3911e-02, -4.6016e-02,  4.6984e-02,  1.2547e-02,
        -9.4051e-03, -5.5864e-02,  2.0545e-02, -4.3055e-02, -1.1849e-02,
        -1.5822e-02,  4.2017e-02, -3.0687e-02,  1.6010e-02,  3.4434e-04,
        -2.5917e-02, -2.5166e-02,  4.0895e-02, -8.0213e-03, -5.2649e-02,
        -2.5029e-02, -2.7638e-02,  3.2899e-03, -8.1440e-03,  4.4946e-02,
         4.0227e-02,  3.6978e-02,  3.7976e-02,  4.9250e-02, -4.8458e-03,
         4.5964e-02, -5.3603e-02,  4.3988e-02,  5.5625e-02,  4.2735e-02,
         1.3856e-02,  4.7858e-02, -3.7621e-02, -4.4194e-02,  5.2330e-02,
        -5.4963e-02, -5.2293e-02, -2.6926e-02, -4.8529e-02,  3.4776e-02,
        -2.4344e-02,  1.9225e-02,  3.7655e-03, -3.9325e-02,  4.2513e-03,
        -2.3622e-02,  4.3289e-02,  6.0585e-03, -2.9483e-02, -1.2522e-02],
       device='cuda:0', requires_grad=True)), ('dec0.bias_hh', Parameter containing:
tensor([-5.4314e-02, -3.4785e-04,  2.1560e-02,  4.8832e-02,  5.3437e-02,
         4.1435e-02, -6.4818e-03, -4.7737e-02, -3.8952e-02, -2.7533e-02,
         1.9391e-02,  2.9810e-02, -3.2622e-02,  3.8880e-02,  4.9448e-02,
        -1.1086e-02, -7.0979e-03,  3.4688e-02, -4.5450e-02,  4.1740e-02,
        -4.4731e-02,  8.5111e-03,  4.1420e-02,  4.2224e-02, -3.1160e-02,
        -7.3735e-03, -5.2804e-02,  4.4288e-03, -4.9372e-02, -3.3067e-02,
         3.6167e-02,  2.2050e-02,  1.6583e-02, -2.4689e-02, -1.3509e-03,
        -4.9079e-02, -3.2954e-02,  1.2602e-02, -3.4122e-02, -3.4271e-02,
         1.8532e-02, -1.2665e-02,  2.1330e-02,  3.8915e-02,  3.8689e-02,
        -4.6176e-02, -4.5716e-02, -5.1561e-02,  5.1743e-02, -1.6775e-02,
         3.9501e-02,  3.2106e-02,  4.2361e-02,  4.2746e-02, -4.5752e-02,
         8.5627e-03,  1.6140e-02, -2.0331e-02,  3.5559e-02,  3.4426e-03,
        -5.5006e-02,  3.9174e-02, -4.9073e-02,  5.3734e-02, -2.6344e-02,
         4.0699e-02, -4.5677e-02, -3.4765e-02, -1.4879e-02,  4.5762e-02,
        -2.0699e-02, -5.1922e-02,  2.1355e-02, -3.9728e-02, -2.5852e-02,
         4.2510e-03, -5.3017e-03, -2.9682e-02,  1.4371e-02, -5.4606e-02,
         2.9009e-02,  4.0950e-02, -5.3324e-02, -3.9776e-02, -6.5893e-03,
         3.1812e-02,  2.7799e-02,  2.3093e-02,  4.0505e-03,  6.9450e-03,
        -5.5513e-02, -1.1216e-03, -4.5230e-02,  1.8331e-02, -1.3832e-02,
        -4.7827e-02,  9.7268e-03, -4.9775e-02,  4.0635e-02, -2.9750e-02,
        -2.6603e-02, -5.2383e-02, -2.6271e-02,  4.4760e-02, -4.4977e-02,
        -2.0815e-02,  2.9303e-02,  4.1710e-02, -3.8871e-02, -4.9031e-02,
         1.9822e-02, -5.1556e-02, -2.6400e-02,  4.7688e-02,  2.7447e-02,
        -2.1615e-02,  3.5941e-02, -1.6914e-02, -1.3334e-03, -5.0135e-02,
         3.4192e-02, -1.1046e-02,  2.5716e-02, -1.4071e-02, -2.9853e-02,
        -1.6913e-02, -9.0685e-03, -5.1221e-02, -5.0518e-03,  4.4755e-02,
         1.8272e-02, -9.5111e-03,  9.7750e-03,  3.4802e-02, -3.1074e-02,
         1.9768e-02, -2.2986e-02,  4.1673e-02,  2.6352e-02, -2.6050e-02,
         4.8874e-02,  5.5759e-02, -5.1898e-02,  5.0577e-02,  6.3329e-03,
         5.4388e-02, -8.0104e-03, -3.9990e-02, -1.0623e-02,  3.4670e-03,
         7.1690e-03, -1.3401e-02,  3.6071e-02,  5.1514e-02, -2.9357e-02,
        -4.8493e-02,  1.9988e-03, -2.3466e-03, -5.9111e-03, -7.9955e-03,
         5.0910e-02,  2.2269e-02,  4.7158e-02, -4.4132e-02,  4.2453e-02,
         8.0713e-03, -5.8834e-03,  6.5519e-03,  3.9658e-02,  5.2578e-02,
         1.9296e-02, -8.0656e-04,  2.8774e-02,  2.5366e-02, -3.8283e-02,
        -5.5670e-02, -3.1826e-02, -5.1663e-02,  5.1051e-02, -4.3537e-02,
         2.8252e-02,  5.3315e-02,  3.7698e-02, -4.8295e-02,  4.0013e-02,
         3.3501e-02,  2.7627e-03,  2.1876e-02,  6.3306e-03, -9.2172e-03,
         1.1390e-02, -1.6903e-02, -1.2428e-02, -5.1384e-02,  4.3018e-02,
         1.6539e-02, -5.5476e-04,  2.9539e-02,  2.3580e-02, -4.8381e-02,
        -2.8777e-02, -3.2902e-02, -2.4602e-02,  3.5602e-02, -5.5497e-02,
        -5.3624e-02, -5.3311e-02,  2.2094e-02,  2.4669e-02, -3.6493e-02,
        -5.4298e-02, -1.1364e-02,  4.7120e-02,  4.0984e-02, -1.7288e-02,
         5.3098e-02,  1.9371e-02, -4.5858e-03,  1.1084e-02,  3.9137e-02,
         1.0596e-02, -3.2349e-02,  5.3775e-02,  2.9679e-03,  3.0918e-02,
         1.6146e-03, -2.4846e-02,  4.0045e-02,  3.3875e-02, -1.2450e-02,
         3.2038e-02, -4.3413e-03, -2.4216e-02, -3.6776e-02,  2.4713e-02,
        -4.6360e-02,  4.9984e-02,  4.9410e-02,  4.6282e-02, -1.0980e-02,
         2.5608e-02,  8.7999e-03,  7.9612e-03, -2.9485e-02, -4.3044e-02,
        -1.8132e-02,  1.0044e-02,  2.2479e-02,  1.1138e-02,  3.0553e-02,
         2.6268e-02, -3.2540e-02,  5.2285e-02,  4.4887e-02,  3.2653e-02,
        -3.4393e-02, -9.8195e-03, -1.4917e-02,  2.9339e-02,  1.6357e-04,
        -5.1508e-02, -3.2936e-03,  5.3566e-02,  2.7429e-02,  5.3993e-02,
        -3.7882e-02,  4.5856e-02, -4.6148e-02,  2.6467e-02, -3.6091e-02,
         5.1037e-02,  2.4709e-02,  5.7936e-03, -1.1700e-02, -1.4658e-03,
        -5.5412e-02, -2.3954e-02,  5.0979e-02, -1.8954e-02, -4.5009e-02,
        -1.1875e-02, -4.4404e-02,  5.3086e-02,  4.7565e-03,  3.8799e-02,
         4.4620e-03,  1.7573e-02, -8.7719e-03, -3.0426e-02, -1.7570e-02,
         2.8143e-02, -1.5513e-02, -3.4711e-02,  1.9742e-02,  5.3522e-02,
        -3.4399e-02, -4.6697e-02, -3.3545e-02,  2.3687e-02,  1.4304e-02,
        -4.7237e-02,  1.6995e-03, -1.4361e-03,  5.5893e-02,  3.4619e-02,
        -2.6345e-02, -2.5278e-03,  4.1864e-02, -8.4243e-03, -3.8347e-02,
        -1.4026e-02, -1.2740e-02, -2.0323e-02, -3.0194e-03,  2.6556e-02,
         3.7820e-02,  2.0340e-03,  4.9590e-02, -2.3773e-02, -4.6185e-02,
        -2.8315e-02,  4.8321e-02,  2.6607e-02, -9.5039e-03,  1.2790e-02,
        -1.6463e-02,  6.3395e-03,  2.8157e-02, -3.6300e-02,  1.6205e-02,
         2.6577e-03, -3.0939e-02, -5.3852e-02,  4.8722e-02,  4.2656e-02,
        -3.0637e-02, -4.0489e-02,  5.1443e-02, -4.9094e-02, -3.0961e-02,
        -1.4944e-02,  1.2966e-02,  4.0281e-02,  8.1465e-03,  4.7381e-02,
         1.2976e-02,  3.0247e-02, -5.2261e-02,  8.8664e-04, -2.7925e-02,
        -3.2024e-02,  1.0613e-02,  3.4249e-02, -5.2474e-02, -5.2845e-02,
         4.9504e-02, -2.2784e-02,  5.0372e-02, -4.4731e-02,  2.1558e-02,
         5.1320e-02, -4.8267e-02, -6.8620e-03, -3.8408e-03, -1.7830e-02,
        -3.6622e-02, -4.3059e-02, -2.0237e-02, -4.8092e-02,  3.9340e-02,
        -1.2035e-02,  4.9688e-02, -1.0586e-02,  1.5447e-03, -1.0001e-02,
        -7.8516e-03,  3.0215e-02,  5.5655e-02,  5.2833e-02, -3.4312e-02,
         1.0683e-02, -2.2263e-02,  1.2955e-02,  3.4744e-02, -1.2794e-02,
         6.0799e-03,  3.6975e-02, -1.1502e-02, -1.3408e-02,  4.4749e-02,
        -3.6320e-02,  2.4224e-03,  1.4787e-02, -2.7788e-02, -5.2849e-02,
        -1.5707e-02,  1.3867e-02, -2.0677e-02,  6.8208e-03, -2.1202e-03,
         5.8092e-03,  5.5749e-02,  5.1258e-02,  6.6723e-03,  4.6463e-02,
         1.6991e-02,  4.9892e-02,  5.8284e-03,  4.9642e-02,  4.5293e-02,
         1.7621e-02,  3.4728e-02, -4.2340e-02,  3.6622e-02, -3.3204e-02,
        -4.0030e-02,  4.7903e-02,  3.4771e-02,  2.1305e-02, -5.1943e-02,
        -1.5119e-02, -3.9140e-02,  8.0310e-03,  4.0327e-02, -7.0904e-03,
         2.6463e-02, -3.1172e-02, -4.1900e-02, -3.8295e-02, -3.1379e-02,
        -2.3337e-02,  4.5196e-03, -1.7726e-03, -7.9424e-03,  9.4581e-03,
         4.0280e-02,  2.4819e-02, -2.1061e-02, -2.6971e-02,  3.0590e-02,
         7.5124e-03,  3.4799e-02, -1.9423e-02, -3.5622e-02,  3.6029e-02,
         4.6362e-02,  3.2129e-02,  2.0305e-03, -1.9222e-02, -3.2193e-02,
         5.3130e-02,  1.5619e-02,  5.4274e-02,  3.9081e-03, -3.6672e-02,
         4.1659e-02,  5.5669e-02,  5.0723e-02,  5.7543e-03, -4.4736e-02,
         7.7025e-03, -2.2670e-02, -4.6009e-02,  6.6234e-04,  2.4327e-02,
         4.9280e-02,  1.4677e-03, -5.2259e-02,  3.0728e-03,  4.2287e-02,
         2.9721e-02, -2.8750e-02, -1.8829e-02, -4.4629e-03, -2.2307e-03,
         2.5505e-02, -1.0785e-02, -1.1491e-02, -5.2726e-02, -4.5080e-02,
         4.5528e-02, -1.3599e-02, -2.9905e-02,  1.2756e-02,  2.2492e-02,
         3.9218e-02, -2.6671e-02, -2.2108e-02, -2.6872e-02, -4.1188e-02,
        -4.4270e-02, -4.6442e-02,  1.8903e-02, -5.3118e-02,  1.9870e-02,
        -5.4848e-02, -2.1822e-02,  5.3733e-02,  3.2336e-02,  2.3497e-02,
         1.6526e-02,  2.8263e-02, -1.3843e-02, -6.1009e-04, -7.4981e-03,
         1.1378e-03,  3.0185e-02,  4.1478e-02, -1.0794e-02, -3.8740e-02,
         6.7733e-05,  9.4774e-03, -1.0141e-02,  3.3841e-02, -4.1851e-02,
         3.6870e-02,  3.0213e-03,  5.0299e-02, -2.8895e-02, -5.2430e-02,
         1.1691e-02, -1.9902e-02, -4.1433e-02,  2.4723e-02,  4.3588e-02,
         1.8648e-03,  3.6591e-02,  5.5815e-02, -2.1749e-03, -3.7987e-03,
         1.2287e-02,  4.8187e-02, -4.4491e-02, -2.4019e-02, -2.6917e-02,
        -4.4901e-02,  3.0133e-02,  2.4705e-02,  5.0146e-02,  1.0937e-02,
         5.5427e-02,  4.1339e-02, -4.9460e-02, -4.5656e-02,  1.2339e-02,
         4.4552e-02,  3.5361e-02,  4.6930e-03, -4.1291e-02, -3.1488e-02,
        -2.7177e-02,  3.0432e-02, -4.9453e-02,  3.8493e-02,  4.1554e-02,
         4.3975e-02, -3.8663e-02,  5.4333e-02,  3.7074e-02, -2.2875e-02,
        -3.6019e-02,  3.8427e-02,  5.5094e-02, -4.3410e-02, -1.2567e-02,
        -7.6725e-03, -2.4383e-02, -3.1763e-02,  1.3465e-02, -2.2475e-02,
        -3.4683e-02, -3.7401e-02,  3.7145e-02,  1.1096e-02,  2.1300e-02,
        -2.8695e-02, -4.7002e-02, -5.5205e-02,  4.0343e-03, -2.6617e-02,
        -2.0272e-02,  2.5231e-02, -2.6411e-02,  5.2113e-02, -3.4814e-02,
         3.9017e-03, -4.2383e-02, -7.2097e-03, -2.3636e-02,  4.2374e-02,
        -1.0524e-02, -5.2164e-02, -1.2701e-02,  5.4288e-02, -3.9596e-02,
         3.6076e-02,  4.8660e-02, -4.6864e-02,  4.6880e-02,  4.8639e-02,
        -1.0853e-02,  2.6438e-04, -1.8171e-03,  3.7494e-02,  9.5454e-03,
         2.4349e-02, -2.0163e-02, -2.3921e-02,  2.4804e-02, -3.3158e-02,
        -3.8879e-02, -2.2406e-03,  4.4934e-02,  1.9368e-02,  3.6382e-04,
         3.9344e-02,  4.2205e-02, -5.0705e-02,  2.3408e-02, -9.3862e-03,
         5.0200e-02, -3.2753e-02, -5.4579e-02,  1.3727e-02,  2.1760e-02,
        -6.2902e-04,  4.2592e-02, -4.7643e-02, -1.9468e-02,  3.9778e-02,
        -5.3297e-02,  2.9491e-02,  2.4177e-02,  3.6220e-02,  3.2289e-02,
         4.8698e-02, -5.5968e-03, -1.8585e-02,  4.0086e-02, -3.3618e-02,
         3.9594e-02,  3.2708e-02, -8.9534e-03,  9.5079e-03, -4.6482e-02,
        -3.4473e-02, -8.5529e-03, -3.5109e-02, -4.3181e-02,  1.3046e-02,
        -2.8012e-02, -1.1622e-02,  3.8452e-02, -1.2304e-02,  1.1571e-03,
         5.5231e-02,  3.9635e-02,  1.9038e-02,  2.4306e-02,  1.5008e-02,
        -4.1599e-02,  5.3637e-02, -4.9772e-02,  1.9754e-02,  3.0517e-02,
         1.7726e-02, -2.5180e-02, -3.2417e-02, -4.3658e-03,  3.0147e-02,
         2.2818e-02, -3.8538e-02, -7.9031e-03,  5.2986e-02,  5.4647e-02,
        -2.6665e-02, -1.4990e-02,  5.8840e-03,  2.8306e-02,  3.5230e-02,
         1.3995e-02, -1.4555e-02,  1.2264e-02,  1.9628e-03, -1.9656e-02,
         5.5068e-02, -3.5816e-02, -1.0470e-02,  5.4619e-02, -4.5457e-03,
        -3.3689e-02, -3.9232e-02,  5.2518e-02, -4.8841e-03, -1.1369e-02,
         1.2793e-02, -3.2375e-02,  5.5402e-02, -7.2164e-03, -3.0371e-02,
         6.4480e-03, -4.1490e-02,  1.6387e-02, -1.6834e-02,  1.2166e-02,
         2.7262e-02, -5.1046e-02,  3.7138e-03,  1.4692e-02, -3.5118e-02,
        -3.1033e-02, -5.2088e-02,  5.3017e-02,  3.2952e-02, -4.0787e-03,
         2.2945e-02,  1.5129e-02, -9.8575e-03, -4.8896e-02, -5.3164e-02,
        -9.2956e-03,  2.1533e-02,  4.4306e-02, -1.0218e-02, -3.7868e-02,
         2.5129e-02,  7.2701e-03, -4.1916e-02, -6.4351e-03, -4.9403e-02,
        -2.4310e-03, -4.1352e-03, -2.0276e-02, -4.2783e-02, -2.9764e-02,
        -2.9097e-02, -4.7253e-02,  5.0846e-02,  4.0974e-03, -3.6016e-02,
         4.9119e-02, -2.4391e-02,  1.6467e-02,  2.4585e-02, -4.3211e-02,
        -3.5496e-02, -4.5751e-02, -3.3196e-02, -2.0754e-02,  1.4264e-02,
        -3.0040e-02,  2.3519e-02,  3.4462e-02,  3.8166e-02,  3.8998e-02,
         1.9414e-02,  4.2224e-02, -3.1279e-02, -5.3359e-02,  2.1586e-03,
        -4.4352e-02,  2.2399e-02, -3.4968e-02, -2.2839e-02,  4.7687e-02,
         5.1602e-03, -4.7797e-02,  2.7619e-02, -3.3506e-02,  5.1558e-03,
        -4.9958e-02, -5.0744e-03,  3.1934e-02,  4.8760e-03,  2.1687e-02,
        -5.3276e-02, -4.2767e-02,  5.2667e-02,  2.0874e-02, -1.1840e-02,
         4.2174e-02,  1.0682e-02,  2.2672e-02, -3.4023e-02, -2.4779e-02,
         5.4432e-02,  2.1089e-02, -4.7555e-02,  4.5275e-02, -8.6612e-03,
        -1.4889e-02,  3.0672e-02,  3.6312e-02,  3.3063e-02, -3.4844e-02,
        -2.0649e-02,  2.7549e-02,  1.8573e-02,  4.8031e-02, -2.7925e-02,
         1.5220e-02, -1.6501e-02, -4.8576e-02,  2.1184e-02, -5.3931e-02,
        -6.3617e-03, -4.7624e-02, -3.7335e-02,  2.9099e-02, -3.0686e-02,
        -3.2077e-02,  3.0637e-02, -1.4381e-02, -2.3737e-02,  4.6013e-02,
         4.4831e-02, -3.5254e-02, -8.5164e-03, -3.6583e-03,  2.7328e-02,
         1.8298e-02, -1.8931e-02, -3.6524e-02,  5.5283e-02, -1.8901e-02,
        -3.7713e-03,  4.7947e-02, -4.4501e-02,  1.3500e-02,  1.6982e-02,
         3.1080e-02, -4.2223e-02, -4.6942e-02, -5.3088e-02, -5.1450e-02,
        -1.9592e-02,  3.4198e-02, -2.5275e-03,  4.2086e-02, -4.7173e-02,
        -3.7381e-02,  1.6460e-02,  2.1626e-02, -2.6100e-02, -1.6616e-02,
        -4.1546e-02, -9.9078e-03,  3.1371e-02, -4.8534e-02, -5.0725e-02,
         4.5495e-02,  1.8215e-02,  3.7966e-03, -2.8280e-02,  2.2486e-02,
        -5.0347e-03,  5.5150e-03,  5.0833e-03, -2.0771e-02,  3.9964e-02,
         4.3136e-02,  2.6295e-02,  5.3400e-02, -3.5901e-02, -1.0467e-02,
         8.6934e-03, -1.1895e-02,  1.7982e-02, -4.8268e-02, -4.0437e-02,
        -5.3304e-02,  4.2209e-02, -5.4996e-02,  4.7328e-02,  2.7078e-02,
        -3.0796e-02,  2.0395e-02, -2.5308e-02,  2.7083e-02,  4.3668e-02,
         1.1504e-02,  3.6385e-02, -1.1261e-02, -5.2011e-02, -1.9743e-02,
        -1.7669e-02,  5.4024e-03,  3.0488e-03,  5.3861e-02, -3.2439e-02,
        -2.1401e-02,  1.1079e-02,  1.7769e-02, -9.0009e-03, -2.0388e-02,
        -2.1822e-02, -2.2254e-02, -4.2212e-02, -3.5062e-03, -2.1719e-02,
         3.1343e-02,  2.4644e-02,  3.1744e-02, -3.2415e-02,  8.5875e-03,
        -4.0208e-02,  3.6180e-02,  9.4068e-03, -3.9166e-02, -2.5660e-02,
         8.0469e-03,  2.8051e-03, -4.5754e-03,  5.1124e-02,  1.2379e-02,
         4.9649e-02, -5.5138e-02,  2.1098e-02, -3.9325e-02, -4.2144e-02,
        -1.4399e-02, -4.7243e-02, -5.3816e-02,  3.3782e-02,  1.8491e-02,
        -8.2818e-03, -4.3821e-02, -4.1322e-02,  3.4278e-02, -5.0926e-02,
         5.4635e-02,  7.3448e-03, -4.1120e-02,  5.4359e-02,  4.2463e-02,
         1.7358e-02,  4.9035e-02,  4.4050e-02, -5.5065e-02,  2.5872e-02,
         1.7320e-02, -4.6966e-02, -3.8100e-02, -2.9547e-02, -5.8818e-03,
        -4.1468e-02,  4.1450e-02, -4.8429e-02, -5.2717e-02,  2.4002e-02,
        -1.7703e-03, -3.4807e-03, -1.3179e-02,  1.4983e-02, -1.1008e-02,
        -9.2256e-03, -2.4643e-02, -4.1493e-02, -3.4836e-04, -9.7565e-03],
       device='cuda:0', requires_grad=True)), ('hid2out.weight', Parameter containing:
tensor([[-0.1344,  0.0339, -0.0005,  ...,  0.0770, -0.0199, -0.0712],
        [-0.1128, -0.0505,  0.1476,  ..., -0.0443, -0.0606,  0.0783],
        [-0.1236, -0.0699,  0.0201,  ..., -0.0457,  0.0820, -0.0701],
        ...,
        [ 0.0475,  0.0921,  0.0130,  ..., -0.0546,  0.1248,  0.2462],
        [-0.0100, -0.0310,  0.0179,  ..., -0.0034,  0.0175, -0.0354],
        [-0.1305, -0.0310, -0.0547,  ..., -0.0142,  0.0932, -0.1020]],
       device='cuda:0', requires_grad=True)), ('hid2out.bias', Parameter containing:
tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
        0.], device='cuda:0', requires_grad=True)), ('out2prob.bias', Parameter containing:
tensor([0., 0.], device='cuda:0', requires_grad=True)), ('lin_class.weight', Parameter containing:
tensor([[0.9154]], device='cuda:0', requires_grad=True))]Training started on 25-06-2020 03:39:02
Starting Epoch 1
Epoch 1 - update         30 => loss:  -0.299 [critic_loss: 1.565] (#OOM: 0)
Epoch 1 - update         60 => loss:  -0.395 [critic_loss: 1.083] (#OOM: 0)
Epoch 1 - update         90 => loss:  -0.399 [critic_loss: 0.618] (#OOM: 0)
Epoch 1 - update        120 => loss:  -0.524 [critic_loss: 0.471] (#OOM: 0)
Epoch 1 - update        150 => loss:  -0.393 [critic_loss: 0.188] (#OOM: 0)
Epoch 1 - update        180 => loss:  -0.577 [critic_loss: 0.179] (#OOM: 0)
Epoch 1 - update        210 => loss:  -0.465 [critic_loss: 0.082] (#OOM: 0)
Epoch 1 - update        240 => loss:  -0.591 [critic_loss: 0.079] (#OOM: 0)
Epoch 1 - update        270 => loss:  -0.355 [critic_loss: 0.033] (#OOM: 0)
Epoch 1 - update        300 => loss:  -0.398 [critic_loss: 0.027] (#OOM: 0)
Epoch 1 - update        330 => loss:  -0.459 [critic_loss: 0.020] (#OOM: 0)
Epoch 1 - update        360 => loss:  -0.489 [critic_loss: 0.043] (#OOM: 0)
Epoch 1 - update        390 => loss:  -0.371 [critic_loss: 0.013] (#OOM: 0)
Epoch 1 - update        420 => loss:  -0.383 [critic_loss: 0.009] (#OOM: 0)
Epoch 1 - update        450 => loss:  -0.212 [critic_loss: 0.008] (#OOM: 0)
Epoch 1 - update        480 => loss:  -0.470 [critic_loss: 0.021] (#OOM: 0)
Epoch 1 - update        510 => loss:  -0.425 [critic_loss: 0.020] (#OOM: 0)
Epoch 1 - update        540 => loss:  -0.401 [critic_loss: 0.009] (#OOM: 0)
Epoch 1 - update        570 => loss:  -0.526 [critic_loss: 0.017] (#OOM: 0)
Epoch 1 - update        600 => loss:  -0.461 [critic_loss: 0.019] (#OOM: 0)
Epoch 1 - update        630 => loss:  -0.371 [critic_loss: 0.020] (#OOM: 0)
Epoch 1 - update        660 => loss:  -0.483 [critic_loss: 0.008] (#OOM: 0)
Epoch 1 - update        690 => loss:  -0.654 [critic_loss: 0.037] (#OOM: 0)
Epoch 1 - update        720 => loss:  -0.372 [critic_loss: 0.005] (#OOM: 0)
Epoch 1 - update        750 => loss:  -0.467 [critic_loss: 0.008] (#OOM: 0)
Epoch 1 - update        780 => loss:  -0.438 [critic_loss: 0.008] (#OOM: 0)
Epoch 1 - update        810 => loss:  -0.320 [critic_loss: 0.004] (#OOM: 0)
Epoch 1 - update        840 => loss:  -0.366 [critic_loss: 0.006] (#OOM: 0)
Epoch 1 - update        870 => loss:  -0.599 [critic_loss: 0.018] (#OOM: 0)
Epoch 1 - update        900 => loss:  -0.371 [critic_loss: 0.014] (#OOM: 0)
Epoch 1 - update        930 => loss:  -0.534 [critic_loss: 0.018] (#OOM: 0)
Epoch 1 - update        960 => loss:  -0.534 [critic_loss: 0.015] (#OOM: 0)
Epoch 1 - update        990 => loss:  -0.421 [critic_loss: 0.007] (#OOM: 0)
Epoch 1 - update       1020 => loss:  -0.581 [critic_loss: 0.017] (#OOM: 0)
Epoch 1 - update       1050 => loss:  -0.380 [critic_loss: 0.020] (#OOM: 0)
Epoch 1 - update       1080 => loss:  -0.493 [critic_loss: 0.010] (#OOM: 0)
Epoch 1 - update       1110 => loss:  -0.434 [critic_loss: 0.009] (#OOM: 0)
Epoch 1 - update       1140 => loss:  -0.248 [critic_loss: 0.003] (#OOM: 0)
Epoch 1 - update       1170 => loss:  -0.578 [critic_loss: 0.021] (#OOM: 0)
Epoch 1 - update       1200 => loss:  -0.611 [critic_loss: 0.017] (#OOM: 0)
Epoch 1 - update       1230 => loss:  -0.563 [critic_loss: 0.015] (#OOM: 0)
Epoch 1 - update       1260 => loss:  -0.513 [critic_loss: 0.008] (#OOM: 0)
Epoch 1 - update       1290 => loss:  -0.308 [critic_loss: 0.006] (#OOM: 0)
Epoch 1 - update       1320 => loss:  -0.533 [critic_loss: 0.019] (#OOM: 0)
Epoch 1 - update       1350 => loss:  -0.483 [critic_loss: 0.037] (#OOM: 0)
Epoch 1 - update       1380 => loss:  -0.462 [critic_loss: 0.013] (#OOM: 0)
Epoch 1 - update       1410 => loss:  -0.347 [critic_loss: 0.004] (#OOM: 0)
Epoch 1 - update       1440 => loss:  -0.302 [critic_loss: 0.018] (#OOM: 0)
Epoch 1 - update       1470 => loss:  -0.497 [critic_loss: 0.016] (#OOM: 0)
Epoch 1 - update       1500 => loss:  -0.371 [critic_loss: 0.005] (#OOM: 0)
Epoch 1 - update       1530 => loss:  -0.444 [critic_loss: 0.009] (#OOM: 0)
Epoch 1 - update       1560 => loss:  -0.276 [critic_loss: 0.014] (#OOM: 0)
Epoch 1 - update       1590 => loss:  -0.462 [critic_loss: 0.010] (#OOM: 0)
Epoch 1 - update       1620 => loss:  -0.411 [critic_loss: 0.004] (#OOM: 0)
Epoch 1 - update       1650 => loss:  -0.275 [critic_loss: 0.008] (#OOM: 0)
Epoch 1 - update       1680 => loss:  -0.339 [critic_loss: 0.014] (#OOM: 0)
Epoch 1 - update       1710 => loss:  -0.526 [critic_loss: 0.008] (#OOM: 0)
Epoch 1 - update       1740 => loss:  -0.239 [critic_loss: 0.006] (#OOM: 0)
Epoch 1 - update       1770 => loss:  -0.273 [critic_loss: 0.008] (#OOM: 0)
Epoch 1 - update       1800 => loss:  -0.409 [critic_loss: 0.006] (#OOM: 0)
Epoch 1 - update       1830 => loss:  -0.409 [critic_loss: 0.008] (#OOM: 0)
Epoch 1 - update       1860 => loss:  -0.542 [critic_loss: 0.019] (#OOM: 0)
Epoch 1 - update       1890 => loss:  -0.547 [critic_loss: 0.010] (#OOM: 0)
Epoch 1 - update       1920 => loss:  -0.283 [critic_loss: 0.007] (#OOM: 0)
Epoch 1 - update       1950 => loss:  -0.536 [critic_loss: 0.013] (#OOM: 0)
Epoch 1 - update       1980 => loss:  -0.528 [critic_loss: 0.013] (#OOM: 0)
Epoch 1 - update       2010 => loss:  -0.411 [critic_loss: 0.008] (#OOM: 0)
Epoch 1 - update       2040 => loss:  -0.348 [critic_loss: 0.010] (#OOM: 0)
Epoch 1 - update       2070 => loss:  -0.509 [critic_loss: 0.017] (#OOM: 0)
Epoch 1 - update       2100 => loss:  -0.332 [critic_loss: 0.007] (#OOM: 0)
Epoch 1 - update       2130 => loss:  -0.400 [critic_loss: 0.008] (#OOM: 0)
Epoch 1 - update       2160 => loss:  -0.358 [critic_loss: 0.009] (#OOM: 0)
Epoch 1 - update       2190 => loss:  -0.632 [critic_loss: 0.028] (#OOM: 0)
Epoch 1 - update       2220 => loss:  -0.487 [critic_loss: 0.015] (#OOM: 0)
Epoch 1 - update       2250 => loss:  -0.469 [critic_loss: 0.012] (#OOM: 0)
Epoch 1 - update       2280 => loss:  -0.586 [critic_loss: 0.017] (#OOM: 0)
Epoch 1 - update       2310 => loss:  -0.422 [critic_loss: 0.010] (#OOM: 0)
Epoch 1 - update       2340 => loss:  -0.274 [critic_loss: 0.006] (#OOM: 0)
Epoch 1 - update       2370 => loss:  -0.320 [critic_loss: 0.004] (#OOM: 0)
Epoch 1 - update       2400 => loss:  -0.279 [critic_loss: 0.010] (#OOM: 0)
Epoch 1 - update       2430 => loss:  -0.609 [critic_loss: 0.011] (#OOM: 0)
Epoch 1 - update       2460 => loss:  -0.452 [critic_loss: 0.010] (#OOM: 0)
Epoch 1 - update       2490 => loss:  -0.539 [critic_loss: 0.012] (#OOM: 0)
Epoch 1 - update       2520 => loss:  -0.424 [critic_loss: 0.006] (#OOM: 0)
Epoch 1 - update       2550 => loss:  -0.394 [critic_loss: 0.016] (#OOM: 0)
Epoch 1 - update       2580 => loss:  -0.513 [critic_loss: 0.009] (#OOM: 0)
Epoch 1 - update       2610 => loss:  -0.387 [critic_loss: 0.008] (#OOM: 0)
Epoch 1 - update       2640 => loss:  -0.460 [critic_loss: 0.012] (#OOM: 0)
Epoch 1 - update       2670 => loss:  -0.394 [critic_loss: 0.015] (#OOM: 0)
Epoch 1 - update       2700 => loss:  -0.586 [critic_loss: 0.016] (#OOM: 0)
Epoch 1 - update       2730 => loss:  -0.417 [critic_loss: 0.004] (#OOM: 0)
Epoch 1 - update       2760 => loss:  -0.600 [critic_loss: 0.021] (#OOM: 0)
Epoch 1 - update       2790 => loss:  -0.441 [critic_loss: 0.006] (#OOM: 0)
Epoch 1 - update       2820 => loss:  -0.389 [critic_loss: 0.008] (#OOM: 0)
Epoch 1 - update       2850 => loss:  -0.375 [critic_loss: 0.017] (#OOM: 0)
Epoch 1 - update       2880 => loss:  -0.367 [critic_loss: 0.006] (#OOM: 0)
Epoch 1 - update       2910 => loss:  -0.389 [critic_loss: 0.010] (#OOM: 0)
Epoch 1 - update       2940 => loss:  -0.420 [critic_loss: 0.006] (#OOM: 0)
Epoch 1 - update       2970 => loss:  -0.445 [critic_loss: 0.007] (#OOM: 0)
Epoch 1 - update       3000 => loss:  -0.340 [critic_loss: 0.006] (#OOM: 0)
Epoch 1 - update       3030 => loss:  -0.453 [critic_loss: 0.008] (#OOM: 0)
Epoch 1 - update       3060 => loss:  -0.425 [critic_loss: 0.009] (#OOM: 0)
Epoch 1 - update       3090 => loss:  -0.278 [critic_loss: 0.011] (#OOM: 0)
Epoch 1 - update       3120 => loss:  -0.282 [critic_loss: 0.003] (#OOM: 0)
Epoch 1 - update       3150 => loss:  -0.343 [critic_loss: 0.006] (#OOM: 0)
Epoch 1 - update       3180 => loss:  -0.425 [critic_loss: 0.010] (#OOM: 0)
Epoch 1 - update       3210 => loss:  -0.623 [critic_loss: 0.011] (#OOM: 0)
Epoch 1 - update       3240 => loss:  -0.533 [critic_loss: 0.018] (#OOM: 0)
Epoch 1 - update       3270 => loss:  -0.370 [critic_loss: 0.010] (#OOM: 0)
Epoch 1 - update       3300 => loss:  -0.233 [critic_loss: 0.003] (#OOM: 0)
Epoch 1 - update       3330 => loss:  -0.275 [critic_loss: 0.006] (#OOM: 0)
Epoch 1 - update       3360 => loss:  -0.565 [critic_loss: 0.013] (#OOM: 0)
Epoch 1 - update       3390 => loss:  -0.510 [critic_loss: 0.014] (#OOM: 0)
Epoch 1 - update       3420 => loss:  -0.272 [critic_loss: 0.006] (#OOM: 0)
Epoch 1 - update       3450 => loss:  -0.366 [critic_loss: 0.010] (#OOM: 0)
Epoch 1 - update       3480 => loss:  -0.387 [critic_loss: 0.011] (#OOM: 0)
Epoch 1 - update       3510 => loss:  -0.320 [critic_loss: 0.006] (#OOM: 0)
Epoch 1 - update       3540 => loss:  -0.470 [critic_loss: 0.018] (#OOM: 0)
Epoch 1 - update       3570 => loss:  -0.499 [critic_loss: 0.011] (#OOM: 0)
Epoch 1 - update       3600 => loss:  -0.451 [critic_loss: 0.005] (#OOM: 0)
Epoch 1 - update       3630 => loss:  -0.244 [critic_loss: 0.011] (#OOM: 0)
Epoch 1 - update       3660 => loss:  -0.499 [critic_loss: 0.012] (#OOM: 0)
Epoch 1 - update       3690 => loss:  -0.353 [critic_loss: 0.012] (#OOM: 0)
Epoch 1 - update       3720 => loss:  -0.394 [critic_loss: 0.006] (#OOM: 0)
Epoch 1 - update       3750 => loss:  -0.372 [critic_loss: 0.009] (#OOM: 0)
Epoch 1 - update       3780 => loss:  -0.553 [critic_loss: 0.016] (#OOM: 0)
Epoch 1 - update       3810 => loss:  -0.550 [critic_loss: 0.017] (#OOM: 0)
Epoch 1 - update       3840 => loss:  -0.453 [critic_loss: 0.008] (#OOM: 0)
Epoch 1 - update       3870 => loss:  -0.396 [critic_loss: 0.005] (#OOM: 0)
Epoch 1 - update       3900 => loss:  -0.638 [critic_loss: 0.036] (#OOM: 0)
Epoch 1 - update       3930 => loss:  -0.486 [critic_loss: 0.009] (#OOM: 0)
Epoch 1 - update       3960 => loss:  -0.522 [critic_loss: 0.015] (#OOM: 0)
Epoch 1 - update       3990 => loss:  -0.319 [critic_loss: 0.011] (#OOM: 0)
Epoch 1 - update       4020 => loss:  -0.377 [critic_loss: 0.004] (#OOM: 0)
Epoch 1 - update       4050 => loss:  -0.439 [critic_loss: 0.005] (#OOM: 0)
Epoch 1 - update       4080 => loss:  -0.657 [critic_loss: 0.021] (#OOM: 0)
Epoch 1 - update       4110 => loss:  -0.367 [critic_loss: 0.009] (#OOM: 0)
Epoch 1 - update       4140 => loss:  -0.345 [critic_loss: 0.015] (#OOM: 0)
Epoch 1 - update       4170 => loss:  -0.334 [critic_loss: 0.005] (#OOM: 0)
Epoch 1 - update       4200 => loss:  -0.630 [critic_loss: 0.022] (#OOM: 0)
Epoch 1 - update       4230 => loss:  -0.440 [critic_loss: 0.006] (#OOM: 0)
Epoch 1 - update       4260 => loss:  -0.408 [critic_loss: 0.011] (#OOM: 0)
Epoch 1 - update       4290 => loss:  -0.349 [critic_loss: 0.010] (#OOM: 0)
Epoch 1 - update       4320 => loss:  -0.395 [critic_loss: 0.005] (#OOM: 0)
Epoch 1 - update       4350 => loss:  -0.320 [critic_loss: 0.006] (#OOM: 0)
Epoch 1 - update       4380 => loss:  -0.591 [critic_loss: 0.013] (#OOM: 0)
Epoch 1 - update       4410 => loss:  -0.545 [critic_loss: 0.016] (#OOM: 0)
Epoch 1 - update       4440 => loss:  -0.472 [critic_loss: 0.012] (#OOM: 0)
Epoch 1 - update       4470 => loss:  -0.428 [critic_loss: 0.013] (#OOM: 0)
Epoch 1 - update       4500 => loss:  -0.514 [critic_loss: 0.007] (#OOM: 0)
Epoch 1 - update       4530 => loss:  -0.486 [critic_loss: 0.015] (#OOM: 0)
Epoch 1 - update       4560 => loss:  -0.614 [critic_loss: 0.017] (#OOM: 0)
Epoch 1 - update       4590 => loss:  -0.469 [critic_loss: 0.008] (#OOM: 0)
Epoch 1 - update       4620 => loss:  -0.321 [critic_loss: 0.012] (#OOM: 0)
Epoch 1 - update       4650 => loss:  -0.372 [critic_loss: 0.011] (#OOM: 0)
Epoch 1 - update       4680 => loss:  -0.534 [critic_loss: 0.007] (#OOM: 0)
Epoch 1 - update       4710 => loss:  -0.382 [critic_loss: 0.013] (#OOM: 0)
Epoch 1 - update       4740 => loss:  -0.464 [critic_loss: 0.009] (#OOM: 0)
Epoch 1 - update       4770 => loss:  -0.432 [critic_loss: 0.011] (#OOM: 0)
Epoch 1 - update       4800 => loss:  -0.320 [critic_loss: 0.009] (#OOM: 0)
Epoch 1 - update       4830 => loss:  -0.335 [critic_loss: 0.008] (#OOM: 0)
--> Epoch 1 finished with mean loss -0.42502
--> Overhead/Training/Evaluation: 0.84/375.12/0.00 mins (total: 375.96 mins)   (1 samples/sec)
Computing evaluation loss...
  0%|                                      | 0/183 [00:00<?, ?batch/s]  1%|▏                             | 1/183 [00:03<11:39,  3.84s/batch]  1%|▎                             | 2/183 [00:07<11:48,  3.91s/batch]  2%|▍                             | 3/183 [00:12<12:22,  4.13s/batch]  2%|▋                             | 4/183 [00:21<16:13,  5.44s/batch]  3%|▊                             | 5/183 [00:26<15:56,  5.37s/batch]  3%|▉                             | 6/183 [00:32<15:46,  5.35s/batch]  4%|█▏                            | 7/183 [00:37<15:46,  5.38s/batch]  4%|█▎                            | 8/183 [00:42<15:33,  5.34s/batch]  5%|█▍                            | 9/183 [00:47<15:22,  5.30s/batch]  5%|█▌                           | 10/183 [00:52<15:12,  5.27s/batch]  6%|█▋                           | 11/183 [00:57<15:00,  5.23s/batch]  7%|█▉                           | 12/183 [00:59<14:07,  4.96s/batch]  7%|██                           | 13/183 [01:03<13:46,  4.86s/batch]  8%|██▏                          | 14/183 [01:12<14:30,  5.15s/batch]  8%|██▍                          | 15/183 [01:18<14:41,  5.25s/batch]  9%|██▌                          | 16/183 [01:23<14:34,  5.24s/batch]  9%|██▋                          | 17/183 [01:28<14:28,  5.23s/batch] 10%|██▊                          | 18/183 [01:31<13:58,  5.08s/batch] 10%|███                          | 19/183 [01:36<13:50,  5.07s/batch] 11%|███▏                         | 20/183 [01:41<13:43,  5.05s/batch] 11%|███▎                         | 21/183 [01:45<13:36,  5.04s/batch] 12%|███▍                         | 22/183 [01:51<13:33,  5.05s/batch] 13%|███▋                         | 23/183 [01:56<13:28,  5.05s/batch] 13%|███▊                         | 24/183 [01:59<13:14,  5.00s/batch] 14%|███▉                         | 25/183 [02:03<13:02,  4.95s/batch] 14%|████                         | 26/183 [02:10<13:06,  5.01s/batch] 15%|████▎                        | 27/183 [02:18<13:19,  5.12s/batch] 15%|████▍                        | 28/183 [02:23<13:13,  5.12s/batch] 16%|████▌                        | 29/183 [02:28<13:06,  5.11s/batch] 16%|████▊                        | 30/183 [02:33<13:00,  5.10s/batch] 17%|████▉                        | 31/183 [02:38<12:55,  5.10s/batch] 17%|█████                        | 32/183 [02:42<12:48,  5.09s/batch] 18%|█████▏                       | 33/183 [02:47<12:43,  5.09s/batch] 19%|█████▍                       | 34/183 [02:52<12:38,  5.09s/batch] 19%|█████▌                       | 35/183 [02:57<12:32,  5.08s/batch] 20%|█████▋                       | 36/183 [03:01<12:21,  5.05s/batch] 20%|█████▊                       | 37/183 [03:05<12:10,  5.00s/batch] 21%|██████                       | 38/183 [03:09<12:01,  4.98s/batch] 21%|██████▏                      | 39/183 [03:13<11:52,  4.95s/batch] 22%|██████▎                      | 40/183 [03:16<11:42,  4.91s/batch] 22%|██████▍                      | 41/183 [03:19<11:32,  4.88s/batch] 23%|██████▋                      | 42/183 [03:23<11:23,  4.85s/batch] 23%|██████▊                      | 43/183 [03:31<11:28,  4.92s/batch] 24%|██████▉                      | 44/183 [03:38<11:31,  4.97s/batch] 25%|███████▏                     | 45/183 [03:44<11:27,  4.98s/batch] 25%|███████▎                     | 46/183 [03:49<11:22,  4.98s/batch] 26%|███████▍                     | 47/183 [03:54<11:18,  4.99s/batch] 26%|███████▌                     | 48/183 [03:59<11:12,  4.98s/batch] 27%|███████▊                     | 49/183 [04:04<11:08,  4.99s/batch] 27%|███████▉                     | 50/183 [04:09<11:03,  4.99s/batch] 28%|████████                     | 51/183 [04:13<10:56,  4.98s/batch] 28%|████████▏                    | 52/183 [04:17<10:48,  4.95s/batch] 29%|████████▍                    | 53/183 [04:22<10:44,  4.95s/batch] 30%|████████▌                    | 54/183 [04:31<10:48,  5.03s/batch] 30%|████████▋                    | 55/183 [04:34<10:39,  5.00s/batch] 31%|████████▊                    | 56/183 [04:39<10:34,  5.00s/batch] 31%|█████████                    | 57/183 [04:45<10:30,  5.00s/batch] 32%|█████████▏                   | 58/183 [04:50<10:25,  5.00s/batch] 32%|█████████▎                   | 59/183 [04:55<10:20,  5.01s/batch] 33%|█████████▌                   | 60/183 [05:00<10:16,  5.01s/batch] 33%|█████████▋                   | 61/183 [05:05<10:11,  5.01s/batch] 34%|█████████▊                   | 62/183 [05:10<10:06,  5.01s/batch] 34%|█████████▉                   | 63/183 [05:14<09:59,  4.99s/batch] 35%|██████████▏                  | 64/183 [05:18<09:51,  4.97s/batch] 36%|██████████▎                  | 65/183 [05:24<09:48,  4.99s/batch] 36%|██████████▍                  | 66/183 [05:31<09:48,  5.03s/batch] 37%|██████████▌                  | 67/183 [05:37<09:44,  5.04s/batch] 37%|██████████▊                  | 68/183 [05:42<09:39,  5.04s/batch] 38%|██████████▉                  | 69/183 [05:47<09:34,  5.04s/batch] 38%|███████████                  | 70/183 [05:52<09:29,  5.04s/batch] 39%|███████████▎                 | 71/183 [05:57<09:23,  5.03s/batch] 39%|███████████▍                 | 72/183 [06:02<09:19,  5.04s/batch] 40%|███████████▌                 | 73/183 [06:08<09:14,  5.04s/batch] 40%|███████████▋                 | 74/183 [06:12<09:08,  5.03s/batch] 41%|███████████▉                 | 75/183 [06:16<09:01,  5.02s/batch] 42%|████████████                 | 76/183 [06:19<08:54,  5.00s/batch] 42%|████████████▏                | 77/183 [06:23<08:48,  4.98s/batch] 43%|████████████▎                | 78/183 [06:27<08:41,  4.96s/batch] 43%|████████████▌                | 79/183 [06:31<08:34,  4.95s/batch] 44%|████████████▋                | 80/183 [06:34<08:28,  4.93s/batch] 44%|████████████▊                | 81/183 [06:38<08:21,  4.92s/batch] 45%|████████████▉                | 82/183 [06:42<08:15,  4.91s/batch] 45%|█████████████▏               | 83/183 [06:46<08:09,  4.89s/batch] 46%|█████████████▎               | 84/183 [06:50<08:03,  4.88s/batch] 46%|█████████████▍               | 85/183 [06:53<07:57,  4.87s/batch] 47%|█████████████▋               | 86/183 [06:55<07:48,  4.83s/batch] 48%|█████████████▊               | 87/183 [06:58<07:42,  4.82s/batch] 48%|█████████████▉               | 88/183 [07:03<07:36,  4.81s/batch] 49%|██████████████               | 89/183 [07:06<07:30,  4.79s/batch] 49%|██████████████▎              | 90/183 [07:10<07:24,  4.78s/batch] 50%|██████████████▍              | 91/183 [07:14<07:18,  4.77s/batch] 50%|██████████████▌              | 92/183 [07:17<07:13,  4.76s/batch] 51%|██████████████▋              | 93/183 [07:21<07:07,  4.75s/batch] 51%|██████████████▉              | 94/183 [07:25<07:01,  4.74s/batch] 52%|███████████████              | 95/183 [07:29<06:56,  4.73s/batch] 52%|███████████████▏             | 96/183 [07:33<06:50,  4.72s/batch] 53%|███████████████▎             | 97/183 [07:37<06:45,  4.71s/batch] 54%|███████████████▌             | 98/183 [07:40<06:39,  4.70s/batch] 54%|███████████████▋             | 99/183 [07:44<06:34,  4.69s/batch] 55%|███████████████▎            | 100/183 [07:48<06:28,  4.69s/batch] 55%|███████████████▍            | 101/183 [07:52<06:23,  4.68s/batch] 56%|███████████████▌            | 102/183 [07:56<06:18,  4.67s/batch] 56%|███████████████▊            | 103/183 [07:59<06:12,  4.65s/batch] 57%|███████████████▉            | 104/183 [08:02<06:06,  4.64s/batch] 57%|████████████████            | 105/183 [08:06<06:01,  4.63s/batch] 58%|████████████████▏           | 106/183 [08:10<05:55,  4.62s/batch] 58%|████████████████▎           | 107/183 [08:13<05:50,  4.61s/batch] 59%|████████████████▌           | 108/183 [08:17<05:45,  4.61s/batch] 60%|████████████████▋           | 109/183 [08:21<05:40,  4.60s/batch] 60%|████████████████▊           | 110/183 [08:25<05:35,  4.59s/batch] 61%|████████████████▉           | 111/183 [08:29<05:30,  4.59s/batch] 61%|█████████████████▏          | 112/183 [08:32<05:24,  4.58s/batch] 62%|█████████████████▎          | 113/183 [08:36<05:19,  4.57s/batch] 62%|█████████████████▍          | 114/183 [08:39<05:14,  4.55s/batch] 63%|█████████████████▌          | 115/183 [08:43<05:09,  4.55s/batch] 63%|█████████████████▋          | 116/183 [08:47<05:04,  4.54s/batch] 64%|█████████████████▉          | 117/183 [08:50<04:59,  4.54s/batch] 64%|██████████████████          | 118/183 [08:54<04:54,  4.53s/batch] 65%|██████████████████▏         | 119/183 [08:58<04:49,  4.52s/batch] 66%|██████████████████▎         | 120/183 [09:02<04:44,  4.52s/batch] 66%|██████████████████▌         | 121/183 [09:05<04:39,  4.51s/batch] 67%|██████████████████▋         | 122/183 [09:09<04:34,  4.51s/batch] 67%|██████████████████▊         | 123/183 [09:13<04:30,  4.50s/batch] 68%|██████████████████▉         | 124/183 [09:16<04:25,  4.49s/batch] 68%|███████████████████▏        | 125/183 [09:20<04:20,  4.48s/batch] 69%|███████████████████▎        | 126/183 [09:24<04:15,  4.48s/batch] 69%|███████████████████▍        | 127/183 [09:27<04:10,  4.47s/batch] 70%|███████████████████▌        | 128/183 [09:31<04:05,  4.47s/batch] 70%|███████████████████▋        | 129/183 [09:35<04:00,  4.46s/batch] 71%|███████████████████▉        | 130/183 [09:39<03:56,  4.46s/batch] 72%|████████████████████        | 131/183 [09:43<03:51,  4.45s/batch] 72%|████████████████████▏       | 132/183 [09:47<03:46,  4.45s/batch] 73%|████████████████████▎       | 133/183 [09:51<03:42,  4.44s/batch] 73%|████████████████████▌       | 134/183 [09:55<03:37,  4.44s/batch] 74%|████████████████████▋       | 135/183 [09:58<03:32,  4.43s/batch] 74%|████████████████████▊       | 136/183 [10:02<03:28,  4.43s/batch] 75%|████████████████████▉       | 137/183 [10:06<03:23,  4.43s/batch] 75%|█████████████████████       | 138/183 [10:08<03:18,  4.41s/batch] 76%|█████████████████████▎      | 139/183 [10:12<03:13,  4.41s/batch] 77%|█████████████████████▍      | 140/183 [10:16<03:09,  4.40s/batch] 77%|█████████████████████▌      | 141/183 [10:20<03:04,  4.40s/batch] 78%|█████████████████████▋      | 142/183 [10:23<03:00,  4.39s/batch] 78%|█████████████████████▉      | 143/183 [10:27<02:55,  4.39s/batch] 79%|██████████████████████      | 144/183 [10:31<02:51,  4.39s/batch] 79%|██████████████████████▏     | 145/183 [10:35<02:46,  4.38s/batch] 80%|██████████████████████▎     | 146/183 [10:36<02:41,  4.36s/batch] 80%|██████████████████████▍     | 147/183 [10:40<02:36,  4.36s/batch] 81%|██████████████████████▋     | 148/183 [10:44<02:32,  4.35s/batch] 81%|██████████████████████▊     | 149/183 [10:47<02:27,  4.35s/batch] 82%|██████████████████████▉     | 150/183 [10:49<02:22,  4.33s/batch] 83%|███████████████████████     | 151/183 [10:53<02:18,  4.33s/batch] 83%|███████████████████████▎    | 152/183 [10:57<02:14,  4.33s/batch] 84%|███████████████████████▍    | 153/183 [11:00<02:09,  4.32s/batch] 84%|███████████████████████▌    | 154/183 [11:04<02:05,  4.32s/batch] 85%|███████████████████████▋    | 155/183 [11:08<02:00,  4.31s/batch] 85%|███████████████████████▊    | 156/183 [11:11<01:56,  4.31s/batch] 86%|████████████████████████    | 157/183 [11:16<01:51,  4.31s/batch] 86%|████████████████████████▏   | 158/183 [11:20<01:47,  4.31s/batch] 87%|████████████████████████▎   | 159/183 [11:24<01:43,  4.30s/batch] 87%|████████████████████████▍   | 160/183 [11:27<01:38,  4.30s/batch] 88%|████████████████████████▋   | 161/183 [11:31<01:34,  4.30s/batch] 89%|████████████████████████▊   | 162/183 [11:35<01:30,  4.29s/batch] 89%|████████████████████████▉   | 163/183 [11:38<01:25,  4.29s/batch] 90%|█████████████████████████   | 164/183 [11:40<01:21,  4.27s/batch] 90%|█████████████████████████▏  | 165/183 [11:44<01:16,  4.27s/batch] 91%|█████████████████████████▍  | 166/183 [11:47<01:12,  4.26s/batch] 91%|█████████████████████████▌  | 167/183 [11:50<01:08,  4.26s/batch] 92%|█████████████████████████▋  | 168/183 [11:54<01:03,  4.25s/batch] 92%|█████████████████████████▊  | 169/183 [11:58<00:59,  4.25s/batch] 93%|██████████████████████████  | 170/183 [12:00<00:55,  4.24s/batch] 93%|██████████████████████████▏ | 171/183 [12:04<00:50,  4.24s/batch] 94%|██████████████████████████▎ | 172/183 [12:08<00:46,  4.24s/batch] 95%|██████████████████████████▍ | 173/183 [12:12<00:42,  4.23s/batch] 95%|██████████████████████████▌ | 174/183 [12:16<00:38,  4.23s/batch] 96%|██████████████████████████▊ | 175/183 [12:18<00:33,  4.22s/batch] 96%|██████████████████████████▉ | 176/183 [12:22<00:29,  4.22s/batch] 97%|███████████████████████████ | 177/183 [12:25<00:25,  4.21s/batch] 97%|███████████████████████████▏| 178/183 [12:27<00:21,  4.20s/batch] 98%|███████████████████████████▍| 179/183 [12:29<00:16,  4.19s/batch] 98%|███████████████████████████▌| 180/183 [12:32<00:12,  4.18s/batch] 99%|███████████████████████████▋| 181/183 [12:34<00:08,  4.17s/batch] 99%|███████████████████████████▊| 182/183 [12:36<00:04,  4.16s/batch]100%|████████████████████████████| 183/183 [12:38<00:00,  4.14s/batch]100%|████████████████████████████| 183/183 [12:38<00:00,  4.14s/batch]
Performing greedy search (args: {'k': 'hhh'})
  0%|                                       | 0/50 [00:00<?, ?batch/s]  2%|▌                              | 1/50 [00:01<01:27,  1.78s/batch]  4%|█▏                             | 2/50 [00:03<01:16,  1.60s/batch]  6%|█▊                             | 3/50 [00:04<01:05,  1.40s/batch]  8%|██▍                            | 4/50 [00:05<01:08,  1.49s/batch] 10%|███                            | 5/50 [00:07<01:08,  1.52s/batch] 12%|███▋                           | 6/50 [00:08<01:05,  1.48s/batch] 14%|████▎                          | 7/50 [00:09<01:00,  1.42s/batch] 16%|████▉                          | 8/50 [00:11<01:00,  1.44s/batch] 18%|█████▌                         | 9/50 [00:12<00:57,  1.41s/batch] 20%|██████                        | 10/50 [00:13<00:53,  1.34s/batch] 22%|██████▌                       | 11/50 [00:14<00:50,  1.29s/batch] 24%|███████▏                      | 12/50 [00:15<00:50,  1.32s/batch] 26%|███████▊                      | 13/50 [00:17<00:49,  1.35s/batch] 28%|████████▍                     | 14/50 [00:19<00:49,  1.37s/batch] 30%|█████████                     | 15/50 [00:20<00:48,  1.38s/batch] 32%|█████████▌                    | 16/50 [00:22<00:47,  1.40s/batch] 34%|██████████▏                   | 17/50 [00:23<00:45,  1.37s/batch] 36%|██████████▊                   | 18/50 [00:25<00:44,  1.39s/batch] 38%|███████████▍                  | 19/50 [00:26<00:43,  1.40s/batch] 40%|████████████                  | 20/50 [00:27<00:40,  1.36s/batch] 42%|████████████▌                 | 21/50 [00:28<00:39,  1.38s/batch] 44%|█████████████▏                | 22/50 [00:30<00:38,  1.39s/batch] 46%|█████████████▊                | 23/50 [00:31<00:37,  1.38s/batch] 48%|██████████████▍               | 24/50 [00:33<00:36,  1.39s/batch] 50%|███████████████               | 25/50 [00:34<00:34,  1.40s/batch] 52%|███████████████▌              | 26/50 [00:36<00:33,  1.41s/batch] 54%|████████████████▏             | 27/50 [00:38<00:32,  1.42s/batch] 56%|████████████████▊             | 28/50 [00:39<00:31,  1.43s/batch] 58%|█████████████████▍            | 29/50 [00:41<00:30,  1.44s/batch] 60%|██████████████████            | 30/50 [00:42<00:28,  1.41s/batch] 62%|██████████████████▌           | 31/50 [00:43<00:26,  1.40s/batch] 64%|███████████████████▏          | 32/50 [00:44<00:25,  1.41s/batch] 66%|███████████████████▊          | 33/50 [00:46<00:24,  1.41s/batch] 68%|████████████████████▍         | 34/50 [00:47<00:22,  1.40s/batch] 70%|█████████████████████         | 35/50 [00:48<00:20,  1.38s/batch] 72%|█████████████████████▌        | 36/50 [00:49<00:19,  1.37s/batch] 74%|██████████████████████▏       | 37/50 [00:50<00:17,  1.35s/batch] 76%|██████████████████████▊       | 38/50 [00:50<00:16,  1.34s/batch] 78%|███████████████████████▍      | 39/50 [00:52<00:14,  1.35s/batch] 80%|████████████████████████      | 40/50 [00:53<00:13,  1.33s/batch] 82%|████████████████████████▌     | 41/50 [00:53<00:11,  1.31s/batch] 84%|█████████████████████████▏    | 42/50 [00:55<00:10,  1.32s/batch] 86%|█████████████████████████▊    | 43/50 [00:55<00:09,  1.29s/batch] 88%|██████████████████████████▍   | 44/50 [00:57<00:07,  1.30s/batch] 90%|███████████████████████████   | 45/50 [00:57<00:06,  1.27s/batch] 92%|███████████████████████████▌  | 46/50 [00:57<00:05,  1.25s/batch] 94%|████████████████████████████▏ | 47/50 [00:59<00:03,  1.26s/batch] 96%|████████████████████████████▊ | 48/50 [01:00<00:02,  1.25s/batch] 98%|█████████████████████████████▍| 49/50 [01:01<00:01,  1.25s/batch]100%|██████████████████████████████| 50/50 [01:02<00:00,  1.24s/batch]100%|██████████████████████████████| 50/50 [01:02<00:00,  1.24s/batch]
AVL score: AVL = 2.71
AVP score: AVP = 0.74
SDiff score: SDIFF = 245.55
Validation 1 -> LOSS = -0.43
Validation 1 -> BLEU = 13.64, 34.2/17.1/10.2/5.8 (BP=1.000, ratio=1.401, hyp_len=17975, ref_len=12828)
Saving best model based on BLEU
Saving best model based on LOSS
Early stopping patience: 30
--> This is model: simrl-r43eba
--> Best LOSS so far: -0.43 @ validation 1
--> Best BLEU so far: 13.64 @ validation 1
Starting Epoch 2
Epoch 2 - update       4860 => loss:  -0.368 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       4890 => loss:  -0.429 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       4920 => loss:  -0.600 [critic_loss: 0.011] (#OOM: 0)
Epoch 2 - update       4950 => loss:  -0.637 [critic_loss: 0.014] (#OOM: 0)
Epoch 2 - update       4980 => loss:  -0.411 [critic_loss: 0.010] (#OOM: 0)
Epoch 2 - update       5010 => loss:  -0.595 [critic_loss: 0.017] (#OOM: 0)
Epoch 2 - update       5040 => loss:  -0.312 [critic_loss: 0.006] (#OOM: 0)
Epoch 2 - update       5070 => loss:  -0.512 [critic_loss: 0.018] (#OOM: 0)
Epoch 2 - update       5100 => loss:  -0.239 [critic_loss: 0.004] (#OOM: 0)
Epoch 2 - update       5130 => loss:  -0.446 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       5160 => loss:  -0.288 [critic_loss: 0.005] (#OOM: 0)
Epoch 2 - update       5190 => loss:  -0.487 [critic_loss: 0.014] (#OOM: 0)
Epoch 2 - update       5220 => loss:  -0.478 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       5250 => loss:  -0.495 [critic_loss: 0.012] (#OOM: 0)
Epoch 2 - update       5280 => loss:  -0.344 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       5310 => loss:  -0.624 [critic_loss: 0.017] (#OOM: 0)
Epoch 2 - update       5340 => loss:  -0.242 [critic_loss: 0.003] (#OOM: 0)
Epoch 2 - update       5370 => loss:  -0.637 [critic_loss: 0.018] (#OOM: 0)
Epoch 2 - update       5400 => loss:  -0.467 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       5430 => loss:  -0.406 [critic_loss: 0.010] (#OOM: 0)
Epoch 2 - update       5460 => loss:  -0.348 [critic_loss: 0.006] (#OOM: 0)
Epoch 2 - update       5490 => loss:  -0.349 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       5520 => loss:  -0.559 [critic_loss: 0.018] (#OOM: 0)
Epoch 2 - update       5550 => loss:  -0.422 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       5580 => loss:  -0.520 [critic_loss: 0.013] (#OOM: 0)
Epoch 2 - update       5610 => loss:  -0.668 [critic_loss: 0.033] (#OOM: 0)
Epoch 2 - update       5640 => loss:  -0.544 [critic_loss: 0.014] (#OOM: 0)
Epoch 2 - update       5670 => loss:  -0.418 [critic_loss: 0.011] (#OOM: 0)
Epoch 2 - update       5700 => loss:  -0.322 [critic_loss: 0.003] (#OOM: 0)
Epoch 2 - update       5730 => loss:  -0.528 [critic_loss: 0.020] (#OOM: 0)
Epoch 2 - update       5760 => loss:  -0.482 [critic_loss: 0.010] (#OOM: 0)
Epoch 2 - update       5790 => loss:  -0.300 [critic_loss: 0.005] (#OOM: 0)
Epoch 2 - update       5820 => loss:  -0.376 [critic_loss: 0.006] (#OOM: 0)
Epoch 2 - update       5850 => loss:  -0.453 [critic_loss: 0.010] (#OOM: 0)
Epoch 2 - update       5880 => loss:  -0.378 [critic_loss: 0.010] (#OOM: 0)
Epoch 2 - update       5910 => loss:  -0.540 [critic_loss: 0.010] (#OOM: 0)
Epoch 2 - update       5940 => loss:  -0.547 [critic_loss: 0.013] (#OOM: 0)
Epoch 2 - update       5970 => loss:  -0.588 [critic_loss: 0.020] (#OOM: 0)
Epoch 2 - update       6000 => loss:  -0.439 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       6030 => loss:  -0.421 [critic_loss: 0.009] (#OOM: 0)
Epoch 2 - update       6060 => loss:  -0.359 [critic_loss: 0.005] (#OOM: 0)
Epoch 2 - update       6090 => loss:  -0.393 [critic_loss: 0.006] (#OOM: 0)
Epoch 2 - update       6120 => loss:  -0.530 [critic_loss: 0.025] (#OOM: 0)
Epoch 2 - update       6150 => loss:  -0.626 [critic_loss: 0.017] (#OOM: 0)
Epoch 2 - update       6180 => loss:  -0.622 [critic_loss: 0.014] (#OOM: 0)
Epoch 2 - update       6210 => loss:  -0.403 [critic_loss: 0.010] (#OOM: 0)
Epoch 2 - update       6240 => loss:  -0.343 [critic_loss: 0.009] (#OOM: 0)
Epoch 2 - update       6270 => loss:  -0.398 [critic_loss: 0.010] (#OOM: 0)
Epoch 2 - update       6300 => loss:  -0.430 [critic_loss: 0.012] (#OOM: 0)
Epoch 2 - update       6330 => loss:  -0.369 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       6360 => loss:  -0.366 [critic_loss: 0.013] (#OOM: 0)
Epoch 2 - update       6390 => loss:  -0.474 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       6420 => loss:  -0.453 [critic_loss: 0.009] (#OOM: 0)
Epoch 2 - update       6450 => loss:  -0.518 [critic_loss: 0.012] (#OOM: 0)
Epoch 2 - update       6480 => loss:  -0.524 [critic_loss: 0.009] (#OOM: 0)
Epoch 2 - update       6510 => loss:  -0.504 [critic_loss: 0.019] (#OOM: 0)
Epoch 2 - update       6540 => loss:  -0.391 [critic_loss: 0.005] (#OOM: 0)
Epoch 2 - update       6570 => loss:  -0.303 [critic_loss: 0.012] (#OOM: 0)
Epoch 2 - update       6600 => loss:  -0.326 [critic_loss: 0.005] (#OOM: 0)
Epoch 2 - update       6630 => loss:  -0.396 [critic_loss: 0.009] (#OOM: 0)
Epoch 2 - update       6660 => loss:  -0.384 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       6690 => loss:  -0.410 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       6720 => loss:  -0.462 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       6750 => loss:  -0.273 [critic_loss: 0.014] (#OOM: 0)
Epoch 2 - update       6780 => loss:  -0.416 [critic_loss: 0.006] (#OOM: 0)
Epoch 2 - update       6810 => loss:  -0.422 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       6840 => loss:  -0.273 [critic_loss: 0.009] (#OOM: 0)
Epoch 2 - update       6870 => loss:  -0.382 [critic_loss: 0.013] (#OOM: 0)
Epoch 2 - update       6900 => loss:  -0.471 [critic_loss: 0.014] (#OOM: 0)
Epoch 2 - update       6930 => loss:  -0.300 [critic_loss: 0.012] (#OOM: 0)
Epoch 2 - update       6960 => loss:  -0.495 [critic_loss: 0.015] (#OOM: 0)
Epoch 2 - update       6990 => loss:  -0.243 [critic_loss: 0.006] (#OOM: 0)
Epoch 2 - update       7020 => loss:  -0.528 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       7050 => loss:  -0.525 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       7080 => loss:  -0.273 [critic_loss: 0.005] (#OOM: 0)
Epoch 2 - update       7110 => loss:  -0.443 [critic_loss: 0.005] (#OOM: 0)
Epoch 2 - update       7140 => loss:  -0.568 [critic_loss: 0.009] (#OOM: 0)
Epoch 2 - update       7170 => loss:  -0.409 [critic_loss: 0.014] (#OOM: 0)
Epoch 2 - update       7200 => loss:  -0.574 [critic_loss: 0.012] (#OOM: 0)
Epoch 2 - update       7230 => loss:  -0.353 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       7260 => loss:  -0.488 [critic_loss: 0.011] (#OOM: 0)
Epoch 2 - update       7290 => loss:  -0.216 [critic_loss: 0.003] (#OOM: 0)
Epoch 2 - update       7320 => loss:  -0.446 [critic_loss: 0.019] (#OOM: 0)
Epoch 2 - update       7350 => loss:  -0.468 [critic_loss: 0.005] (#OOM: 0)
Epoch 2 - update       7380 => loss:  -0.467 [critic_loss: 0.011] (#OOM: 0)
Epoch 2 - update       7410 => loss:  -0.471 [critic_loss: 0.011] (#OOM: 0)
Epoch 2 - update       7440 => loss:  -0.654 [critic_loss: 0.019] (#OOM: 0)
Epoch 2 - update       7470 => loss:  -0.339 [critic_loss: 0.012] (#OOM: 0)
Epoch 2 - update       7500 => loss:  -0.340 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       7530 => loss:  -0.330 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       7560 => loss:  -0.392 [critic_loss: 0.010] (#OOM: 0)
Epoch 2 - update       7590 => loss:  -0.451 [critic_loss: 0.013] (#OOM: 0)
Epoch 2 - update       7620 => loss:  -0.366 [critic_loss: 0.006] (#OOM: 0)
Epoch 2 - update       7650 => loss:  -0.353 [critic_loss: 0.011] (#OOM: 0)
Epoch 2 - update       7680 => loss:  -0.275 [critic_loss: 0.006] (#OOM: 0)
Epoch 2 - update       7710 => loss:  -0.484 [critic_loss: 0.015] (#OOM: 0)
Epoch 2 - update       7740 => loss:  -0.565 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       7770 => loss:  -0.362 [critic_loss: 0.003] (#OOM: 0)
Epoch 2 - update       7800 => loss:  -0.453 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       7830 => loss:  -0.540 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       7860 => loss:  -0.578 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       7890 => loss:  -0.554 [critic_loss: 0.016] (#OOM: 0)
Epoch 2 - update       7920 => loss:  -0.489 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       7950 => loss:  -0.346 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       7980 => loss:  -0.638 [critic_loss: 0.020] (#OOM: 0)
Epoch 2 - update       8010 => loss:  -0.410 [critic_loss: 0.005] (#OOM: 0)
Epoch 2 - update       8040 => loss:  -0.475 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       8070 => loss:  -0.299 [critic_loss: 0.015] (#OOM: 0)
Epoch 2 - update       8100 => loss:  -0.461 [critic_loss: 0.012] (#OOM: 0)
Epoch 2 - update       8130 => loss:  -0.437 [critic_loss: 0.009] (#OOM: 0)
Epoch 2 - update       8160 => loss:  -0.434 [critic_loss: 0.013] (#OOM: 0)
Epoch 2 - update       8190 => loss:  -0.387 [critic_loss: 0.019] (#OOM: 0)
Epoch 2 - update       8220 => loss:  -0.354 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       8250 => loss:  -0.447 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       8280 => loss:  -0.387 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       8310 => loss:  -0.450 [critic_loss: 0.009] (#OOM: 0)
Epoch 2 - update       8340 => loss:  -0.286 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       8370 => loss:  -0.459 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       8400 => loss:  -0.397 [critic_loss: 0.005] (#OOM: 0)
Epoch 2 - update       8430 => loss:  -0.339 [critic_loss: 0.004] (#OOM: 0)
Epoch 2 - update       8460 => loss:  -0.442 [critic_loss: 0.011] (#OOM: 0)
Epoch 2 - update       8490 => loss:  -0.458 [critic_loss: 0.009] (#OOM: 0)
Epoch 2 - update       8520 => loss:  -0.631 [critic_loss: 0.017] (#OOM: 0)
Epoch 2 - update       8550 => loss:  -0.364 [critic_loss: 0.005] (#OOM: 0)
Epoch 2 - update       8580 => loss:  -0.340 [critic_loss: 0.011] (#OOM: 0)
Epoch 2 - update       8610 => loss:  -0.640 [critic_loss: 0.014] (#OOM: 0)
Epoch 2 - update       8640 => loss:  -0.334 [critic_loss: 0.005] (#OOM: 0)
Epoch 2 - update       8670 => loss:  -0.368 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       8700 => loss:  -0.498 [critic_loss: 0.013] (#OOM: 0)
Epoch 2 - update       8730 => loss:  -0.622 [critic_loss: 0.014] (#OOM: 0)
Epoch 2 - update       8760 => loss:  -0.313 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       8790 => loss:  -0.350 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       8820 => loss:  -0.534 [critic_loss: 0.022] (#OOM: 0)
Epoch 2 - update       8850 => loss:  -0.355 [critic_loss: 0.005] (#OOM: 0)
Epoch 2 - update       8880 => loss:  -0.547 [critic_loss: 0.010] (#OOM: 0)
Epoch 2 - update       8910 => loss:  -0.337 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       8940 => loss:  -0.431 [critic_loss: 0.010] (#OOM: 0)
Epoch 2 - update       8970 => loss:  -0.277 [critic_loss: 0.005] (#OOM: 0)
Epoch 2 - update       9000 => loss:  -0.360 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       9030 => loss:  -0.451 [critic_loss: 0.004] (#OOM: 0)
Epoch 2 - update       9060 => loss:  -0.647 [critic_loss: 0.015] (#OOM: 0)
Epoch 2 - update       9090 => loss:  -0.517 [critic_loss: 0.015] (#OOM: 0)
Epoch 2 - update       9120 => loss:  -0.372 [critic_loss: 0.004] (#OOM: 0)
Epoch 2 - update       9150 => loss:  -0.619 [critic_loss: 0.014] (#OOM: 0)
Epoch 2 - update       9180 => loss:  -0.499 [critic_loss: 0.011] (#OOM: 0)
Epoch 2 - update       9210 => loss:  -0.318 [critic_loss: 0.006] (#OOM: 0)
Epoch 2 - update       9240 => loss:  -0.409 [critic_loss: 0.009] (#OOM: 0)
Epoch 2 - update       9270 => loss:  -0.348 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       9300 => loss:  -0.452 [critic_loss: 0.011] (#OOM: 0)
Epoch 2 - update       9330 => loss:  -0.624 [critic_loss: 0.025] (#OOM: 0)
Epoch 2 - update       9360 => loss:  -0.417 [critic_loss: 0.011] (#OOM: 0)
Epoch 2 - update       9390 => loss:  -0.391 [critic_loss: 0.009] (#OOM: 0)
Epoch 2 - update       9420 => loss:  -0.416 [critic_loss: 0.009] (#OOM: 0)
Epoch 2 - update       9450 => loss:  -0.565 [critic_loss: 0.015] (#OOM: 0)
Epoch 2 - update       9480 => loss:  -0.402 [critic_loss: 0.007] (#OOM: 0)
Epoch 2 - update       9510 => loss:  -0.472 [critic_loss: 0.018] (#OOM: 0)
Epoch 2 - update       9540 => loss:  -0.435 [critic_loss: 0.005] (#OOM: 0)
Epoch 2 - update       9570 => loss:  -0.649 [critic_loss: 0.013] (#OOM: 0)
Epoch 2 - update       9600 => loss:  -0.378 [critic_loss: 0.008] (#OOM: 0)
Epoch 2 - update       9630 => loss:  -0.435 [critic_loss: 0.010] (#OOM: 0)
Epoch 2 - update       9660 => loss:  -0.413 [critic_loss: 0.020] (#OOM: 0)
Epoch 2 - update       9690 => loss:  -0.444 [critic_loss: 0.007] (#OOM: 0)
--> Epoch 2 finished with mean loss -0.42659
--> Overhead/Training/Evaluation: 0.90/372.33/0.00 mins (total: 373.23 mins)   (1 samples/sec)
Computing evaluation loss...
  0%|                                      | 0/183 [00:00<?, ?batch/s]  1%|▏                             | 1/183 [00:03<11:25,  3.77s/batch]  1%|▎                             | 2/183 [00:07<11:45,  3.90s/batch]  2%|▍                             | 3/183 [00:11<11:42,  3.90s/batch]  2%|▋                             | 4/183 [00:15<11:54,  3.99s/batch]  3%|▊                             | 5/183 [00:19<11:46,  3.97s/batch]  3%|▉                             | 6/183 [00:23<11:32,  3.91s/batch]  4%|█▏                            | 7/183 [00:25<10:34,  3.61s/batch]  4%|█▎                            | 8/183 [00:29<10:35,  3.63s/batch]  5%|█▍                            | 9/183 [00:32<10:37,  3.66s/batch]  5%|█▌                           | 10/183 [00:36<10:33,  3.66s/batch]  6%|█▋                           | 11/183 [00:40<10:29,  3.66s/batch]  7%|█▉                           | 12/183 [00:44<10:32,  3.70s/batch]  7%|██                           | 13/183 [00:48<10:32,  3.72s/batch]  8%|██▏                          | 14/183 [00:52<10:32,  3.74s/batch]  8%|██▍                          | 15/183 [00:56<10:32,  3.76s/batch]  9%|██▌                          | 16/183 [01:00<10:28,  3.76s/batch]  9%|██▋                          | 17/183 [01:03<10:24,  3.76s/batch] 10%|██▊                          | 18/183 [01:07<10:20,  3.76s/batch] 10%|███                          | 19/183 [01:11<10:21,  3.79s/batch] 11%|███▏                         | 20/183 [01:15<10:17,  3.79s/batch] 11%|███▎                         | 21/183 [01:19<10:13,  3.78s/batch] 12%|███▍                         | 22/183 [01:23<10:08,  3.78s/batch] 13%|███▋                         | 23/183 [01:26<10:03,  3.77s/batch] 13%|███▊                         | 24/183 [01:30<10:01,  3.78s/batch] 14%|███▉                         | 25/183 [01:34<09:59,  3.79s/batch] 14%|████                         | 26/183 [01:36<09:42,  3.71s/batch] 15%|████▎                        | 27/183 [01:40<09:38,  3.71s/batch] 15%|████▍                        | 28/183 [01:43<09:32,  3.69s/batch] 16%|████▌                        | 29/183 [01:47<09:30,  3.70s/batch] 16%|████▊                        | 30/183 [01:51<09:28,  3.71s/batch] 17%|████▉                        | 31/183 [01:55<09:27,  3.73s/batch] 17%|█████                        | 32/183 [01:59<09:23,  3.73s/batch] 18%|█████▏                       | 33/183 [02:03<09:20,  3.74s/batch] 19%|█████▍                       | 34/183 [02:05<09:07,  3.68s/batch] 19%|█████▌                       | 35/183 [02:08<09:02,  3.67s/batch] 20%|█████▋                       | 36/183 [02:10<08:51,  3.61s/batch] 20%|█████▊                       | 37/183 [02:13<08:48,  3.62s/batch] 21%|██████                       | 38/183 [02:17<08:45,  3.62s/batch] 21%|██████▏                      | 39/183 [02:21<08:41,  3.62s/batch] 22%|██████▎                      | 40/183 [02:25<08:40,  3.64s/batch] 22%|██████▍                      | 41/183 [02:29<08:37,  3.64s/batch] 23%|██████▋                      | 42/183 [02:33<08:34,  3.65s/batch] 23%|██████▊                      | 43/183 [02:36<08:30,  3.64s/batch] 24%|██████▉                      | 44/183 [02:40<08:26,  3.65s/batch] 25%|███████▏                     | 45/183 [02:44<08:25,  3.66s/batch] 25%|███████▎                     | 46/183 [02:48<08:21,  3.66s/batch] 26%|███████▍                     | 47/183 [02:51<08:17,  3.66s/batch] 26%|███████▌                     | 48/183 [02:56<08:15,  3.67s/batch] 27%|███████▊                     | 49/183 [03:00<08:12,  3.68s/batch] 27%|███████▉                     | 50/183 [03:03<08:09,  3.68s/batch] 28%|████████                     | 51/183 [03:08<08:06,  3.69s/batch] 28%|████████▏                    | 52/183 [03:12<08:03,  3.69s/batch] 29%|████████▍                    | 53/183 [03:16<08:01,  3.70s/batch] 30%|████████▌                    | 54/183 [03:20<07:57,  3.70s/batch] 30%|████████▋                    | 55/183 [03:23<07:53,  3.70s/batch] 31%|████████▊                    | 56/183 [03:27<07:51,  3.71s/batch] 31%|█████████                    | 57/183 [03:29<07:42,  3.67s/batch] 32%|█████████▏                   | 58/183 [03:33<07:39,  3.67s/batch] 32%|█████████▎                   | 59/183 [03:37<07:36,  3.68s/batch] 33%|█████████▌                   | 60/183 [03:41<07:33,  3.69s/batch] 33%|█████████▋                   | 61/183 [03:45<07:30,  3.69s/batch] 34%|█████████▊                   | 62/183 [03:49<07:27,  3.70s/batch] 34%|█████████▉                   | 63/183 [03:52<07:23,  3.70s/batch] 35%|██████████▏                  | 64/183 [03:56<07:20,  3.70s/batch] 36%|██████████▎                  | 65/183 [04:01<07:17,  3.71s/batch] 36%|██████████▍                  | 66/183 [04:05<07:14,  3.71s/batch] 37%|██████████▌                  | 67/183 [04:09<07:11,  3.72s/batch] 37%|██████████▊                  | 68/183 [04:12<07:07,  3.71s/batch] 38%|██████████▉                  | 69/183 [04:17<07:04,  3.72s/batch] 38%|███████████                  | 70/183 [04:21<07:01,  3.73s/batch] 39%|███████████▎                 | 71/183 [04:25<06:58,  3.73s/batch] 39%|███████████▍                 | 72/183 [04:29<06:55,  3.74s/batch] 40%|███████████▌                 | 73/183 [04:32<06:51,  3.74s/batch] 40%|███████████▋                 | 74/183 [04:36<06:47,  3.74s/batch] 41%|███████████▉                 | 75/183 [04:40<06:44,  3.74s/batch] 42%|████████████                 | 76/183 [04:44<06:40,  3.74s/batch] 42%|████████████▏                | 77/183 [04:48<06:37,  3.75s/batch] 43%|████████████▎                | 78/183 [04:51<06:32,  3.74s/batch] 43%|████████████▌                | 79/183 [04:55<06:29,  3.74s/batch] 44%|████████████▋                | 80/183 [04:59<06:25,  3.74s/batch] 44%|████████████▊                | 81/183 [05:03<06:22,  3.75s/batch] 45%|████████████▉                | 82/183 [05:07<06:18,  3.74s/batch] 45%|█████████████▏               | 83/183 [05:10<06:14,  3.75s/batch] 46%|█████████████▎               | 84/183 [05:14<06:11,  3.75s/batch] 46%|█████████████▍               | 85/183 [05:18<06:07,  3.75s/batch] 47%|█████████████▋               | 86/183 [05:22<06:03,  3.75s/batch] 48%|█████████████▊               | 87/183 [05:25<05:59,  3.75s/batch] 48%|█████████████▉               | 88/183 [05:29<05:55,  3.75s/batch] 49%|██████████████               | 89/183 [05:33<05:52,  3.75s/batch] 49%|██████████████▎              | 90/183 [05:37<05:48,  3.75s/batch] 50%|██████████████▍              | 91/183 [05:41<05:44,  3.75s/batch] 50%|██████████████▌              | 92/183 [05:45<05:41,  3.75s/batch] 51%|██████████████▋              | 93/183 [05:47<05:36,  3.74s/batch] 51%|██████████████▉              | 94/183 [05:51<05:32,  3.74s/batch] 52%|███████████████              | 95/183 [05:54<05:28,  3.74s/batch] 52%|███████████████▏             | 96/183 [05:58<05:24,  3.73s/batch] 53%|███████████████▎             | 97/183 [06:02<05:21,  3.74s/batch] 54%|███████████████▌             | 98/183 [06:06<05:18,  3.74s/batch] 54%|███████████████▋             | 99/183 [06:10<05:14,  3.74s/batch] 55%|███████████████▎            | 100/183 [06:14<05:10,  3.74s/batch] 55%|███████████████▍            | 101/183 [06:18<05:07,  3.75s/batch] 56%|███████████████▌            | 102/183 [06:21<05:03,  3.74s/batch] 56%|███████████████▊            | 103/183 [06:25<04:59,  3.74s/batch] 57%|███████████████▉            | 104/183 [06:29<04:55,  3.75s/batch] 57%|████████████████            | 105/183 [06:33<04:52,  3.74s/batch] 58%|████████████████▏           | 106/183 [06:37<04:48,  3.75s/batch] 58%|████████████████▎           | 107/183 [06:40<04:44,  3.75s/batch] 59%|████████████████▌           | 108/183 [06:44<04:41,  3.75s/batch] 60%|████████████████▋           | 109/183 [06:48<04:37,  3.75s/batch] 60%|████████████████▊           | 110/183 [06:52<04:33,  3.75s/batch] 61%|████████████████▉           | 111/183 [06:55<04:29,  3.75s/batch] 61%|█████████████████▏          | 112/183 [06:59<04:26,  3.75s/batch] 62%|█████████████████▎          | 113/183 [07:03<04:22,  3.75s/batch] 62%|█████████████████▍          | 114/183 [07:07<04:18,  3.75s/batch] 63%|█████████████████▌          | 115/183 [07:11<04:15,  3.75s/batch] 63%|█████████████████▋          | 116/183 [07:15<04:11,  3.75s/batch] 64%|█████████████████▉          | 117/183 [07:19<04:07,  3.75s/batch] 64%|██████████████████          | 118/183 [07:21<04:03,  3.74s/batch] 65%|██████████████████▏         | 119/183 [07:25<03:59,  3.75s/batch] 66%|██████████████████▎         | 120/183 [07:29<03:55,  3.74s/batch] 66%|██████████████████▌         | 121/183 [07:33<03:52,  3.75s/batch] 67%|██████████████████▋         | 122/183 [07:36<03:48,  3.74s/batch] 67%|██████████████████▊         | 123/183 [07:40<03:44,  3.74s/batch] 68%|██████████████████▉         | 124/183 [07:43<03:40,  3.74s/batch] 68%|███████████████████▏        | 125/183 [07:47<03:37,  3.74s/batch] 69%|███████████████████▎        | 126/183 [07:51<03:33,  3.74s/batch] 69%|███████████████████▍        | 127/183 [07:54<03:29,  3.74s/batch] 70%|███████████████████▌        | 128/183 [07:58<03:25,  3.74s/batch] 70%|███████████████████▋        | 129/183 [08:02<03:21,  3.74s/batch] 71%|███████████████████▉        | 130/183 [08:06<03:18,  3.74s/batch] 72%|████████████████████        | 131/183 [08:09<03:14,  3.74s/batch] 72%|████████████████████▏       | 132/183 [08:13<03:10,  3.74s/batch] 73%|████████████████████▎       | 133/183 [08:16<03:06,  3.73s/batch] 73%|████████████████████▌       | 134/183 [08:20<03:02,  3.73s/batch] 74%|████████████████████▋       | 135/183 [08:24<02:59,  3.74s/batch] 74%|████████████████████▊       | 136/183 [08:28<02:55,  3.74s/batch] 75%|████████████████████▉       | 137/183 [08:30<02:51,  3.73s/batch] 75%|█████████████████████       | 138/183 [08:34<02:47,  3.73s/batch] 76%|█████████████████████▎      | 139/183 [08:37<02:43,  3.73s/batch] 77%|█████████████████████▍      | 140/183 [08:41<02:40,  3.73s/batch] 77%|█████████████████████▌      | 141/183 [08:44<02:36,  3.72s/batch] 78%|█████████████████████▋      | 142/183 [08:47<02:32,  3.72s/batch] 78%|█████████████████████▉      | 143/183 [08:51<02:28,  3.72s/batch] 79%|██████████████████████      | 144/183 [08:53<02:24,  3.71s/batch] 79%|██████████████████████▏     | 145/183 [08:57<02:20,  3.71s/batch] 80%|██████████████████████▎     | 146/183 [09:01<02:17,  3.71s/batch] 80%|██████████████████████▍     | 147/183 [09:05<02:13,  3.71s/batch] 81%|██████████████████████▋     | 148/183 [09:09<02:09,  3.71s/batch] 81%|██████████████████████▊     | 149/183 [09:12<02:06,  3.71s/batch] 82%|██████████████████████▉     | 150/183 [09:16<02:02,  3.71s/batch] 83%|███████████████████████     | 151/183 [09:20<01:58,  3.71s/batch] 83%|███████████████████████▎    | 152/183 [09:24<01:55,  3.71s/batch] 84%|███████████████████████▍    | 153/183 [09:27<01:51,  3.71s/batch] 84%|███████████████████████▌    | 154/183 [09:31<01:47,  3.71s/batch] 85%|███████████████████████▋    | 155/183 [09:34<01:43,  3.70s/batch] 85%|███████████████████████▊    | 156/183 [09:37<01:40,  3.70s/batch] 86%|████████████████████████    | 157/183 [09:41<01:36,  3.71s/batch] 86%|████████████████████████▏   | 158/183 [09:45<01:32,  3.71s/batch] 87%|████████████████████████▎   | 159/183 [09:48<01:28,  3.70s/batch] 87%|████████████████████████▍   | 160/183 [09:51<01:25,  3.70s/batch] 88%|████████████████████████▋   | 161/183 [09:55<01:21,  3.70s/batch] 89%|████████████████████████▊   | 162/183 [09:59<01:17,  3.70s/batch] 89%|████████████████████████▉   | 163/183 [10:02<01:13,  3.70s/batch] 90%|█████████████████████████   | 164/183 [10:06<01:10,  3.70s/batch] 90%|█████████████████████████▏  | 165/183 [10:09<01:06,  3.69s/batch] 91%|█████████████████████████▍  | 166/183 [10:13<01:02,  3.69s/batch] 91%|█████████████████████████▌  | 167/183 [10:16<00:59,  3.69s/batch] 92%|█████████████████████████▋  | 168/183 [10:19<00:55,  3.69s/batch] 92%|█████████████████████████▊  | 169/183 [10:23<00:51,  3.69s/batch] 93%|██████████████████████████  | 170/183 [10:26<00:47,  3.68s/batch] 93%|██████████████████████████▏ | 171/183 [10:29<00:44,  3.68s/batch] 94%|██████████████████████████▎ | 172/183 [10:33<00:40,  3.68s/batch] 95%|██████████████████████████▍ | 173/183 [10:34<00:36,  3.67s/batch] 95%|██████████████████████████▌ | 174/183 [10:38<00:33,  3.67s/batch] 96%|██████████████████████████▊ | 175/183 [10:41<00:29,  3.67s/batch] 96%|██████████████████████████▉ | 176/183 [10:45<00:25,  3.67s/batch] 97%|███████████████████████████ | 177/183 [10:48<00:21,  3.67s/batch] 97%|███████████████████████████▏| 178/183 [10:51<00:18,  3.66s/batch] 98%|███████████████████████████▍| 179/183 [10:53<00:14,  3.65s/batch] 98%|███████████████████████████▌| 180/183 [10:55<00:10,  3.64s/batch] 99%|███████████████████████████▋| 181/183 [10:57<00:07,  3.63s/batch] 99%|███████████████████████████▊| 182/183 [10:59<00:03,  3.62s/batch]100%|████████████████████████████| 183/183 [11:01<00:00,  3.61s/batch]100%|████████████████████████████| 183/183 [11:01<00:00,  3.61s/batch]
Performing greedy search (args: {'k': 'hhh'})
  0%|                                       | 0/50 [00:00<?, ?batch/s]  2%|▌                              | 1/50 [00:01<01:08,  1.41s/batch]  4%|█▏                             | 2/50 [00:03<01:18,  1.65s/batch]  6%|█▊                             | 3/50 [00:04<01:05,  1.40s/batch]  8%|██▍                            | 4/50 [00:05<01:08,  1.50s/batch] 10%|███                            | 5/50 [00:07<01:09,  1.54s/batch] 12%|███▋                           | 6/50 [00:08<01:01,  1.40s/batch] 14%|████▎                          | 7/50 [00:09<00:56,  1.32s/batch] 16%|████▉                          | 8/50 [00:09<00:52,  1.24s/batch] 18%|█████▌                         | 9/50 [00:11<00:50,  1.24s/batch] 20%|██████                        | 10/50 [00:12<00:51,  1.29s/batch] 22%|██████▌                       | 11/50 [00:13<00:47,  1.21s/batch] 24%|███████▏                      | 12/50 [00:14<00:45,  1.19s/batch] 26%|███████▊                      | 13/50 [00:15<00:45,  1.23s/batch] 28%|████████▍                     | 14/50 [00:17<00:44,  1.25s/batch] 30%|█████████                     | 15/50 [00:18<00:44,  1.26s/batch] 32%|█████████▌                    | 16/50 [00:20<00:43,  1.29s/batch] 34%|██████████▏                   | 17/50 [00:22<00:43,  1.32s/batch] 36%|██████████▊                   | 18/50 [00:23<00:41,  1.31s/batch] 38%|███████████▍                  | 19/50 [00:24<00:39,  1.28s/batch] 40%|████████████                  | 20/50 [00:26<00:39,  1.30s/batch] 42%|████████████▌                 | 21/50 [00:26<00:36,  1.28s/batch] 44%|█████████████▏                | 22/50 [00:27<00:35,  1.26s/batch] 46%|█████████████▊                | 23/50 [00:29<00:34,  1.28s/batch] 48%|██████████████▍               | 24/50 [00:31<00:33,  1.30s/batch] 50%|███████████████               | 25/50 [00:31<00:31,  1.26s/batch] 52%|███████████████▌              | 26/50 [00:32<00:29,  1.24s/batch] 54%|████████████████▏             | 27/50 [00:33<00:28,  1.25s/batch] 56%|████████████████▊             | 28/50 [00:34<00:27,  1.24s/batch] 58%|█████████████████▍            | 29/50 [00:36<00:26,  1.25s/batch] 60%|██████████████████            | 30/50 [00:37<00:24,  1.23s/batch] 62%|██████████████████▌           | 31/50 [00:38<00:23,  1.24s/batch] 64%|███████████████████▏          | 32/50 [00:40<00:22,  1.26s/batch] 66%|███████████████████▊          | 33/50 [00:41<00:21,  1.26s/batch] 68%|████████████████████▍         | 34/50 [00:43<00:20,  1.27s/batch] 70%|█████████████████████         | 35/50 [00:44<00:18,  1.26s/batch] 72%|█████████████████████▌        | 36/50 [00:45<00:17,  1.27s/batch] 74%|██████████████████████▏       | 37/50 [00:46<00:16,  1.25s/batch] 76%|██████████████████████▊       | 38/50 [00:47<00:15,  1.25s/batch] 78%|███████████████████████▍      | 39/50 [00:48<00:13,  1.24s/batch] 80%|████████████████████████      | 40/50 [00:50<00:12,  1.25s/batch] 82%|████████████████████████▌     | 41/50 [00:50<00:11,  1.24s/batch] 84%|█████████████████████████▏    | 42/50 [00:51<00:09,  1.23s/batch] 86%|█████████████████████████▊    | 43/50 [00:53<00:08,  1.24s/batch] 88%|██████████████████████████▍   | 44/50 [00:55<00:07,  1.25s/batch] 90%|███████████████████████████   | 45/50 [00:55<00:06,  1.24s/batch] 92%|███████████████████████████▌  | 46/50 [00:56<00:04,  1.23s/batch] 94%|████████████████████████████▏ | 47/50 [00:57<00:03,  1.23s/batch] 96%|████████████████████████████▊ | 48/50 [00:58<00:02,  1.21s/batch] 98%|█████████████████████████████▍| 49/50 [00:58<00:01,  1.20s/batch]100%|██████████████████████████████| 50/50 [00:59<00:00,  1.19s/batch]100%|██████████████████████████████| 50/50 [00:59<00:00,  1.19s/batch]
AVL score: AVL = 2.99
AVP score: AVP = 0.73
SDiff score: SDIFF = 217.19
Validation 2 -> LOSS = -0.44
Validation 2 -> BLEU = 16.37, 37.7/20.0/12.3/7.7 (BP=1.000, ratio=1.368, hyp_len=17545, ref_len=12828)
Saving best model based on BLEU
Saving best model based on LOSS
Early stopping patience: 30
--> This is model: simrl-r43eba
--> Best LOSS so far: -0.44 @ validation 2
--> Best BLEU so far: 16.37 @ validation 2
Starting Epoch 3
Epoch 3 - update       9720 => loss:  -0.521 [critic_loss: 0.022] (#OOM: 0)
Epoch 3 - update       9750 => loss:  -0.460 [critic_loss: 0.007] (#OOM: 0)
Epoch 3 - update       9780 => loss:  -0.451 [critic_loss: 0.032] (#OOM: 0)
Epoch 3 - update       9810 => loss:  -0.263 [critic_loss: 0.008] (#OOM: 0)
Epoch 3 - update       9840 => loss:  -0.339 [critic_loss: 0.009] (#OOM: 0)
Epoch 3 - update       9870 => loss:  -0.375 [critic_loss: 0.011] (#OOM: 0)
Epoch 3 - update       9900 => loss:  -0.398 [critic_loss: 0.006] (#OOM: 0)
Epoch 3 - update       9930 => loss:  -0.410 [critic_loss: 0.013] (#OOM: 0)
Epoch 3 - update       9960 => loss:  -0.359 [critic_loss: 0.009] (#OOM: 0)
Epoch 3 - update       9990 => loss:  -0.363 [critic_loss: 0.005] (#OOM: 0)
Epoch 3 - update      10020 => loss:  -0.272 [critic_loss: 0.004] (#OOM: 0)
Epoch 3 - update      10050 => loss:  -0.287 [critic_loss: 0.002] (#OOM: 0)
Epoch 3 - update      10080 => loss:  -0.346 [critic_loss: 0.003] (#OOM: 0)
Epoch 3 - update      10110 => loss:  -0.573 [critic_loss: 0.013] (#OOM: 0)
Epoch 3 - update      10140 => loss:  -0.440 [critic_loss: 0.005] (#OOM: 0)
Epoch 3 - update      10170 => loss:  -0.370 [critic_loss: 0.016] (#OOM: 0)
Epoch 3 - update      10200 => loss:  -0.443 [critic_loss: 0.015] (#OOM: 0)
Epoch 3 - update      10230 => loss:  -0.606 [critic_loss: 0.017] (#OOM: 0)
Epoch 3 - update      10260 => loss:  -0.405 [critic_loss: 0.012] (#OOM: 0)
Epoch 3 - update      10290 => loss:  -0.405 [critic_loss: 0.014] (#OOM: 0)
Epoch 3 - update      10320 => loss:  -0.277 [critic_loss: 0.009] (#OOM: 0)
Epoch 3 - update      10350 => loss:  -0.552 [critic_loss: 0.009] (#OOM: 0)
Epoch 3 - update      10380 => loss:  -0.511 [critic_loss: 0.011] (#OOM: 0)
Epoch 3 - update      10410 => loss:  -0.495 [critic_loss: 0.010] (#OOM: 0)
Epoch 3 - update      10440 => loss:  -0.460 [critic_loss: 0.013] (#OOM: 0)
Epoch 3 - update      10470 => loss:  -0.446 [critic_loss: 0.008] (#OOM: 0)
Epoch 3 - update      10500 => loss:  -0.365 [critic_loss: 0.018] (#OOM: 0)
Epoch 3 - update      10530 => loss:  -0.535 [critic_loss: 0.011] (#OOM: 0)
Epoch 3 - update      10560 => loss:  -0.291 [critic_loss: 0.005] (#OOM: 0)
Epoch 3 - update      10590 => loss:  -0.508 [critic_loss: 0.009] (#OOM: 0)
Epoch 3 - update      10620 => loss:  -0.311 [critic_loss: 0.008] (#OOM: 0)
Epoch 3 - update      10650 => loss:  -0.310 [critic_loss: 0.005] (#OOM: 0)
Epoch 3 - update      10680 => loss:  -0.388 [critic_loss: 0.006] (#OOM: 0)
Epoch 3 - update      10710 => loss:  -0.408 [critic_loss: 0.006] (#OOM: 0)
Epoch 3 - update      10740 => loss:  -0.514 [critic_loss: 0.014] (#OOM: 0)
Epoch 3 - update      10770 => loss:  -0.480 [critic_loss: 0.008] (#OOM: 0)
Epoch 3 - update      10800 => loss:  -0.346 [critic_loss: 0.004] (#OOM: 0)
Epoch 3 - update      10830 => loss:  -0.340 [critic_loss: 0.005] (#OOM: 0)
Epoch 3 - update      10860 => loss:  -0.301 [critic_loss: 0.003] (#OOM: 0)
Epoch 3 - update      10890 => loss:  -0.378 [critic_loss: 0.005] (#OOM: 0)
Epoch 3 - update      10920 => loss:  -0.343 [critic_loss: 0.016] (#OOM: 0)
Epoch 3 - update      10950 => loss:  -0.464 [critic_loss: 0.007] (#OOM: 0)
Epoch 3 - update      10980 => loss:  -0.291 [critic_loss: 0.005] (#OOM: 0)
Epoch 3 - update      11010 => loss:  -0.320 [critic_loss: 0.008] (#OOM: 0)
Epoch 3 - update      11040 => loss:  -0.235 [critic_loss: 0.009] (#OOM: 0)
Epoch 3 - update      11070 => loss:  -0.419 [critic_loss: 0.011] (#OOM: 0)
Epoch 3 - update      11100 => loss:  -0.401 [critic_loss: 0.010] (#OOM: 0)
Epoch 3 - update      11130 => loss:  -0.621 [critic_loss: 0.018] (#OOM: 0)
Epoch 3 - update      11160 => loss:  -0.320 [critic_loss: 0.005] (#OOM: 0)
Epoch 3 - update      11190 => loss:  -0.567 [critic_loss: 0.016] (#OOM: 0)
Epoch 3 - update      11220 => loss:  -0.434 [critic_loss: 0.018] (#OOM: 0)
Epoch 3 - update      11250 => loss:  -0.494 [critic_loss: 0.009] (#OOM: 0)
Epoch 3 - update      11280 => loss:  -0.459 [critic_loss: 0.012] (#OOM: 0)
Epoch 3 - update      11310 => loss:  -0.362 [critic_loss: 0.006] (#OOM: 0)
Epoch 3 - update      11340 => loss:  -0.361 [critic_loss: 0.009] (#OOM: 0)
Epoch 3 - update      11370 => loss:  -0.511 [critic_loss: 0.012] (#OOM: 0)
Epoch 3 - update      11400 => loss:  -0.469 [critic_loss: 0.006] (#OOM: 0)
Epoch 3 - update      11430 => loss:  -0.347 [critic_loss: 0.008] (#OOM: 0)
Epoch 3 - update      11460 => loss:  -0.593 [critic_loss: 0.011] (#OOM: 0)
Epoch 3 - update      11490 => loss:  -0.323 [critic_loss: 0.005] (#OOM: 0)
Epoch 3 - update      11520 => loss:  -0.461 [critic_loss: 0.018] (#OOM: 0)
Epoch 3 - update      11550 => loss:  -0.410 [critic_loss: 0.004] (#OOM: 0)
Epoch 3 - update      11580 => loss:  -0.409 [critic_loss: 0.008] (#OOM: 0)
Epoch 3 - update      11610 => loss:  -0.517 [critic_loss: 0.015] (#OOM: 0)
Epoch 3 - update      11640 => loss:  -0.627 [critic_loss: 0.010] (#OOM: 0)
Epoch 3 - update      11670 => loss:  -0.296 [critic_loss: 0.005] (#OOM: 0)
Epoch 3 - update      11700 => loss:  -0.253 [critic_loss: 0.004] (#OOM: 0)
Epoch 3 - update      11730 => loss:  -0.461 [critic_loss: 0.011] (#OOM: 0)
Epoch 3 - update      11760 => loss:  -0.623 [critic_loss: 0.015] (#OOM: 0)
Epoch 3 - update      11790 => loss:  -0.366 [critic_loss: 0.010] (#OOM: 0)
Epoch 3 - update      11820 => loss:  -0.587 [critic_loss: 0.014] (#OOM: 0)
Epoch 3 - update      11850 => loss:  -0.371 [critic_loss: 0.005] (#OOM: 0)
Epoch 3 - update      11880 => loss:  -0.505 [critic_loss: 0.008] (#OOM: 0)
Epoch 3 - update      11910 => loss:  -0.340 [critic_loss: 0.011] (#OOM: 0)
Epoch 3 - update      11940 => loss:  -0.629 [critic_loss: 0.013] (#OOM: 0)
Epoch 3 - update      11970 => loss:  -0.678 [critic_loss: 0.020] (#OOM: 0)
Epoch 3 - update      12000 => loss:  -0.552 [critic_loss: 0.015] (#OOM: 0)
Epoch 3 - update      12030 => loss:  -0.546 [critic_loss: 0.016] (#OOM: 0)
Epoch 3 - update      12060 => loss:  -0.400 [critic_loss: 0.007] (#OOM: 0)
Epoch 3 - update      12090 => loss:  -0.579 [critic_loss: 0.015] (#OOM: 0)
Epoch 3 - update      12120 => loss:  -0.411 [critic_loss: 0.009] (#OOM: 0)
Epoch 3 - update      12150 => loss:  -0.543 [critic_loss: 0.013] (#OOM: 0)
Epoch 3 - update      12180 => loss:  -0.648 [critic_loss: 0.011] (#OOM: 0)
Epoch 3 - update      12210 => loss:  -0.521 [critic_loss: 0.010] (#OOM: 0)
Epoch 3 - update      12240 => loss:  -0.586 [critic_loss: 0.012] (#OOM: 0)
Epoch 3 - update      12270 => loss:  -0.551 [critic_loss: 0.015] (#OOM: 0)
Epoch 3 - update      12300 => loss:  -0.315 [critic_loss: 0.007] (#OOM: 0)
Epoch 3 - update      12330 => loss:  -0.439 [critic_loss: 0.007] (#OOM: 0)
Epoch 3 - update      12360 => loss:  -0.611 [critic_loss: 0.016] (#OOM: 0)
Epoch 3 - update      12390 => loss:  -0.649 [critic_loss: 0.015] (#OOM: 0)
Epoch 3 - update      12420 => loss:  -0.523 [critic_loss: 0.020] (#OOM: 0)
Epoch 3 - update      12450 => loss:  -0.238 [critic_loss: 0.009] (#OOM: 0)
Epoch 3 - update      12480 => loss:  -0.374 [critic_loss: 0.008] (#OOM: 0)
Epoch 3 - update      12510 => loss:  -0.394 [critic_loss: 0.009] (#OOM: 0)
Epoch 3 - update      12540 => loss:  -0.464 [critic_loss: 0.007] (#OOM: 0)
Epoch 3 - update      12570 => loss:  -0.458 [critic_loss: 0.011] (#OOM: 0)
Epoch 3 - update      12600 => loss:  -0.501 [critic_loss: 0.007] (#OOM: 0)
Epoch 3 - update      12630 => loss:  -0.248 [critic_loss: 0.004] (#OOM: 0)
Epoch 3 - update      12660 => loss:  -0.249 [critic_loss: 0.010] (#OOM: 0)
Epoch 3 - update      12690 => loss:  -0.306 [critic_loss: 0.009] (#OOM: 0)
Epoch 3 - update      12720 => loss:  -0.332 [critic_loss: 0.007] (#OOM: 0)
Epoch 3 - update      12750 => loss:  -0.468 [critic_loss: 0.008] (#OOM: 0)
simultaneousMMT/slurm_scripts/rl/en-de-multimodal-full-large-rl.sh: line 8: 41969 Killed                  CUDA_VISIBLE_DEVICES=0 nmtpy train -C ../simultaneousMMT/custom_configs/en-de/en-de-simrl-multimodal-full-large.conf train.seed:0
Thu Jun 25 20:25:06 2020       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 435.21       Driver Version: 435.21       CUDA Version: 10.1     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce RTX 208...  On   | 00000000:3D:00.0 Off |                  N/A |
| 41%   50C    P2    91W / 260W |   8903MiB / 11019MiB |     19%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GeForce RTX 208...  On   | 00000000:3E:00.0 Off |                  N/A |
|ERR!   81C    P2   141W / 260W |   7533MiB / 11019MiB |     86%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GeForce RTX 208...  On   | 00000000:3F:00.0 Off |                  N/A |
| 41%   38C    P8     3W / 260W |    869MiB / 11019MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GeForce RTX 208...  On   | 00000000:40:00.0 Off |                  N/A |
| 41%   60C    P2    61W / 260W |   4807MiB / 11019MiB |     22%      Default |
+-------------------------------+----------------------+----------------------+
|   4  Tesla V100-PCIE...  On   | 00000000:41:00.0 Off |                  Off |
| N/A   40C    P0    38W / 250W |      0MiB / 32510MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   5  GeForce RTX 208...  On   | 00000000:60:00.0 Off |                  N/A |
| 64%   82C    P2   245W / 260W |   8741MiB / 11019MiB |     87%      Default |
+-------------------------------+----------------------+----------------------+
|   6  GeForce RTX 208...  On   | 00000000:61:00.0 Off |                  N/A |
|ERR!   81C    P2   254W / 260W |   9647MiB / 11019MiB |     85%      Default |
+-------------------------------+----------------------+----------------------+
|   7  GeForce RTX 208...  On   | 00000000:62:00.0 Off |                  N/A |
| 41%   59C    P2    65W / 260W |  10755MiB / 11019MiB |     30%      Default |
+-------------------------------+----------------------+----------------------+
|   8  GeForce RTX 208...  On   | 00000000:63:00.0 Off |                  N/A |
|ERR!   79C    P2   228W / 260W |   8877MiB / 11019MiB |     89%      Default |
+-------------------------------+----------------------+----------------------+
|   9  Tesla V100-PCIE...  On   | 00000000:64:00.0 Off |                    0 |
| N/A   41C    P0    46W / 250W |   9136MiB / 32510MiB |      2%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      8212      C   .../jive/miniconda2/envs/simnmt/bin/python  8893MiB |
|    1     24511      C   /data/zli/anaconda3/envs/torch/bin/python   7523MiB |
|    2     30641      C   /data/ozan/anaconda/bin/python               859MiB |
|    3     38032      C   .../jive/miniconda2/envs/simnmt/bin/python  4797MiB |
|    5     24512      C   /data/zli/anaconda3/envs/torch/bin/python   8731MiB |
|    6     24513      C   /data/zli/anaconda3/envs/torch/bin/python   9637MiB |
|    7     23339      C   .../jive/miniconda2/envs/simnmt/bin/python 10745MiB |
|    8     24514      C   /data/zli/anaconda3/envs/torch/bin/python   8867MiB |
|    9     32380      C   .../jive/miniconda2/envs/simnmt/bin/python  9125MiB |
+-----------------------------------------------------------------------------+
 20:25:07 up 21 days,  8:57, 18 users,  load average: 15.11, 15.28, 16.43
