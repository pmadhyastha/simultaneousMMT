\begin{table}[htb]
\begin{center}
\begin{footnotesize}
\begin{tabular}{|l|l|l|l|l|l|}
\hline
\bf Metric & \bf System & \bf Avg & \bf $\overline{s}_{\text{sel}}$ & \bf $s_{\text{Test}}$ & \bf $p$-value \\
\hline
\multirow{2}{*}{BLEU $\uparrow$}
& baseline & 47.7 & 0.9 & - & - \\
& system 1 & 46.2 & 0.9 & - & 0.00 \\
\hline
\multirow{2}{*}{METEOR $\uparrow$}
& baseline & 63.7 & 0.8 & - & - \\
& system 1 & 62.6 & 0.8 & - & 0.00 \\
\hline
\multirow{2}{*}{TER $\downarrow$}
& baseline & 38.0 & 0.9 & - & - \\
& system 1 & 39.8 & 0.9 & - & 0.00 \\
\hline
\multirow{2}{*}{Length }
& baseline & 97.9 & 0.8 & - & - \\
& system 1 & 98.5 & 0.8 & - & 0.20 \\
\hline
\end{tabular}
\end{footnotesize}
\end{center}
\caption{\label{tab:scores} Metric scores for all systems: jBLEU V0.1.1 (an exact reimplementation of NIST's mteval-v13.pl without tokenization); Meteor V1.4 fr on rank task with all default modules NOT ignoring punctuation; Translation Error Rate (TER) V0.8.0; Hypothesis length over reference length as a percent; . p-values are relative to baseline and indicate whether a difference of this magnitude (between the baseline and the system on that line) is likely to be generated again by some random process (a randomized optimizer). Metric scores are averages over multiple runs. $s_{sel}$ indicates the variance due to test set selection and has nothing to do with optimizer instability.}
\end{table}
